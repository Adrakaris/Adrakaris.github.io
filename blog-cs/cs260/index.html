<!DOCTYPE html>
<html>
<head>
	<title>CS260</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0> 
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 7fr 1fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="../../" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="../../blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="../../about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column tinycolumn">
						<a href="https://ko-fi.com/yijunhu" class="nav">Donate</a>
					</div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS260</h1>
                    <p class="subheading">Algorithms</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Introduction</h1>
			</div>
		</header>

		<div class="cbox">
			<p>
                These notes will be added to throughout the year. Check back later for more.
            </p>

            <ol>
                <li><a href="#greedy">Greedy Algorithms</a></li>
				<li><a href="#paths">Greedy Algorithms and Shortest Paths</a></li>
				<li><a href="#divide">Divide and Conquer</a></li>
				<li><a href="#dynprogramming">Dynamic Programming</a></li>
				<li><a href="#intractibility">Intractbility</a></li>
				<li><a href="#networks">Networks*</a></li>
            </ol>

			<p class="small">*that might be incomplete depending on timing this is 1.5 days before the exam.</p>
		</div>


		<div class="colourband">
			<h2 id="greedy">Greedy Algorithms</h2>
		</div>

		<div class="cbox">
			<h3>Overview</h3>

			<p>
				A <i>Greedy Algorithm</i> is one that builds up a solution from small steps, snatching or discarding the next available one without regard for the bigger picture, based on some simple rule(s).
			</p>

			<ol>
				
				<li><a href="#greedy1">Interval Scheduling</a></li>
				<li><a href="#greedy2">Interval Partitioning</a></li>
				<li><a href="#greedy3">Minimising Lateness</a></li>
				<li><a href="#greedy4">Strategies of Analysis</a></li>
			</ol>

			<h3 id="greedy1">Interval Scheduling</h3>

            <div class="cornell">
                <div class="ir">
					<p>
						Interval Scheduing
					</p>
                    
                </div>
                <div>
					<p>
						You manage a scheduling system for a conference room, and want to schedule as many meetings as possible, with no overlap between meetings and no rearranging of meeting times. 
					</p>
                    <p>
						Let us call meetings <i>jobs</i>, the general term for this problem. Job \(j\) starts at time \(s_j\) and finishes at \(f_j\). Two jobs are <b>compatible</b> [to be scheduled] if they do not overlap.
					</p>
					<p>
						Our goal is to make an algorithm with the following inputs and outputs
						<ul>
							<li><b><span class="sc">in:</span></b> A sequence of jobs (which are just pairs \((s_j,f_j\))</li>
							<li><b><span class="sc">out:</span></b> The <b>maximum subset</b> of mutually compatible jobs</li>
						</ul>
					</p>

					<p>
						We can use a greedy algorithm to solve this, but we need the rules that allow us to implement it. We'll sort the jobs by a specific rule, then take jobs in order, provided they are compatible with all the jobs already selected. 
					</p>

					<p>
						This ordering rule can be many things: earliest start time, earliest finish time, or shortest interval time being some possibilities. (You can try work out which one is correct, but I'm going to immediately reveal it below.)
					</p>
                </div>

				<div class="ir">
					<p>Earliest Finish First</p>
				</div>
				<div>
					<p>
						This is the correct sort rule. We can write an algorithm, for jobs \(j_i\) with starts/finishes \((s_i, f_i)\) for \(i = 1..n\).
					</p>

					<div class="codediv">earliest_finish_first(\(n\), \(s_{1..n}\), \(f_{1..n}\)):
	sort jobs by finish time, and renumber them such that \(f_1 \leq f_2 \leq ... \leq f_n\)
	\(S = \varnothing\)  # set of jobs selected
	for \(j = 1 .. n\):
		if job \(j\) compatible with \(S\):
			\(S\).add(\(j\))
	return \(S\)</div>
				</div>

				<div class="ir">
					<p>Running Time</p>
				</div>
				<div>
					<p>We can prove this algorithm runs with \(O(n \log n)\). This is because:</p>

					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>We know that sorting is at best \(O(n \log n)\). </p>
						<p>The for loop, we can prove is only \(O(n)\). If the for loop is \(O(n)\), then the compatibility check must be \(O(1)\)</p>
						
						<ul>
							<li>If we keep a track of job \(j*\), which is the <i>last job</i> added to \(S\)</li>
							<li>\(j\) would be compatible with \(S\) if and only if \(s_j \geq f_{j*}\)</li>
						</ul>
						<p>Thus comparison is indeed order \(O(1)\).</p>
					</div>

					
				</div>

				<div class="ir">
					<p>Proof of Correctness</p>
				</div>
				<div>
					<p>
						<b><i>Theorem.</i></b> The Earliest Finish First (EFF) algorithm is optimal.
					</p>
					
					<button class="collapsible">Proof...</button>
					<div class="ccontent">
						<p><b><i>Proof.</i></b> We will prove by contradiction. Let us assume EFF is not optimal.</p>
						<p>
							Let \(i_1, i_2, ..., i_k\) be the set of jobs selected by EFF.
						</p>
						<p>
							Let \(j_1, j_2, ..., j_m\) be the optimal set, with \(i_1 = j_1, i_2 = j_2, ... i_r = j_r\) for as large of a value of \(r\) as possible. If EFF is not optimal, then \(m > k\).
						</p>
						<p>
							If job \(i_{r+1}\) does not exist, then by nature of the algorithm all jobs after \(i_r\) are incompatible with it. However, since \(i_r = j_r\), and we know that the optimal must <i>strictly</i> have more jobs than EFF, there must be compatible jobs after \(i_r\), thus we reach a contradiction.
						</p>

						<p>
							If job \(i_{r+1}\) exists, it cannot finish later than \(j_{r + 1}\), because of the sorting rule. Thus we can just replace \(j_{r+1}\) with \(i_{r+1}\), and guarantee that all jobs \(j_r+2\) and afterwards is compatible. Thus the optimal is still optimal, and the condition that we have the <i>largest possible \(r\)</i> has been violated.
						</p>

						$$\tag*{$\Box$}$$
					</div>
				</div>
            </div>

			<h3 id="greedy2">Interval Partitioning</h3>

			<div class="cornell">
				<div class="ir">
					<p>Interval Partitioning</p>
				</div>
				<div>
					<p>You are in charge of scheduling lectures into lecture rooms. Lecture \(j\) starts at \(s_j\) and finishes at \(f_j\), and your goal is to find the <i>minimum</i> number of classrooms needed to schedule all lectures such that no two overlap.</p>
					
					<p>
						<ul>
							<li><b><span class="sc">in:</span></b> A sequence of jobs (which are just pairs \((s_j,f_j\))</li>
							<li><b><span class="sc">out:</span></b> The smallest possible collection of sets (classrooms) of compatible jobs</li>
						</ul>
					</p>

					<p>
						(As the section implies) we use a greedy algorithm, and need to decide the ordering rule. This can be: Earlist start first, Earliest finish first, Shortest lecture first, or something else.
					</p>
				</div>

				<div class="ir">
					<p>Earliest Start First</p>
				</div>
				<div>
					<p>This is the correct sort rule.</p>
					<div class="codediv">earliest_start_first(\(n\), \(s_1 .. s_n\), \(f_1 .. f_n\)):
	sort lectures by start times, renumber s.t. \(s_1 \leq s_2 \leq ... \leq s_n\)
	\(d = 0\)  # number of allocated rooms
	for \(j = 1 .. n\):
		if (lecture \(j\) compat. with all lectures in any classroom \(k\)):
			schedule \(j\) in \(k\)
		else:
			allocate new room \(d+1\)
			schedule \(j\) in room \(d+ 1\)
			\(d = d+1\)
	return the schedule</div>
				</div>

				<div ><p>Running Time</p></div>
				<div>
					<p>
						If we use a suitable data structure to store the rooms in, this algorithm can be \(O(n \log n)\).
					</p>

					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							First of all, sorting is \(O(n \log n)\). If we store all rooms in a <i>Priority Queue</i>, with the key being the <i>finish time of the last lecture</i>:
						</p>
						<ul>
							<li>When we allocate a new room, we insert it into the PQ.</li>
							<li>When we schedule \(j\) in \(k\), we increase the key of \(k\) to \(f_j\).</li>
							<li>To determine whether \(j\) is compatiable with any \(k\), we compare \(s_j\) to <code>findMin</code> of the PQ.</li>
						</ul>

						<p>
							The total number of searches in the priority queue is on order \(O(n)\), where each PQ operation is \(O(\log n)\), thus we get \(O(n \log n)\).
						</p>
					</div>

					<p>
						This implementation will always schedule the next compatible lecture in the room with the earliest finish time.
					</p>
				</div>

				<div>
					<p>Definitions and Observations</p>
				</div>
				<div>
					<p class="blue"><b><i>Definition.</i></b> The <b>Depth</b> of a set of open intervals is the max number of intervals that contain some point. Basically, the point where the most lectures overlap from all rooms determines the depth, which is the number of rooms.</p>

					<img src="./classrooms.svg" alt="at most 3 concurrent lectures = depth 3" style="max-width: 400px; width: 100%;">

					<p>
						Minimum number of rooms would equal the depth (since no lectures can overlap)
					</p>

					<p>
						Also take note that the Earliest Start First (ESF) never schedules two incompatible lectures in one room.
					</p>
				</div>

				<div>
					<p>Proof of Correctness</p>
				</div>
				<div>
					

					<p>
						<b><i>Theorem.</i></b> ESF is optimal.
					</p>

					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Let \(d =\) the number of rooms ESF allocates.
						</p>
						<p>
							Room number \(d\) is opened because we need to schedule a lecture \(j\), which is incompatible with all lectures in rooms \(1 .. d-1\). 
						</p>
						<p>
							Because of the earliest start sort, each incompatible lecture in all prior rooms must have a start time \(\leq s_j\). Furthermore, all \(d\) lectures (including \(j\)) will have ended by \(f_j\).
						</p>
						<p>
							Thus there will be \(d\) lectures overlapping at some time \(s_k + \epsilon\) for a number \(\epsilon\), which is our depth. Since depth = max number of rooms, this demonstrates that ESF is optimal.
							$$\tag*{$\Box$}$$
						</p>
					</div>
				</div>


			</div>

			<h3 id="greedy3">Minimising Lateness</h3>

			<div class="cornell">
				<div>
					<p>Minimising Lateness</p>
				</div>
				<div>
					<p>You are in charge of a single mainframe that can process one job at one time. Job \(j\) requires \(t+j\) units of time to process and is due at \(d_j\) (but can be late). (If \(j\) starts at \(s_j\) it finishes at \(f_j = s_j + t_j\).) Your goal is to schedule jobs to minimise <i>maximum lateness</i>.</p>

					<p>
						Let lateness be defined \(\ell_j = \max(0, f_j - d_j)\), and max lateness is thus \(L = \max(\textrm{all } \ell_j)\).
					</p>

					<p>
						<ul>
							<li><b><span class="sc">in:</span></b> A sequence of jobs (which are just pairs \((t_j,d_j\))</li>
							<li><b><span class="sc">out:</span></b> An ordering of jobs with the least amount of lateness.</li>
						</ul>
					</p>

					<p>
						Some rules we can consider are order by: shortest processing time, earliest deadline, or shortest slack (\(d_j-t_j\)).
					</p>
				</div>

				<div>
					<p>Earliest Deadline First</p>
				</div>
				<div>
					<div class="codediv">
earliest_deadline_first(\(n\), \(t_{1..n} \), \(d_{1..n} \)):
	sort jobs by due time and renumber s.t. \(d_1 \leq d_2 \leq ... \leq d_n\)
	\(t = 0\)
	for \(j = 1..n\):
		assign job \(j\) to interval \([t, t+t_j] \)
		\(s_j = t; f_j = t+t_j\)
		\(t = t+t_j\)
	return intervals \([s_1, f_1] .. [s_n, f_n] \)
					</div>
				</div>

				<div>
					<p>Important Observations and Lemmas</p>
				</div>
				<div>
					<p>
						<b><i>1.</i></b> There exists an optimal schedule with no idle time. If we have a schedule with idle time between jobs, which has no lateness, we can simply remove all idle time and still have no lateness. 
					</p>

					<p>
						<b><i>2.</i></b> Earliest Deadline First (EDF) has no idle time by design.
					</p>

					<p class="blue">
						<b><i>Definition.</i></b> Given a schedule S, an <b>inversion</b> is a pair of jobs \(i, j\) where \(i < j\) (meaning i is due before j) and \(j\) is scheduled before \(i\).
					</p>

					<p>
						<b><i>3.</i></b> The EDF schedule is the (unique) schedule with no inversions (by design).
					</p>

					<p>
						<b><i>4.</i></b> If some schedule with no idle time has an inversion, then it has an adjacent inversion (inverted jobs are next to each other)
					</p>

					<button class="collapsible">Proof of 4...</button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Let \(i-j\) be the <i>closest</i> inversion: <br>
							
						</p>
						<p>
							If \(i, j\) are adjacent, we are done. However, if we have a case like the following:
						</p>
						Schedule: <code>...[ ][j][k][ ][ ][i][ ]...</code>
						<p>
							Then there would exist a job \(k\) between \(j, i\), which is directly after \(j\).
						</p>

						<p>
							If \(j > k\), then \(j-k\) is an adjacent inversion. Else if \(j < k\), then \(k > i\) and \(k-i\) is a <i>closer</i> inversion. Repeat until we encounter an adjacent inversion. $$\tag*{$\Box$}$$
						</p>
					</div>

					<p>
						<b><i>Key Lemma.</i></b> Eschanging two adjacent inverted jobs \(i, j\) reduces the number of inversions by one, and does <b>not</b> increase maximum lateness. 
					</p>

					<button class="collapsible">Proof of Lemma... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Let \(\ell\) denote the lateness <i>before</i>, and \(\ell'\) denote lateness <i>after</i>.
						</p>
						<p>
							\(\ell'_k = \ell_k \; \forall k \neq i, j\), and \(\ell'_i \leq \ell_i\) when \(i\) is moved forward in the schedule.
						</p>
						<p>
							if job \(j\) is not late, we are done. If it is late, then the new lateness \(\ell'_j = f'_j - d_j\) (by definition)
							\begin{align}
								\ell'_j &= f'_j - d_j \\
								&= f_i - d_j \textrm{ (see diagram)} \\
								&\leq f_i - d_i \textrm{ since } d_i \leq d_j \\
								&\leq \ell_i
							\end{align}

							<img src="./inversion.svg" alt="Inverting i and j: new finish time of j is old finish time of i" style="max-width: 400px; width: 100%; display:block;">

							Thus lateness does not increase. $$\tag*{$\Box$}$$
						</p>
					</div>
				</div>

				<div>
					<p>Proof of Correctness</p>
				</div>
				<div>
					<p>
						<b><i>Theorem.</i></b> EDF is optimal.
					</p>

					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Let us define \(S*\) to be the optimal schedule with the <i>fewest</i> inversions. (We say that optimal solutions may have inversions).
						</p>

						<p>
							By observation 1 \(S*\) will have no idle time. 
						</p>

						<p>
							<i>Case 1</i> If \(S* \) has no inversions, then the schedule \(S\) generated by EDF will equal \(S*\) by observation 3.
						</p>
						<p>
							<i>Case 2.</i> If \(S* \) has an inversion:
						</p>
						<ul>
							<li>Let \(i-j\) be an adjacent inversion (Obs. 4)</li>
							<li>Exchanging \(i, j\) decreases the number of inversions by 1, and does not increase max lateness (lemma)</li>
							<li>This contradicts the fewest inversion condition on \(S*\), as we can exchange all the way to no inversions and be (more) optimal - which is the schedule generated by EDF. </li>
						</ul>
						$$\tag*{$\Box$}$$
					</div>
				</div>

				
			</div>

			<h3 id="greedy4">Strategies to Analyse Greedy Algorithms</h3>

				<div class="cornell">
					<div></div>
					<div>
						<p>Three strategies were explored for analysing and proving the optimality of greedy algorithms.</p>
					</div>
					
					<div>
						<p>Greedy Stays Ahead</p>
					</div>
					<div>
						<p>
							Demonstrating that after each incremental step, the greedy algorithm solution is at least as good as any other solution. This incremental building was employed in the proof of Interval Scheduling.
						</p>
					</div>

					<div>
						<p>Structural Bound</p>
					</div>
					<div>
						<p>
							Discover a simple bound / principle on the structure of the problem, which gives the lowest (most optimal) bound on possible solutions, and show that the greedy algorithm always reaches that bound. This was employed in Interval Partitioning with the depth bound.
						</p>
					</div>

					<div>
						<p>Exchange Argument</p>
					</div>
					<div>
						<p>
							By gradually transforming a hypothetical optimal solution (which is not the greedy algorithm one) into the greedy algorithm solution without hurting its quality. This was employed by swapping inversions in Minimising Lateness.
						</p>
					</div>
				</div>

		</div>

		<div class="colourband">
			<h2 id="paths">Greedy Algorithms and Shortest Paths</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#path-1">Djikstra's Algorithm</a></li>
				<li><a href="#path-2">Minimum Spanning Trees</a></li>
			</ol>

			<h3 id="path-1">Djikstra's Algorithm</h3>

			<div class="cornell">
				<div>
					<p>Single pair shortest paths</p>
				</div>
				<div>
					<p>
						<b><i>Problem.</i></b> Given a digraph \(G = (V, E)\), with edge lengths \(\ell_e \geq 0\), a source \(s \in V\) and a destination \(t \in V\), find the shortest directed path from s to t. 
					</p>
					<p>
						Called single pair because there is one source and one destination.
					</p>
				</div>

				<div>
					<p>Variations</p>
				</div>
				<div>
					<p>
						We can also have just single source shortest path, where from a source we have to find the path to ALL other vertices (i.e. tree rooted at S). 
					</p>
				</div>
				<div>
					<p>Djikstra's Algorithm</p>
				</div>
				<div>
					<p>
						Djikstra's is a well known algorithm for single source shortest path. It is a greedy algorithm, and works on graphs with <b>non-negative weights</b> only. 
					</p>
					<ul>
						<li>
							Maintain a set of explored nodes S and an array \(d : d_u =\) length of shortest path \(s \leadsto u\). 
						</li>
						<li>
							Initially set \(S \longleftarrow \{s\};\; d_s \longleftarrow 0\).
						</li>
						<li>
							Repeatedly choose new \(v \not \in S\) which minimises the "path value" of \(v: \pi(v)\). 
							\[\pi(v) = \min_{e = \langle u, v \rangle : u \in S}(d_u + \ell_e)\]
							i.e. pick the edge leading out from any node \(u\) in the explored set which has the smallest \(d_u + \ell_e\) value. 
						</li>
						<li>
							Add that \(v\) to S, and set \(d_v \longleftarrow \pi(v)\). If we need to recover the path later, store also the edge that lead to \(v\). 
						</li>
					</ul>
				</div>
				<div>
					<p>Proof of correctness</p>
				</div>
				<div>
					<p>
						<b><i>Assertion.</i></b> For all nodes \(u \in S, \; d_u\) is the length of the shortest path.
					</p>
					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Do induction on \(|S|\), the explored set. 	
						</p>
						<p>
							Base case: \(|S| = 1,\; S = \{s\},\; d_s = 0\), which is trivially true.
						</p>
						<p>
							Induction: Assume true for some \(|S| = n \geq 1\). 
						</p>
						<ul>
							<li>
								Let \(v\) be the next node added to S. \((u, v)\) is that final edge.
							</li>
							<li>
								A shortest path \(s \leadsto u + (u, v)\) has path length \(\pi(v)\). 
							</li>
							<li>
								Consider any other \(s \leadsto v\) path P (we want to prove that P is no shorter than \(\pi(v)\)).
								<figure>
									<img src="./djikstra-proof.png" alt="Another path s to v" style="max-width: 200px;">
									<figcaption>Green path in figure</figcaption>
								</figure>
							</li>
							<li>
								The path must have an edge where it leaves S. Let the first edge that does this be \(e = (x, y)\) for some nodes \(x, y\). 
							</li>
							<li>
								Since Djikstra, by definition, picks the node outside of S that is <i>closest</i> to \(s\), \(P \geq \pi(v)\) as soon as \(y\) is reached, since \(v\) was picked before \(y\). 
							</li>
							<li>
								Formally, this can be written as 
								\[\ell(P) = \ell(P') + \ell_{(x,y)} = d_x + \ell_e = \pi(y) \geq \pi(v)\]
								$$\tag*{$\Box$}$$
							</li>
						</ul>
					</div>
				</div>

				<div>
					<p>An efficient implementation - optimisations</p>
				</div>
				<div>
					<p>
						<b><i>Optimisation 1.</i></b> For all unexplored nodes \(v \not \in S\), explicitly maintain \(\pi(v)\) instead of computing from definition each time. Initialise it to \(\infty\) when we don't have a value. 
					</p>
					<p>
						\(\pi(v)\) can only decrease, since S increases and we allow more nodes into the minimisation formula. Suppose we do get a \(u\) added to S, with an edge \(e = \langle u, v \rangle\), it is sufficient to compute just 
						\[\pi(v) = \min(\pi(v), \pi(u) + \ell_e)\]
					</p>
					<p>
						<b><i>Optimisation 2.</i></b> Use a min-oriented priority queue to choose unexplored node \(v\) as efficiently as possible, sorting by the value \(\pi(v)\). 
					</p>
				</div>

				<div>
					<p>Implementation pseudocode</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">djikstra(\(V, E, \ell, s\)):
	for each \(v \not \in S\):
		\(\pi[v] \longleftarrow \infty\)
		\(pred[v] \longleftarrow \textrm{NULL}\)
	\(\pi(s) \longleftarrow 0\)
	pq \(\longleftarrow\) new PriorityQueue
	for each \(v \in V\):
		pq.insert(\(\pi[v], v\))
	while not pq.empty:
		\(u \longleftarrow\) pq.del_min()
		for each edge \(e = \langle u, v \rangle \in E\):
			if \(\pi[v] &gt; \pi[u] + \ell_e\):
				pq.decrease_key(\(v, \pi[u] + \ell_e\))
				\(\pi[v] \longleftarrow \pi[u] + \ell_e\)
				\(pred[v] \longleftarrow e\)</div>
				</div>

				<div>
					<p>Time complexity</p>

				</div>
				<div>
					<p>
						The running time is dominated by the number of priority queue operaions. We insert, delete minimum, and decrease key a total of \(O(n)\) times. Our implementation of the priority queue thus determines our running time. 
					</p>
					<p>
						Note that \(m\) is used for the number of edges, which is \(\Theta(n)\) on sparse graphs and \(\Theta(n^2)\) on dense graphs.
					</p>
					<table>
						<tr>
							<th>PQ</th> <th>Insert</th> <th>Delete-min</th> <th>Decrease-key</th> <th>Total</th>
						</tr>
						<tr>
							<td>Node indexed array: \(A[i] =\) priority of \(i\)</td>
							<td>\(O(1)\)</td> <td>\(O(n)\)</td> <td>\(O(1)\)</td> <td>\(O(n^2)\)</td>
						</tr>
						<tr>
							<td>Binary Heap</td> <td>\(O(\log n)\)</td> <td>\(O(\log n)\)</td> <td>\(O(\log n)\)</td> <td>\(O(m \log n)\)</td>
						</tr>
						<tr>
							<td>"D-way Heap"</td> <td>\(O(d \log_d n)\)</td> <td>\(O(d \log_d n)\)</td> <td>\(O(\log_d n)\)</td> <td>\(O(m \log_{\frac{m}{n}} n)\)</td>
						</tr>
						<tr>
							<td>Fibonacci Heap</td> <td>\(O(1)\)</td> <td>\(O(\log n)\) amortised</td> <td>\(O(1)\) amortised</td> <td>\(O(m + n \log n\)</td>
						</tr>
						<tr>
							<td>Integer PQ</td> <td>\(O(1)\)</td> <td>\(O(\log \log n)\)</td> <td>\(O(1)\)</td> <td>\(O(m + n \log \log n)\)</td>
						</tr>
					</table>
					<p>
						For dense graphs, a node-indexed array is best, whilst for sparse graphs, a binary heap is best. 
					</p>
				</div>
			</div>
			
			<h3 id="path-2">Minimum Spanning Trees</h3>

			<div class="cornell">
				<div>
					<p>Cutsets</p>
				</div>
				<div>
					<p class="blue">
						<b><i>Definition.</i></b> A <b>cut</b> is an arbitrary partition of nodes into two non-empty sets: \(S, V \setminus S \;(\textrm{or } V - S)\).
					</p>

					<p class="blue">
						<b><i>Definition.</i></b> A <b>cutset</b> of cut S is the set of edges : each edge has exactly one endpoint in S. 
					</p>

					<figure>
						<img src="./cutset.png" alt="Cutset and cut" style="max-width: 400px;">
						<figcaption><i>A cut (pink) and a cutset (blue edges)</i></figcaption>
					</figure>
				</div>

				<div>
					<p>Cycle-cut intersetion</p>
				</div>
				<div>
					<p class="side">
						<b><i>Proposition.</i></b> An (arbitrary) cycle intersects a cutset on an <b>even</b> number of edges. 
					</p>
					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> by picture:
							<figure>
								<img src="./cutset-cycle-even-edges.png" alt="" style="max-width: 400px;">
							</figure>
							In short, if a cycle and cut don't intersect, we're all OK. If a cycle starts in the cut and leaves the cut, it must eventually return, so leaving and returning matches, making pairs.
							$$\tag*{$\Box$}$$
						</p>
					</div>
				</div>

				<div>
					<p>Spanning and Minimum Spanning Trees</p>
				</div>
				<div>
					<p>
						A <b>spanning tree</b> is a subgraph that is acyclic and connected. <a href="../cs130/#tre-1">See here for notes on spanning trees</a>.
					</p>
					<p>
						Given an undirected, connected graph with weights, the <b>minimum spanning tree</b> is the spanning tree with minimum edge cost. 
					</p>
					<p class="side">
						<b><i>Theorem.</i></b> <b>Cayley's Theorem.</b> A complete graph \(k_n\) has \(n^{n-2} \) spanning trees. 
					</p>
					<p>
						This theorem means though that brute-forcing spanning trees is a no-no.
					</p>
				</div>

				<div>
					<p>Fundamental cycle</p>
				</div>
				<div>
					<p>
						<b><i>Define.</i></b> Let \(H=(V, T)\) be spanning tree of \(G = (V, E)\). For any non-tree edge \(e \in E, \; T \cup \{e\}\) contains a unique cycle C (maximal acylcic). 
					</p>
				</div>
			</div>
		</div>

		<div class="colourband">
			<h2 id="divide">Divide and Conquer</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#div-1">Merge Sort</a></li>
				<li><a href="#div-2">Closest pair of points</a></li>
				<li><a href="#div-3"><b>Master Theorem</b></a></li>
				<li><a href="#div-4">Integer Multiplication</a></li>
			</ol>

			<div class="cornell">
				<div>
					<p>Divide and Conquer Paradigm</p>
				</div>
				<div>
					<p>
						Divide and conquer is a strategy for solving algorithms where the main problem is <b>divided</b> into <b>independent</b> subproblems, which are then solved (<b>conquered</b>) <b>recursively</b>.
					</p>

					<p>
						In general, this goes as follows:
						<ul>
							<li>Divide a problem of size \(n\) into subgroups, e.g. 2 subproblems of size \(\frac{n}{2}\) - <i>typically \(O(n)\)</i></li>
							<li>Solve the 2 subproblems recursively</li>
							<li>Combine the subproblems into one - <i>typically \(O(n)\)</i></li>
						</ul>
						This can end up faster than "traditional" polynomial time methods. 
					</p>
				</div>
			</div>

			<h3 id="div-1">Merge Sort</h3>

			<p>
				Your classic divide and conquer sorting algorithm that you learned in GCSE. 
			</p>

			<div class="cornell">
				<div>
					<p>Process</p>
				</div>
				<div>
					<ul>
						<li>For an input size (n), divide into a <i>left</i> half and a <i>right</i> half. - \(O(n)\)</li>
						<li><b>Recursively</b> sort left and right halves (refer to above)</li>
						<li>Combine two sorted halves, maintaining the order. - \(O(n)\)</li>
					</ul>
				</div>
				<div>
					<p>Combining halves</p>
				</div>
				<div>
					<p>
						Suppose we have two sorted halves A and B, with
						\[A = [a_1, a_2, a_3], \; B = [b_1, b_2, b_3, b_4]\]
						Maintain two pointers to the sorted arrays. Scan A and B, comparing \(a_i\) and \(b_j\). Append the <i>smaller</i> value to the output list, and increment that pointer only. Finally, append any left over values from one array if the other has reached the end. 
					</p>
					<p>
						<i>Linear for scanning, constant for comparing, constant for appending.</i>
					</p>
					
				</div>
				<div>
					<p>Implementation</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">mergesort(list L):
	if L.length = 1:
		return L
	divide L into two halves, A and B
	A \(\longleftarrow\) mergesort(A)
	B \(\longleftarrow\) mergesort(B)
	L \(\longleftarrow\) merge(A, B)  # implementation of this function 
		# is left as an excersise to the reader
	return L</div>

					<p>
						Let's say mergesort has a procesing time of \(T(n)\). Thus, every subproblem (\(A, B\)) takes \(T(\frac{n}{2})\) time. The <code>merge</code> portion as we know is linear, and is \(\Theta(n)\).
					</p>
					<p>
						Thus we can conclude \(T(n)\) is bounded by \(\Theta(n) + 2T(\frac{n}{2})\).
					</p>
				</div>
				<div>
					<p>Useful recurrence relation</p>
				</div>
				<div>
					<p>
						The above gives rise to a useful recurrence relation. 
					</p>
					<p>
						<b><i>Definition.</i></b> Let \(T(n)\) to be the number of comparisons mergesort makes proportional to list size \(n\). This satisfies the recurrence 
						\[T(n) = \begin{cases} 0 & \textrm{if } n = 1 \\ T(\left \lfloor{\frac{n}{2}} \right \rfloor) + T(\left \lceil{\frac{n}{2}} \right \rceil) + n & \textrm{if } n > 1 \end{cases}\]
					</p>
				</div>
				<div>
					<p>Complexity of merge sort</p>
				</div>
				<div>
					<p>
						<b><i>Claim.</i></b> For mergesort, \(T(n) = O(n \log n)\).
					</p>
					<p><i>
						Assume \(n\) is a power of 2. This does not affect the integrity much but makes proving easier.
					</i></p>
					<p class="small">
						The following proofs demonstrate important techniques so are <i>shown by default</i>. Clicking the drop-down bar (twice) will hide them. 
					</p>
				</div>
				<div>
					<p>Proof by tree</p>
				</div>
				<div>
					<p>
						<b><i>Assertion 1.</i></b> \(T(n) = O(n \log n)\) if \(T(n)\) satisfies the following recurrence (given above assumption):

						\[T(n) = \begin{cases} 0 & \textrm{if } n = 1 \\ 2T(\frac{n}{2}) + n & \textrm{if } n > 1 \end{cases}\]
					</p>

					<button class="collapsible">Proof by Tree... </button>
					<div class="ccontent inv">
						<p>
							<b><i>Proof.</i></b> Represent the recurrence in tree form:
							<figure>
								<img src="./recurrence-tree.png" alt="recurrence tree">
							</figure>
							<ul>
								<li>On level \(i\), there are \(2^i\) nodes. Each node has \(\frac{n}{2^i} \) items so does that amount of work. Thus each level does \(n\) work. </li>
								<li>\(n = 2^k\) (is a power of 2), and we <i>halve</i> \(n\) every time, thus we do \(k\) halvings to get to \(2^0\): 1 item. Thus \(k\) = the number of levels = \(\log_2 n\).</li>
							</ul>
							\(\log_2 n\) times \(n\) work gets \(O(n \log n)\)
							$$\tag*{$\Box$}$$
						</p>
					</div>
				</div>
				<div>
					<p>Proof by induction</p>
				</div>
				<div>
					<p>A proof method that may be easier to use (and certainly is easier to represent in HTML). We use the <b>same proposition</b> as above.</p>

					<button class="collapsible">Proof by Induction...</button>
					<div class="ccontent inv">
						<p>
							<b><i>Proof.</i></b> Induction on the size of \(n\).
							<ul>
								<li><b>Base case:</b> \(n=1\), thus \(T(1) = 0 = 1 \log_2 1\).</li>
								<li><b>Hypothesis:</b> Assume \(T(n) = n \log_2 n\).</li>
							</ul>
						</p>
						<p>
							Let us look at \(T(2n)\).
							\begin{align}
								T(2n) &= 2T(n) + 2n\\
								&= 2n \log_2 n + 2n \textrm{ by ind hyp.} \\
								&= 2n (\log_2 n + 1) = 2n (\log_2 n + \log_2 2) \\
								&= 2n \log_2(2n).
							\end{align}
							$$\tag*{$\Box$}$$
						</p>
					</div>
				</div>

				<div>
					<p>Extending inductive proof</p>
				</div>
				<div>
					<p>
						So far we have been ignoring floors and ceilings. In fact, we can take as granted that <b>floor and ceil may be ignored</b> outright. However, here's an inductive proof for the assertion that uses floor and ceil. 
					</p>
					<p>
						<b><i>Assertion 2.</i></b> \(T(n) \leq n \left \lceil{\log_2 n} \right \rceil\) if \(T(n)\) satisfies the following recurrence. \(n\) is not assumed to be a power of 2. 
					</p>

					<div style="display: none;">
						\[ \require{enclose} \]	
					</div>
					

					<button class="collapsible">Proof by Strong Induction... </button>
					<div class="ccontent inv">
						<p>
							<b><i>Proof.</i></b> Strong induction on \(n\). Instead of just assuming the prior is true, we assume <i>all priors</i> are true. The base case is the same and is true. Please note \(\log\) means \(\log_2\). 
						</p>
						<p>
							Let \(n_1 = \left \lfloor{n} \right \rfloor\) and \(n_2 = \left \lceil{n} \right \rceil\). <i>Note that \(n_1 + n_2 = n\)</i>. The <b>inductive hypothesis</b> is that the recurrence is satisifed for <b>all</b> values up to \(n-1\). 
						</p>
						<p>
							Some easy-to-miss changes are circled. The circles are for emphasis and are otherwise meaningless.
							\begin{align}
								T(n) &\leq T(n_1) + T(n_2) + n \\
								&\leq n_1 \left \lceil{\log n_1} \right \rceil + n_2 \left \lceil{\log n_2} \right \rceil + n \textrm{ by ind hyp.} \\
								\because n_2 \geq n_1,\; T(n) &\leq n_1 \left \lceil{\log 
									{\scriptstyle \enclose{circle}{\kern .06em n_2\kern .06em}}
								} \right \rceil + n_2 \left \lceil{\log n_2} \right \rceil + n \\
								&\leq (n_1 + n_2) \left \lceil{\log n_2} \right \rceil + n \\
								& \leq n \left \lceil{\log n_2} \right \rceil + n \\
								& \leq n (\left\lceil{\log  {\scriptstyle \enclose{circle}{\kern .06em n \kern .06em}} }\right\rceil + 1) + n \; *\\
								& \leq n \left\lceil{\log n}\right\rceil .
							\end{align}
							$$\tag*{$\Box$}$$
						</p>
						<p>
							The jump marked \(*\) is explained as follows:
							\begin{align}
								2^{\log n_2} = n_2 &= \left\lceil{\frac{n}{2}}\right\rceil \\
								&\leq \left\lceil{\frac{2^\left\lceil{\log n}\right\rceil }{2}}\right\rceil \\
								&\textrm{since } n = 2^{\log n} \textrm{, } 2^{\left\lceil{\log n}\right\rceil } \textrm{ must be not lower. } \\
								&\leq \frac{2^{\left\lceil{\log n}\right\rceil }}{2} \textrm{ since RHS must } \in \mathbb{Z} \\
								&\leq 2^{\left\lceil{\log n}\right\rceil -1} \\
								\therefore \log n_2 = \log{2^{\log n_2}} &= \left\lceil{\log n}\right\rceil - 1.
							\end{align}
						</p>
						<p class="small">
							If you're curious, right click -> show math as tex commands, it's nasty to write, thank god for vscode snippets.
						</p>
					</div>
				</div>
			</div>

			<h2 id="div-2">Closest Pair of Points</h2>

			<div class="cornell">
				<div>
					<p>Problem </p>
				</div>
				<div>
					<p>
						Given \(n\) points in a euclidean plane, find the closest pair of points. 
					</p>
				</div>
				<div>
					<p>Possible non-divide and conquer solutions</p>
				</div>
				<div>
					<p>
						Listed are some non-divide and conquer solutions, for interest. 
					</p>
					<button class="collapsible">Expand... </button>
					<div class="ccontent">
						<p>
							<b>Brute force</b>: the "naive" algorithm, I think you can guess what this does. Solves in \(\Theta(n^2)\) but we can do better. 
						</p>
						<p>
							<b>Dimension collapsing</b>: we can consider only 1 dimension (say, x), sort the points and find the closest. This can be done in \(\Theta(n \log n)\) by sorting points, and comparing only consecutive ones. The only problem is it has counterexamples and may not always work depending on your configuration of points.
						</p>
						<p>
							This relies on an assumption of <i>non-degeneracy</i> (see later).
						</p>
						<p>
							We can extend this idea to doing dimension collapsing for 2D by repeating the process for both x and y, finding the points which are closest together on those two axes.
						</p>
						<p>
							However this is also flawed and has counterexamples. 
						</p>

						<p>
							<b>Division:</b> We can try a technique more akin to divide and conquer - dividing the space up into halves or quadrants, then considering each quadrant separately, then the boundaries of quadrants. Ideally we'd want all quadrants to have an equal number of dots, but this is hard to guarantee. 
						</p>
					</div>
					
				</div>

				<div>
					<p>The divide and conquer algorithm</p>
				</div>
				<div>
					<p>
						<b><i>Definition.</i></b> the <b>non-degeneracy</b> assumption asserts that each point should have a unique \(x\) coordinate. 
					</p>
					<p>
						We <b>divide</b> by drawing a vertical line \(L : \frac{n}{2}\) points on each side. We <b>conquer</b> by recursively finding the closest point in each "half". We <b>combine</b> by checking if there's a closer pair that lies across the boundary. 
					</p>
					<p>
						It seems initially hard to do this more efficiently than the brute force \(n^2 \), but there are a few tricks we can employ. 
					</p>
					<p>
						Take the minimum distance from the pairs on either side, let it be \(\delta\). Say our two halves find closest points with length 12 and 21, \(\delta = \min(12, 21)\). It is the sufficient to <b>only check points within \(\delta\) distance from L</b>.
					</p>
					<p>
						We need only sort the boundary points by their \(y\) coordinate, then compute the closest points in \(y\). This much may be quite easy to understand, but that still leaves us with quadratic time if we have to compute distances to all other points, but in fact, we only need to compute <b>7</b>.
					</p>
					<p>
						Every point checks against up to 7 others, and the minimum distance found is kept track of as the boundary points are iterated through. This actually will reduce our combine step down to <b>linear time</b>: \(O(n)\). 
					</p>
				</div>

				<div>
					<p>Why 7!?</p>
				</div>
				<div>
					<p>
						To prove this, first <b>let</b> all boundary points be sorted by y coordinate, and numbered in increasing order, \(s_1, s_2, \dots, s_n\).  \(s_i\) is the point with the \(i\)<sup>th</sup> largest y value. 
					</p>
					<p>
						<b><i>Proposition.</i></b> If \(|i-j| \geq 7\), the distance between \(s_i, s_j\) is <b>at least \(\delta\)</b>.
					</p>
					<button class="collapsible">Proof...</button>
					<div class="ccontent inv">
						<p>
							Let us take any point \(s_i\) with some \(y\) coordinate. Consider a rectangle \(\delta\) tall and \(2\delta\) wide, spanning the boundary region with its bottom being the \(y\) coord of \(s_i\). Split this rectangle into 8 squares, \((0.5\delta)^2\).
						</p>
						<img src="./7-rectangles.png" alt="rectangle split into 8 squares" style="max-width: 200px;">
						<img src="./7-furthest-points.png" alt="furthest away points in one square are delta over root 2 apart" style="max-width: 300px;">
						<p>
							The furthest away two points could be in one single square is at opposite diagonals, \(\frac{\delta}{2}\) away. \(\frac{\delta}{2 } < \delta\) - this means that <b>this is an impossible configuration</b>: both points would be on the same side, with the distance between less than \(\delta\), which is <i>defined</i> to be the shortest distance from both sides. 
						</p>
						<p>
							Thus, there can only be <b>one point per square</b>. At most 7 other points <i>may</i> be valid options (have distances closer than \(\delta\)), so we need only check 7. 
						</p>
						$$\tag*{$\Box$}$$
					</div>
				</div>

				<div>
					<p>Implementation</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">closestPair(points \(p_1, p_2, \dots, p_n\)):
	computer vertical line \(L\) such that half the points are on either side  # (1)
	\(\delta_1 \longleftarrow\) closestPair(left half)  
	\(\delta_2 \longleftarrow\) closestPair(right half)  # (2)
	\(\delta \longleftarrow \min(\delta_1, \delta_2)\)
	delete all points further than \(\delta\) from \(L\)  # (3)
	sort remaining points by \(y\) coordinate  # (4)
	for \(p \in \) remaining points:  # (5)
		work out the distances up to the next 7 points 
		if the smallest of those \(\leq \delta\):
			\(\delta \longleftarrow \) that distance 
	return \(\delta\)</div>
				</div>

				<div>
					<p>Running time</p>
				</div>
				<div>
					<p>
						The labelled sections are as follows: (1) is \(O(n)\), (2) is \(2T(\frac{n}{2})\), (3) is \(O(n)\), (4) is \(O(n \log n)\), and 5 is \(O(n)\).
					</p>
					<p>
						As to \(T(n)\), knowing the running time of dividing and combining steps we can get the recurrence 

						\[ T(n) = \begin{cases} \Theta(1) & n = 1 \\ T(\left\lfloor{\frac{n}{2}}\right\rfloor )  + T(\left\lceil{\frac{n}{2}}\right\rceil ) + \Theta(n \log n) & \textrm{otherwise} \end{cases} \]
					</p>
					<p>
						This resolves down to a running time of \(\Theta(n \log^2 n)\).
					</p>
				</div>

				<div>
					<p>Improvements on running time</p>
				</div>
				<div>
					<p>
						Later work has been done to push the algorithm down to \(O(n \log n\). This involves techniques such as preserving sorting of delta strip, returning lists of points sorted by x and y, and sorting points by merely doing one merge, which is linear time. 
					</p>
				</div>
			</div>

			<h2 id="div-3">Master Theorem</h2>

			<div class="cornell">
				<div>
					<p>Premise</p>
				</div>
				<div>
					<p >
						The <b>master theorem</b> is a single method to solve all common recurrences of the form
						<span style="border-spacing: 0px 0px;">\[T(n) = aT(\frac{n}{b}) + f(n)\]</span>
						
						Where \(a, b \in \mathbb{Z}^+\), \(b \geq 2\), and with \(T(0) = 0, \; T(1) = \Theta(1)\). 
					</p>
					<p>
						An input of size \(n\) makes \(a\) recursive calls of size <span style="border-spacing: 0px 0px;">\(\frac{b}{n}\)</span>. We also assume that \(f(n) \geq 0\). 
					</p>
				</div>

				<div>
					<p>Deriving via recursion trees</p>
				</div>
				<div>
					<p></p>
					<button class="collapsible">Expand... </button>
					<div class="ccontent">
						<p>
							We can solve such recurrences with a recursion tree. (We assume that \(n\) is a power of \(b\).)
							<ul>
								<li>Let  \(a=\) branching factor</li>
								<li>\(a^i = \) number of subproblems at layer \(i\)</li>
								<li>\(1+\log_b n\) levels</li>
								<li>\(\frac{n}{b^i} =\) size of subproblems at layer \(i\).</li>
							</ul>
						</p>

						<p>
							Suppose \(T(n)\) satisfies \(T(n) = aT(\frac{n}{b}) + n^c\) for some c: i.e. \(f(n)\) is a polynomial. 
						</p>
						<p>
							Then at we have \(n^c\) work at layer 0, \(a(\frac{n}{b})^c\) at 1, \(a^2 \frac{n}{b^2}^c\) at 2, ..., \(a^i \frac{n}{b^i}^c\) at level \(i\), for a total of \(\log_b (n) \) levels. 
						</p>
						<p>
							The last \(\log_b n\)<sup>th</sup> level has \(a^i = a^{\log_b n} = n^{\log_b a}\):
							\[a^{\log_b n} = (b^{\log_b a})^{\log_b n} = b^{\log_b a \cdot \log_b n} = (b^{\log_b n})^{\log_b a} \]
						</p>

						<p>
							At level \(i\), the work is \(a^i (\frac{n}{b^i})^c = a^i \frac{n^c}{b^{ic}} = n^c \frac{a^i}{b^{ic}} = n^c (\frac{a}{b^c})^i\).
						</p>

						<p>
							Thus we can say \(T(n) = n^c \sum_{i=0}^{\log_b n} r^i\) where \(r = \frac{a}{b^c}\) (which is a constant).
						</p>

						<p>
							Looking at this sum, we can get that 
							\[
							T(n) = n^c \sum^{\log_b n}_{i=0} r^i = \begin{cases} 
								\Theta(n^c) &\textrm{if }r&lt;1 &c &gt; \log_b a\\
								\Theta(n^c \log n) &\textrm{if } r = 1 &c = \log_b a \\
								\Theta(n^{\log_b a}) &\textrm{if } r &gt; &c &lt; \log_b a 1
							\end{cases}
							\]
							Since it is a gemetric series thus follows geometric series rules. 
						</p>
					</div>

					
				</div>
				<div>
					<p>Master Theorem</p>
				</div>
				<div class="blue">
					<p >
						<b><i>Theorem.</i></b> Let \(a \geq 1, b \geq 2, c &gt; 0.\) Let \(T(n)\) be a function on \(\mathbb{Z}_{\geq 0} \) that satisfies the recurrence
						<span style="border-spacing: 0px 0px;">\[T(n) = aT(\frac{n}{b}) + \Theta(n^c)\]</span> 
						With \(T(0) = 0, T(1) = \Theta(1),\) and <span style="border-spacing: 0px 0px;">\(\frac{n}{b}\) is either \(\left\lceil{\frac{n}{b}}\right\rceil \) or \(\left\lfloor{\frac{n}{b}}\right\rfloor \).</span>   Then the following cases apply:
						<ol>
							<li>\(c &lt; \log_b a \implies T(n) = \Theta(n^{\log_b a})\) - recurring part dominates</li>
							<li>\(c = \log_b a \implies T(n) = \Theta(n^c \log n)\) - balances</li>
							<li>\(c &gt; \log_b a \implies T(n) = \Theta(n^c)\) - processing part dominates</li>
						</ol>
					</p>
				</div>

				<div>
					<p>Overview of proof</p>
				</div>
				<div>
					<ul>
						<li>
							Prove theorem when \(b \in \mathbb{Z}\) and \(n\) is a power of \(b\)
						</li>
						<li>
							Extend the domains of reasoning to rationals or reals
						</li>
						<li>
							Deal with floor and ceil for <i>at most 2 levels</i>, e.g. 
							\begin{align}
								\left\lceil{\frac{\left\lceil{\frac{\left\lceil{\frac{n}{b}}\right\rceil }{b}}\right\rceil }{b}}\right\rceil & &lt; \frac{n}{b^3} + (\frac{1}{b^2} + \frac{1}{b} + 1) \\
								&\leq \frac{n}{b^3} + 2 \;\; (\because b \geq 2)
							\end{align}
						</li>
					</ul>
				</div>

				<div>
					<p>Theorem extension</p>
				</div>
				<div>
					<p>
						We can replace \(\Theta\) with \(O, \Omega\) with no problem. 
					</p>
					<p>
						We can replace the initial conditions with \(T(n) = \Theta(1)\) for all \(n \leq n_0\) (which is a constant) and only require the recurrence holds for \(n > n_0\). 
					</p>
				</div>

				<div>
					<p>Examples</p>
				</div>
				<div>
					<p>
						\(T(n) = 3T(\left\lfloor{\frac{n}{2}}\right\rfloor + 5n\)
					</p>
					<button class="collapsible">Expand... </button>
					<div class="ccontent">
						<p>
							\(a = 3, b = 2\) (and ignore floor). \(c = 1, 5n = O(n)\). \(\log_b a = \log_2 3 \approx 1.58\)
						</p>
						<p>
							Case 1 of 3, thus \(T(n) = \Theta(n^{\log_2 3} )\)
						</p>
					</div>
					<p>
						\(T(n) = T(\left\lfloor{\frac{n}{2}}\right\rfloor) + T(\left\lceil{\frac{n}{2}}\right\rceil ) + 17n \)
					</p>
					<button class="collapsible">Expand... </button>
					<div class="ccontent">
						<p>
							Can "merge" the floors and ceils, thus \(a=2, b=2, c=1\). 
						</p>
						<p>
							\(T(n) = O(n \log n)\)
						</p>
					</div>
				</div>

				<div>
					<p>Gaps in master theorem</p>
				</div>
				<div>
					<p>
						Master theroem will not work if the number of branches is not constant: \(T(n) = nT(\frac{n}{2 }) + n^2 \),
					</p>
					<p>
						The number of subproblem branches is less than 1: \(T(n) = \frac{1}{2} T(\frac{n}{2}) + n^2\),
					</p>
					<p>
						And cannot directly work if work done by subproblem is not \(\Theta(n^c)\): \(T(n) = 2T(\frac{n}{2 }) + n \log n\).
					</p>
				</div>
			</div>

			<h3 id="div-4">Integer Multiplication</h3>

			<div class="cornell">
				<div>
					<p>Addition and Subtraction</p>
				</div>
				<div>
					<p>
						Numbers are stored and processed in computers at binary strings. This changes the considerations when doing basic arithmetic. 
					</p>
					<p>
						Looking at addition and subtraction first, i.e. given 2 binary strings \(a, b\) compute \(a+b\) and \(a-b\). 
					</p>
					<p>
						The addition and subtraction method one learns in primary school is the most efficient algorithm, at \(O(n)\) where \(n\) is the number of digits. 
					</p>
				</div>
				<div>
					<p>Multiplication</p>
				</div>
				<div>
					<p>
						<b><i>Problem.</i></b> Given 2 \(n\) bit numbers, multiply them.
					</p>
					<p>
						The "primary school algorithm" for binary multiplication, in short, is <i>for every digit in the 2<sup>nd</sup> number, write the corresponding shifted 1<sup>st</sup> number multiplied by that digit.</i> We can see that this performs \(\Theta(n^2)\) bit operations. 
					</p>
					<p>
						The question is, is the so-called "primary school algorithm" most efficient for multiplication?
					</p>
					<p>
						The answer is no: one can do some divide and conquer tricks to make an algorithm more efficeint than this. 
					</p>
				</div>
			</div>

			<button class="collapsible">Warmup... </button>
			<div class="ccontent cnul">
			<div class="cornell">
				<div>
					<p>A possible algorithm</p>
				</div>
				<div>
					<p>
						But can we do better? To multiply numbers \(x, y\), we could divide them in half into low and high order bits. 
					</p>
					<p>
						Take \(x = 10001101\), if we have an \(m = \left\lceil{\frac{n}{2}}\right\rceil \) that is the number of bits to split at, \(x = 2^m a + b\) where \(a, b\) are the first half and second half of the bits respectively. 

						\begin{align}
							\textrm{let } &m = \left\lceil{\frac{n}{2}}\right\rceil \\
							\textrm{then } &a = \left\lfloor{\frac{x}{2^m}}\right\rfloor & b = x \mod 2^m \\
							&c = \left\lfloor{\frac{y}{2^m}}\right\rfloor & d = y \mod 2^m
						\end{align}

						Note that multiplying or dividing by a power of 2 is very easy (bit shifts). We know that 
						\[xy = (2^m a + b)(2^m c + d) = 2^{2m} ac + 2^m(bc + cd) + bd\]
						Which as we can see contains 4 smaller multiplications: \(ac, bc, cd, bd\). 
					</p>
				</div>
				<div>
					<p>Implementation</p>
				</div>
				<div>
					<p>
						\(\textrm{The implementation is left as an exersise to the reader.}\)
					</p>
				</div>
				<div>
					<p>Running time</p>
				</div>
				<div>
					<p>
						This algorithm has the recurrence
						\[
						T(n) = \begin{cases} \Theta(1) & n=1 \\ 4T(\left\lceil{\frac{n}{2}}\right\rceil + \Theta(n) & n &gt; 1) \end{cases}
						\] 

						Which, with the master theorem, \(a = 4, b = 2, c = 1\), we get... \(\Theta(n^2)\). 
					</p>
					<p>
						Ah.
					</p>
				</div>
			</div>
			</div>

			<div class="cornell">
				<div>
					<p>Karatsuba algorithm</p>
				</div>
				<div>
					<p>
						An algorithm developed by Anatoly <b>Karatsuba</b> in 1960 allows us to do multiplication quicker than quadratic time.
					</p>
					<p>
						Divide \(x, y\) into low and high bits as in warmup : 
						\begin{align}
							\textrm{let } &m = \left\lceil{\frac{n}{2}}\right\rceil 
							&x = 2^m a + b, \; y = 2^m c + d \\
							\textrm{then } &a = \left\lfloor{\frac{x}{2^m}}\right\rfloor & b = x \mod 2^m \\
							&c = \left\lfloor{\frac{y}{2^m}}\right\rfloor & d = y \mod 2^m
						\end{align}

						\begin{align}
							xy = (2^m a + b)(2^m c + d) &= 2^{2m} ac + 2^m(bc + ad) + bd \\
							&=2^{2m} ac + 2^m(ac + bd - (a-b)(c-d)) + bd
						\end{align}

						We then have divided \(xy\) down into <i>three</i> (nontrivial) multiplication options.
					</p>
				</div>

				<div>
					<p>Implementation</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">karatsuba(\(x, y, n\)):
	if \(n = 1\):
		return \(xy\)
	else:
		calculate \(m, a, b, c, d\) as above
		\(e \longleftarrow\) karatsuba(\(a, c, m\))
		\(f \longleftarrow\) karatsuba(\(b, d, m\))
		\(g \longleftarrow\) karatsuba(\(|a-b|, |c-d|, m\))
		flip sign of \(g\) if needed 
		return \(2^{2m}e + 2^m(e + f - g) + f\)</div>
				</div>

				<div>
					<p>Running time</p>
				</div>
				<div>
					<p>
						We have the recurrence 

						\[T(n) = \begin{cases} \Theta(1) & n=1 \\ 3T(\left\lceil{\frac{n}{2}}\right\rceil + \Theta(n) & n &gt; 1 \end{cases}\]

						Via master theorem \(log_b a \approx 1.585 \), thus \(T(n) = O(n^1.585)\). 
					</p>
				</div>

				<div>
					<p>Further comments</p>
				</div>
				<div>
					<p>
						Integer multiplication, as one would think, is the base of many algorithms. Note that multiply, divide, square, and sqrt all have the same complexity.
					</p>
					<p>
						Karatsuba's algorithm is <i>still not optimal</i> - better has been done after. 
					</p>
				</div>
			</div>
		</div>

		<div class="colourband">
			<h2 id="dynprogramming">Dynamic Programming</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#dyn-1">Weighted Interval Scheduling</a></li>
				<li><a href="#dyn-2">Knapsack Problem</a></li>
				<li><a href="#dyn-3">Edit Distance and Sequence Alignment</a></li>
				<li><a href="#dyn-4">Shortest Path with Negative Weights</a></li>
			</ol>

			<div class="cornell">
				<div><p>Overview</p></div>
				<div>
					<p>
						Dynamic programming involves breaking up a problem into smaller, <b>overlapping</b> supbroblems (of polynomial size), and then combine solutions to find an answer.
					</p>
					<p>
						It's called dynamic programming for ... reasons, but a lot of dynamic programming solutions can be summed up with <i>"caching intermediate values to look up later"</i>. 
					</p>
				</div>
			</div>

			<h3 id="dyn-1">Weighted Interval Scheduling</h3>

			<div class="cornell">
				<div>
					<p>Problem</p>
				</div>
				<div>
					<p>
						A job \(j\)	starts and finishes at \(s_j, f_j\) respectively. It also has <i>weight</i> \(w_j\). Two jobs are compatible if there are no overlaps. We want the set of compatible jobs which <b>maximises weight</b>. 
					</p>
					<p class="small">
						The original interval scheduling problem is a special case of this where all weights are equal. 
					</p>
					<p>
						<b><i>Convention. </i></b>Jobs are given in ascending order of finishing time: \(f_1 \leq f_2 \leq \dots \leq f_n\). 
					</p>
					<p>
						We define parameter \(p(j)\) to be the largest index \(i &lt; j\) such that job i is compatible with job j: the job that finishes closest to \(s_j\) without overlapping. Index \(i\) starts at 1, and if a job doesn't have a compatible job before it, \(p(j) = 0\).
					</p>
				</div>
				<div>
					<p>Bellman optimality equation</p>
				</div>
				<div>
					<p>
						We can find the best possible <i>value</i> by representing this dynamic programming problem as a binary choice.
					</p>
					<p>
						Consider \(n\) subproblems of only jobs \(1, 2, \dots, j\), where \(j\) ranges from \(1 .. n\). 
					</p>
					<p>
						<b><i>Define.</i></b> \(OPT(j)\) as the max weight of any subset of compatible jobs \(i .. j\). 
					</p>
					<p>
						Our goal is thus to find \(OPT(n)\).
					</p>
					<p>
						Consider how we find \(OPT(j)\), if we already know all OPT values before \(j\). 
						<ol>
							<li>
								\(OPT(j)\) does <i>not</i> include job \(j\): \(OPT(j) = OPT(j-1)\), since removing job \(j\) does not affect optimality.
							</li>
							<li>
								\(OPT(j)\) includes job \(j\): the total weight of OPT includes \(w_j\). The rest of the jobs that are compatible cannot overlap with \(s_j\), thus the latest possible previous job is \(p(j)\).
								\[\implies OPT(j) = OPT(p(j)) + w_j\]
							</li>
						</ol>
						Whichever one of these is higher, becomes our \(OPT(j)\). 
					</p>
					<div class="blue">
						<p style="padding: 0px 0px;">
							<b><i>Definition.</i></b> The <b>Bellman Optimality Equation</b> for this problem is defined as follows:
							\[OPT(j) = \begin{cases} 0 & \textrm{if }j = 0 \\ \max(OPT(j-1), w_j + OPT(p(j))) & \textrm{if }j &gt; 0 \end{cases} \]
						</p>
					</div>
					
				</div>

				<div>
					<p>Brute-force implementation</p>
				</div>
				<div>
					<p>
						We can try program this in a recursive, "brute-force" way.
					</p>
					<div class="codediv">bruteForce(\(n, s_1 .. s_n, f_1 .. f_n\)):
	sort jobs by finish time, relabel accordingly
	compute \(p[1], p[2], \dots, p[n] \) via binary search 
	return computeOpt(\(n, p\))

computeOpt(\(j, p\)):
	if \(j = 0\):
		return 0
	else:
		return \(\max(\textrm{computeOpt}(j-1, p), w_j + \textrm{computeOpt}(p[j], p))\)</div>
				</div>

				<div>
					<p>Complexity</p>
				</div>
				<div>
					<p>
						The important part of working out running time is <code>computeOpt</code>. It may be difficult to work out, but the running time of this algorith is actually \(O(\phi^n)\) - yes, <a class="text" href="https://en.wikipedia.org/wiki/Golden_ratio">that phi</a>.
					</p>
					<p>
						The recursive approach is very slow, since the subproblems are <i>overlapping</i>, the number of recursive calls grows like the fibonacci system (which is why we get phi).
					</p>
					<p>
						This is not very good.
					</p>
				</div>

				<div>
					<p>Memoisation</p>
				</div>
				<div>
					<p>
						"Top-down dynamic programming", or memoisation, a fancy way of saying having a global array that you store previous results of OPT so you don't need to spend time computing it over and over again.
					</p>
					<p>
						Generally the global array is called \(M\), and can also store a special symbol (e.g. \(-1, \infty\)) to represent "uncalculated".
					</p>
				</div>
				<div>
					<p>Memoisation implementation</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">global \(M[] \); global \(p[] \);

memoise(\(n, s_1 .. s_n, f_1 .. f_n\)):
	sort jobs by finish time, relabel accordingly
	compute \(p[1], p[2], \dots, p[n] \) via binary search 
	\(m[0] \longleftarrow 0\)  # base case
	return mOpt(\(n\))

mOpt(\(j\)):
	if \(M[j]\) uninitialised:
	    \(M[j] \longleftarrow \max(\textrm{mOpt}(j-1), w_j + \textrm{mOpt}(p[j]))\)
	return \(M[j] \)</div>
				</div>

				<div>
					<p>Complexity </p>
				</div>
				<div>
					<p>
						<b><i>Claim.</i></b> This algorithm has time complexity \(O(n \log n)\)
					</p>
					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> 
							<ul>
								<li>Sorting is \(O(n \log n)\)</li>
								<li>Computing \(p[j] \; \forall j = O(n \log n)\)</li>
								<li>Resolving <code>mOpt</code> is \(O(n)\), since:</li>
							</ul>
							For <code>mOpt</code>, invocation is \(O(1)\), then <i>either</i> \(M[j]\) is returned immediately (\(O(1)\)), <i>or</i> \(M[j]  \) is initialised after two recursive calls.
						</p>
						<p>
							Let us have a <b>process measure</b> \(\phi =\) number of initialised entries in \(M\). Initially \(\phi = 0 \; \because \textrm{ only } M[0] = 0\), and throughout, \(\phi \leq n\). 
						</p>
						<p>
							If <code>mOpt</code> performs the <i>or</i> operation (recursive calls), we initialise \(M[j] \) and increment \(\phi\). This can be done at most \(n\) times. Thus since every time <code>mOpt</code> recurses, it makes two calls, so we have at most \(2n\) recursive calls - <b>linear time</b>. 
						</p>
						<p>
							\(\textrm{mOpt}(n) = O(n)\), thus overall memoisation is \(O(n \log n\). $$\tag*{$\Box$}$$
						</p>
					</div>
				</div>

				<div>
					<p>Finding the optimal schedule</p>
				</div>
				<div>
					<p>
						So far we have found the optimal <i>value</i>, but no actual solution. Luckily finding the solution can be done in linear time.
					</p>

					<div class="codediv">findSoln(j):
	if \(j = 0\): return \(\varnothing\).
	else if \(w_j + M[p[j]] &gt; M[j-1]\):
		return \(\{j\} \cup \textrm{findSoln}(p[j])\)
	else:
		return \(\textrm{findSoln}(j-1)\)</div>
						
					<p>
						Since there is only one recursive call, this is linear time. We can combine this with the above algorithm to find a proper optimal schedule in \(O(n \log n)\).
					</p>
				</div>
			</div>

			<h3 id="dyn-2">Knapsack Problem</h3>

			<div class="cornell">
				<div>
					<p>Problem</p>
				</div>
				<div>
					<p>
						There are \(n\) items. Item \(i\) has value \(v_i > 0\) and weights \(w_i > 0\). The value of a set of items is ìs the sum of their values. We also have a <b>knapsack</b>, which can hold a total of \(W\) weight. 
					</p>
					<p>
						<b><i>Goal.</i></b> Given a set of items and a knapsack with a set weight, <i>maximise</i> the value of items that knapsack can carry. 
					</p>
					<p>
						We assume weights and values are all integers.
					</p>
				</div>

				<div>
					<p>Dynamic programming subproblems</p>
				</div>
				<div>
					<p>
						We have two variables, \(i, w\), and so we need a dynamic progrmaming setup which allows us to split on both, and get a function \(OPT(i, w)\). 
					</p>
					<p>
						<b><i>Define.</i></b> \(OPT(i, w)\) as optimal value of knapsack with items \(1..i\) subject to weight \(w\). 
					</p>
					<p>
						Our goal is \(OPT(n, W)\).
					</p>
				</div>

				<div>
					<p>Bellman Equation</p>
				</div>
				<div>
					<p>
						When we reach item \(i\), we want to know if it's in OPT or not. So we have the following cases.
						<ol>
							<li>
								\(OPT(i, w)\) does not include \(i \implies OPT(i, w\) selects best items from \(1, 2, \dots, i-1\).
							</li>
							<li>
								\(OPT(i, w)\) includes \(i \implies\) we (a) collect value \(v_i\), (b) get a new weight limit \(w-w_i\), (c) OPT keeps on selecting according to new weight limit \(w-w_i\). 
							</li>
						</ol>

						Thus we get the equation

						\[
						OPT(i, w) = \begin{cases} 
						    0 & \textrm{if } i = 0 \\
							OPT(i-1, w) & \textrm{if }w_i > w \\
							\max(OPT(i-1, w), v_i + OPT(i-1, w-w_i)) & \textrm{otherwise}
						\end{cases}
						\]

						Note that even if item \(i\) does not exceed the weight, it might not be in the optimal anyway. 
					</p>
				</div>

				<div>
					<p>Bottom-up programming</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">global \(M\)[][]  # 1st index items, 2nd index weights 

knapsack(\(n, W, w_1 .. w_n, v_1 .. v_n\)):
	for \(w = 0 .. W\):
		\(M[0, w] \longleftarrow 0\)
	for \(i = 1 .. n\):
		for \(w = 0 .. W\):
			if \(w_i > w\):
				\(M[i][w] \longleftarrow M[i-1][w]\)
			else:
				\(M[i][w] \longleftarrow \max(M[i-1][w] , v_i = M[i-1][w-w_i])\)
	return \(M[n][W] \)</div>
					
					<p>
						Looking at the final table, we start from the bottom right <code>[n][W]</code> corner. If the value above is the same, then \(i\) for that row not chosen. If value above is less, \(i\) is chosen, take \(v_i\) away from current value and find first occurence of that in row above. 
					</p>
				</div>

				<div>
					<p>Complexity</p>
				</div>
				<div>
					<p>
						This algorithm has \(\Theta(nW)\) time and space complexity, which should be easy enough to see. 
					</p>
				</div>
			</div>

			<h3 id="dyn-3">Edit Distance and Sequence Alignment</h3>

			<div class="cornell">
				<div>
					<p>Problem</p>
				</div>
				<div>
					<p>
						Given two strings (like <code>occurrance</code> and <code>ocurrence</code>), we want to know how "similar" they are. However this is very vague. How could we measure difference? 
					</p>
					
				</div>
				<div>
					<p>Possible solution considerations</p>
				</div>
				<div>
					<p>
						We could align strings from first letter: <br>
						<code>occurrance</code><br>
						<code>ocurrence</code><br>
						This has 6 mismatches and 1 "gap" (where a letter matches onto an empty space). But what is the <i>best</i> alignment? 
					</p>
					<p>
						Perhaps we want to minimise both gaps and mismatches: <br>
						<code>occurrance</code><br>
						<code>oc urrence</code><br>
						This has one gap and one mismatch.
					</p>
					<p>
						Perhaps however we think mismatches are much, much worse than gaps: <br>
						<code>occurra nce</code><br>
						<code>oc urr ence</code><br>
						Now we have no mismatches, but three gaps.
					</p>
					<p>
						What we decide for the gap/mismatch weighting on will change based on the context said problem is in.
					</p>
				</div>

				<div>
					<p>Edit distance</p>
				</div>
				<div>
					<p>
						<b><i>Edit distance.</i></b> (Lehvenstein 1966) A measure of the "cost" of editing one string into another, with a gap penalty of \(\delta\), and a mismatch penalty for two letters \(pq\) to be \(\alpha_{pq}\). 
					</p>
					<p>
						The cost is then \(\sum \delta + \sum \alpha_{pq} \). <br>
						<code>CT GACCTACG</code><br>
						<code>CTGGACGAACG</code><br>
						has cost \(\delta + \alpha_{CG} + \alpha_{TA} \).
					</p>
				</div>

				<div>
					<p>Sequence alignment algorithm</p>
				</div>
				<div>
					<p>
						<b><i>Problem.</i></b> Given 2 strings \(x_1 x_2 \dots x_n\) and \(y_1 y_2 \dots y_n\), minimise the edit distance.
					</p>
					<p>
						<b><i>Define.</i></b> An <b>alignment</b> M is a set of ordered pairs \(x_i - y_j\) such that each character appears in at most 1 pair, and there are no crossings: <br>
						\(x_i x_j\)
					</p>
					<p>
						<b><i>Define.</i></b> the <b>cost of alignment</b> to be 
						\[
						cost(M) = \sum_{(x_i, y_j) \in M} \alpha_{x_i y_j} + \sum_{i : x_i \textrm{ unmatched}} + \sum_{i: y_i \textrm{ unmatched}}
						\]
						i.e. the mismatch penalty for two letters \(x_i y_j\) (may be 0), and the cost of all unmatched letters.
					</p>
				</div>

				<div>
					<p>Problem Structure</p>
				</div>
				<div>
					<p>
						<b><i>Define.</i></b> \(OPT(i, j)\) as the min cost of aligning <i>prefix strings</i> \(x_1 x_2 \dots x_n\) and \(y_1 y_2 \dots y_n\). Our goal is thus \(OPT(m, n)\).
					</p>
					<p>
						There are only three options for \(x_i, y_i\):
						<ol>
							<li>
								\(OPT(i, j)\) matches \(x_i\) to \(y_j\). We pay the penalty (if any) \(\alpha_{x_i y_j} \) and the minimum possible cost of \(OPT(i-1, j-1)\).
							</li>
							<li>
								\(OPT(i, j)\) leaves \(x_i\) unmatched. Then we pay gap penalty and min cost of \(OPT(i-1, j)\)
							</li>
							<li>
								\(OPT(i, j)\) leaves \(y_j\) unmatched. Then we pay gap penalty plus min cost of \(OPT(i, j-1)\)
							</li>
							Thus our bellman equation goes as follows:

							\[
							OPT(i, j) = \begin{cases} 
							    j \cdot \delta & i = 0 \\
								i \cdot \delta & j = 0 \\
								\min\begin{cases}
								    \alpha_{x_i y_j} + OPT(i-1, j-1) \\
									\delta + OPT(i-1, j) \\
									\delta + OPT(i, j-1)
								\end{cases} & \textrm{otherwise. }
							\end{cases}
							\]
						</ol>
					</p>
				</div>

				<div>
					<p>Bottom up implementation</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">global \(M\)[][]

seqAlign(\(m, n, x_1\dots x_m, y_1 \dots y_n\)):
	for \(i = 0 .. m \):
		\(M[i][0] \longleftarrow i\delta\)
	for \(j=0..m\):
		\(M[0][j] \longleftarrow j\delta\)
	for \(i = 1 .. m\):
		for \(j = 1 .. n\):
			\(M[i, j] \longleftarrow \min \begin{cases}
				\alpha_{x_i y_j} + M[i-1][j-1]\\
				\delta + M[i-1][j] \\
				\delta + M[i][j-1]
				\end{cases} \)
    return \(M[m, n] \)</div>
				</div>
				<div>
					<p>Complexity</p>
				</div>
				<div>
					<p class="side">
						<b><i>Theorem.</i></b> The algorithm solves edit distance in \(\Theta(mn)\) time.
					</p>
					<p>
						The proof is trivial and left as an exercise to the reader.
					</p>
					<p>
						We can backtrace the algorithm to find the optimal alignment itself. 
					</p>
				</div>
			</div>

			<h3 id="dyn-4">Shortest Paths with Negative Weights</h3>

			<div class="cornell">
				<div>
					<p>Problem</p>
				</div>
				<div>
					<p>
						<b><i>Problem.</i></b> Given a directed graph \(G=(V,E)\) with <b>arbitrary</b> edge lengths \(\ell_{vw} \), find the shortest path from some vertex \(s\) to \(t\), assuming said path exists.
					</p>
				</div>

				<div>
					<p>Djikstra fails</p>
				</div>
				<div>
					<p>
						Base djikstra does not work on these types of graphs. Even adding an equal constant to all edges to teporarily remove negatives, then doing Djikstra, does not work. 
					</p>
					<figure>
						<img src="./djikstra-fails.png" alt="Counterexamples to djikstra on negative and negative with positive weight">
						<figcaption>
							<i>Djikstra does not work in either case.</i> <br> In the first example, s->t is calculated as 2, but s->v->w->t is actually lower at 1. In the second example, s->t is still 2 (12 with weight), yet s->v->w->t with weight 25 becomes 1 after reweighting. 
						</figcaption>
					</figure>
				</div>

				<div>
					<p>On negative cycles</p>
				</div>
				<div>
					<p class="blue">
						<b><i>Definition.</i></b> <b>Negative cyces</b> are directed cycles where the sum of weights is less than 0. 
					</p>
					<p>
						Negative cycles makes this problem somewhat difficult. 
					</p>
					<p class="side">
						<b><i>Lemma 1.</i></b> If there exists a path \(v \leadsto t\) which contains any part of a negative cycle, the shortest path \(v \leadsto t\) does <b>not</b> exist.
					</p>
					<button class="collapsible">Proof 1...</button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> \(v \leadsto t\) is on the negative cycle \(W\). When we reach the part of the path on the negative cycle, we can loop round and round the cycle until our path weight spirals to \(-\infty\), so we have <i>no minimum weight</i>. $$\tag*{$\Box$}$$
						</p>
					</div>
					<p class="side">
						<b><i>Lemma 2.</i></b> If there are no negative cycles, \(\exists\) shortest path \(v \leadsto t\; \forall v, t\) that is <b>simple</b> (no repeat vertices).
					</p>
					<button class="collapsible">Proof 2...</button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Let us assume we have a shortest path with fewest edges. If that path contains a directed cycle \(W\), we can remove it without increasing the path length (since there are no negative cycles). $$\tag*{$\Box$}$$
						</p>
					</div>
				</div>

				<div>
					<p>Formal problems</p>
				</div>
				<div>
					<p>
						We can formulate some formal problems:
					</p>
					<p>
						<b><i>Single Destination Shortest Path.</i></b> Given a digraph \(G = (V, E)\) with edge weights \(\ell_{wv}\), no negative cycles, and a destination node \(t\), find the shortest \(v \leadsto t \; \forall v \in V\). 
					</p>
					<p>
						<b><i>Negative Cycle.</i></b> Given a digraph \(G = (V, E)\) with weights \(\ell_{wv} \), find if a negative cycle exists.
					</p>
				</div>

				<div>
					<p>Dynamic programming for shortest path</p>
				</div>
				<div>
					<p>
						<b><i>Define.</i></b> \(OPT(i, v)\) as the length of the shortest path \(v \leadsto t\) that uses \(\leq i\) edges. 
					</p>
					<p>
						Our goal is thus \(OPT(n-1, v)\)
					</p>
					<p>
						We have two cases:
						<ol>
							<li>
								\(OPT(i, v)\), the shortest path, uses strictly \(&lt; i\) edges. 
								\[OPT(i, v) = OPT(i-1, v)\]
							</li>
							<li>
								The shortest path uses exactly \(i\) edges. Then let \((v, w)\) be the first edge on that path, with weight \(\ell{vw} \). This edge <i>has</i> to be optimal (because dynamic programming).
								\[OPT(i, v) = \ell_{vw} + OPT(i-1, w)\]
								(Find path \(w \leadsto t\))
							</li>
						</ol>	
					</p>
				</div>
				<div>
					<p>Bellman equation</p>
				</div>
				<div>
					<p>
						\[
						OPT(i, v) = \begin{cases}
						0 &i=0,v=t \\
						\infty &i=0,v\neq t \\
						\min \begin{cases} OPT(i-1, v) \\ \min_{\forall (v, w) \in E}(OPT(i-1, w) + \ell_{vw}) \end{cases} &i>0 
						\end{cases}
						\]
					</p>
				</div>
				<div>
					<p>Implementation</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">global \(M\)[][]

shortestPath(\(V, E, \ell, t\)):
	foreach node \(v \in V\):
		\(M[0, v] \longleftarrow \infty\)
	\(M[0, t] \longleftarrow 0\)
	for \(i=1..n-1\):
		foreach node \(v \in V\):
			\(M[i, v] \longleftarrow M[i-1, v] \)
			foreach edge \((v, w) \in E\):
				\(M[i, v] \longleftarrow \min(M[i, v], M[i-1, w] + \ell_{vw} )\)</div>
				</div>
				<div>
					<p>Complexity</p>
				</div>
				<div>
					<p class="side">
						<b><i>Theorem 1.</i></b> Given a digraph with no negative cycles, we can get the length of the shortest path \(\forall v \in V\) in \(\Theta(mn)\) time and \(\Theta(n^2)\) space. 
					</p>
					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> The table is \(n \times n-1 = \Theta(n^2)\) for \(n\) verices. Each iteration of \(i\) examines all edges exactly once (\(m\) edges), for \(\Theta(n)\) iterations, thus we get \(\Theta(mn)\) time. 
							$$\tag*{$\Box$}$$
						</p>
					</div>
				</div>
				<div>
					<p>Finding paths</p>
				</div>
				<div>
					<p>
						This algorithm so far only finds optimal lengths. To find optimal paths, we could
					</p>
					<p>
						(Either) Maintain an array <code>successor[(i, v)]</code> that points to the next node on the shortest path \(v \leadsto t\) using \(\leq i\) edges, when we are setting M.
					</p>
					<p>
						(Or) After getting the optimal length \(M[i, v]\), consider only the edges such that \(M[i, v] = M[i-1, w] + \ell_{vw} \) Any path that satisfies this all the way to \(i=0\) that is satisfactory is shortest. 
					</p>
				</div>
				<div>
					<p>Bellman Ford Moore algorithm</p>
				</div>
				<div>
					<p>
						The <b>Bellman-Ford-Moore (BMF) algorithm</b> is an improvement on the shortest path algorithm. 
						<ul>
							<li>
								<b>Space optimisation.</b> Maintain 2 1d arrays (for linear space): \(d:d[v] \) is the length of shortest path \(v \leadsto t\) found so far; and \(\textrm{succ}: \textrm{succ}(v)\) is the next node on said \(v \leadsto t\) path. 
							</li>
							<li>
								<b>Performance optimisation.</b> If \(d[w] \) is not updated in iteration \(i-1\), there is no reason to consider other edges going into \(w\) in iteration \(i\).
							</li>
						</ul>
					</p>
				</div>
				<div>
					<p>Implementation</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">bellmanFordMoore(\(V, E, \ell, t\)):
	foreach \(v \in V\):
		\(d[v] \longleftarrow \infty\)
		\(succ[v] \longleftarrow \textrm{null}\)
	\(d[t] \longleftarrow 0\)
	for \(i = 1 .. n-1\):
		foreach \(w \in V\):
			if \(d[w]\) updated previously:
				forach edge \((v, w) \in E\):
					if \(d[v] &gt; d[w] + \ell_{vw} \):
						\(d[v] \longleftarrow d[w] + \ell_{vw} \)
						\(succ[v] \longleftarrow w\)
		if no value changed in prior loop:
			return;</div>
				</div>
				<div>
					<p>Complexity</p>
				</div>
				<div>
					<p class="side">
						<b><i>Lemma 3.</i></b> For each \(v \in V, \; d[v]\) is the length of some \(v \leadsto t\) path. 
					</p>
					<button class="collapsible">Proof 3...</button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Whenever we update \(d[v]\), it refers to an edge that goes to another vertex that will eventually go to \(t\). Domino.
							$$\tag*{$\Box$}$$
						</p>
					</div>
					<p class="side">
						<b><i>Lemma 4.</i></b> The value of \(d[v]\) cannot increase 
					</p>
					<button class="collapsible">Proof 4...</button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> It should be easy enough to see that in the algorithm we only ever try to minimise from infinity, not the other way round.
							$$\tag*{$\Box$}$$
						</p>
					</div>
					<p class="side">
						<b><i>Lemma 5.</i></b> After iteration \(i\), \(d[v] \leq\) length of the shortet path with \(\leq i\) edges. (Please note the inequality, versus the equality of the original algorithm).
					</p>
					<button class="collapsible">Proof 5...</button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Induction on \(i\): the base case \(i=0\) is just the set inital conditions.
						</p>
						<p>
							Let \(P\) be a \(v \leadsto t\) path with \(\leq i+1\) edges. Consider P with the first edge extracted: \(v \longrightarrow w \leadsto t\). Let \(w \leadsto t\) be \(P'\). 
						</p>
						<p>
							Note that \(P'\) has \(\leq i\) edges. <br>
							Let \(d^i [v] =\) value of \(d[v] \) after iteration \(i\). 
						</p>
						<p>
							Apply lemma to \(P'\) (i.e. applying inductive hypothesis): \(d^i [w] \leq \ell(P')\). <br>
							We want \(d^{i+1} [v] \leq \ell(P) \; \forall P\).
						</p>
						<p>
							Take \(d^{i+1} [v] \)
							\begin{align}
								d^{i+1} [v] &\leq d[v] \textrm{ by lemma 4} \\
								&\leq \ell_{vw} + d [w] \textrm{ considering edge } vw \at \textrm{ iteration } i+1 \\
								&\leq \ell_{vw} + d^i[w] \\
								&\leq \ell_{vw} + \ell(P') \textrm{ inductive hyp.} \\
								&\leq \ell(P)
							\end{align}
							$$\tag*{$\Box$}$$
						</p>
					</div>
					<p class="side">
						<b><i>Theorem 2.</i></b> Assuming no negative cycles, BFM solves in \(O(mn)\)* time and \(\Theta(n)\) space. 
					</p>
					
					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Use <i>lemma 2</i> (scroll up), then <i>lemma 5</i>, then <i>lemma 3</i>. 
						</p>
					</div>

					<p>
						* <b>Note:</b> \(O(mn)\) not \(\Theta(mn)\). BFM is typically faster than \(\Theta(mn)\). 
					</p>
					<p>
						Edge \(\langle v, w \rangle\) is considered in pass \(i+1 \) <i>only if</i> \(d[w] \) is updated in pass \(i\). If the shortest path has \(k\) edges, the algorithm will find it after \(\leq k\) passes. 
					</p>
				</div>

				<div>
					<p>Getting the path</p>
				</div>
				<div>
					<p>
						<b><i>Claim.</i></b> Throughout the BFM algorithm, following \(succ[v] \) pointers gives directed path from \(v \leadsto t\) of length \(d[v]\). 
					</p>
					<button class="collapsible">Counterexample... </button>
					<div class="ccontent">
						<p>
							<b><i>Counterexample.</i></b> This is false - the length of \(succ[v]\) path may be <i>shorter</i> than \(d[v]\). 
							<figure>
								<img src="./bfm-counter-1.png" alt="Counterexample graph 1" style="max-width: 400px;">
							</figure>
							Consider the graph, with nodes \(t, 1, 2, 3\). The state of the graph is shown after the algorithm is ran on \(t, 1, 2\).
						</p>
						<p>
							Now we consider 3, we scan incoming edges: edge from 1, \(d[1] = 10\), but \(\ell_{\langle 1, 3\rangle} = 1\), thus \(1 \leadsto t\) has a length of 2.
						</p>
						<p>
							Update \(d[1] \longleftarrow 2, succ[1] \longleftarrow 3\). We are now done.
						</p>
						<p>
							Look at node 2. \(d[2] = 20\) but the path \(\langle 2 1 3 t \rangle\) is of length 12, which is shorter than \(d\). 
						</p>
					</div>
				</div>
				<div>
					<p>Successor graphs</p>
				</div>
				<div>
					<p>
						If the graph has negative cycles, BFM may produce a successor graph which has a cycle. 
					</p>
					<figure>
						<img src="./bfm-neg-cycle.png" alt="negative cycle graph" style="max-width: 300px;">
						<figcaption><i>This graph has a negative cycle</i></figcaption>
					</figure>
					<p>
						Suppose we've considered nodes \(t, 1, 2, 3\), but not 4 (state shown). Consider the incoming edge \(\langle 1, 4 \rangle\). \(d[1] = 5\), but \(\ell_{\langle 1, 4\rangle} = -8, \; d[4] = 11, \; 11-8 &lt; 5\), so we update \(d[1] \longleftarrow 3, \; succ[1] \longleftarrow 4\). 
					</p>
					<p>
						Now the successor edges form a cycle, and none will ever reach \(t\). 
					</p>
				</div>
				<div>
					<p>BFM and negative cycles</p>
				</div>
				<div>
					<p class="side">
						<b><i>Lemma 6.</i></b> Any directed cycle W in a successor graph produced by BFM is a negative cycle. 
					</p>
					<button class="collapsible">Proof 6...</button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> If \(succ[v] = w\), we must have \(d[v] \geq d[w] + \ell_{vw} \)
						</p>
						<p><i>
							\(LHS = RHS\) when \(succ[v]\) is set to \(w\). \(d[w]\) however can later decrease, making RHS lower, whereas \(d[v]\) can only decrease if \(succ[v] \) is reset. 
						</i></p>
						<p>
							Let \(\langle v_1, v_2, \dots, v_k, v_1 \rangle \) be the sequence of nodes in cycle W. Assume edge \(\langle v_k, v_1 \rangle\) be the last edge added in W. 
						</p>
						<p>
							Just before this, we have
							\begin{align}
								d(v_1) &\geq d(v_2) + \ell_{v_1 v_2} \\
								d(v_2) &\geq d(v_3) + \ell_{v_2 v_3} \\
								&\dots \\
								d(v_{k-1}) &\geq d(v_k) + \ell_{v_{k-1} v_k} \\
								d(v_k) & &gt; d(v_1) + \ell_{v_k v_1} \textrm{ (strict inequality)}
							\end{align}
							Since this is <i>before</i> \(d(v_k)\) is updated. \(\sum LHS &gt; \sum RHS\) (because of the one strict inequality), thus \(\sum d(LHS) &gt; \sum d(RHS) + \ell_{RHS} \) - note that \(d[LHS], d[RHS]\) are the same vertices, thus
							\begin{align}
								& 0 &gt; \ell_{RHS} \\
								\Longleftrightarrow & 0 &gt;\ell_{v_1 v_2} + \ell_{v_2 v_3} + \dots + \ell_{v_k v_1}
							\end{align}
							Thus we have a negative cycle. $$\tag*{$\Box$}$$
						</p>
					</div>
				</div>
				<div>
					<p>Correctness</p>
				</div>
				<div>
					<p class="side">
						<b><i>Theorem 3.</i></b> If there are no negative cycles, BFM algorithm will find the shortest paths. 
					</p>
					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> If <i>(lemma 6)</i> successor graph has no cycles, we have a tree of shortest paths. 
						</p>
						<p>
							Following successors from \(v\) gets a path to \(t\). 
						</p>
						<p>
							Let \(P = \langle v= v_1, v_2, \dots, v_t=t \rangle\), i.e. the nodes along said path. 
						</p>
						<p>
							On algorithm halt, \(succ[v] = w \implies d[v] = d[w] + \ell_{vw} \), since the LHS and RHS equal when this is set, and \(d[w]\) never updated again as the algorithm halts. Thus,
							\begin{align}
								d(v_1) &= d(v_2) + \ell_{v_1 v_2} \\
								d(v_2) &= d(v_3) + \ell_{v_2 v_3} \\
								&\dots \\
								d(v_{t-1}) &= d(v_t) + \ell_{v_{t-1} + v_t} \;\; (d(v_t) = d(t) = 0)
							\end{align}
							\(\sum LHS = \sum RHS \implies d[v] = d[t] + \ell_{v_1 v_2} + \ell_{v_2 v_3} + \dots + \ell_{v_{t-1} v_t}\)
						</p>
						<p>
							Which is the max length of any shortest path by Theorem 2 (somehow). $$\tag*{$\Box$}$$
						</p>
					</div>

					<p class="small">
						Better algorithms than BFM have been developed in more recent years. 
					</p>
				</div>
			</div>



		</div>

		<div class="colourband">
			<h2 id="intractibility">Intractibility</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#int-1">Poly-Time Reductions</a></li>
				<li><a href="#int-2">Numerical Problems</a></li>
				<li><a href="#int-3">P vs NP</a></li>
				<li><a href="#int-4">NP Completeness</a></li>
			</ol>

			<p>
				We will now look at "hard" problems, things that can't be solved in polynomial time, and at "polynomial" reductions, which are a way to argue that one problem is no harder than another.
			</p>
			<p>
				A <b>poly-time</b> algorithm is solvable in \(O(n^x)\) time, and is considered <i>easy</i>. Granted, that doesn't mean they're practical, but since constants <i>tend</i> to be small, poly-time algorithms still scale quite well.
			</p>

			<h3 id="int-1">Poly-Time Reductions</h3>

			<p class="blue">
				<b><i>Definition.</i></b> Problem X is <b>poly-time reducible</b> to problem Y if X can be solved in a polynomial number of steps plus a polynomial number of calls to problem Y (which is the <b>oracle</b> or <b>black box</b>). 
			</p>
			<figure>
				<img src="./poly-reduction.png" alt="" style="max-width: 630px;">
				<figcaption>Polynomial-time reduction: There are poly-time pre- and post-processing steps and calls to Y</figcaption>
			</figure>
			<p>
				This is denoted \(X \leq_p Y\): "X is <b>no harder</b> than Y".
			</p>
			<ul>
				<li>If \(X \leq_p Y\) and Y is solvable in poly-time, then X is also.</li>
				<li>If \(X \leq_p Y\) and X is unsolvable in poly-time, Y cannot be either.</li>
				<li>If \(X \leq_p Y \land Y \leq_p X\), then denote \(X \equiv_p Y\) and X is solvable in polytime <b>if and only if</b> Y is.</li>
			</ul>

			<h4>Independent Set and Vertex Cover</h4>

			<p class="side">
				<b><i>Problem.</i></b> <b>Independent Set</b>: Given a graph \(G = (V,E)\) and integer \(k\), is there a subset \(\geq k\) vertices : no vertices in the subset are adjacent?
			</p>

			<p class="side">
				<b><i>Problem.</i></b> <b>Vertex Cover</b>: Given graph and integer \(k\), is there a subset \(\leq k\) vertices : each edge is adjacent to at least a vertex in the subset?
			</p>

			<p class="side">
				<b><i>Theorem 1.</i></b> Independent set \(IS \equiv_p VC\) vertex cover.
			</p>

			<button class="collapsible active">Proof 1... </button>
			<div class="ccontent" style="display: block;">
				<p>
					<b><i>Proof.</i></b> Looking at the graph, it seems the vertex cover is just whatever is not the independent set. Thus we want: S is an indep. set of size \(k \iff\) \(V \setminus S\) is the vertex cover of size \(n-k\).
				</p>
				<img src="./vertex-cover.png" alt="" align="right">
				<p>
					Our reduction is then: <br> 
					&emsp;Input: \(G,k\) <br>
					&emsp;Output: result of calling black box with \(G, n-k\)
				</p>
				<p>
					Proving \(\implies\):
				<ul>
					<li>Let S be indep. set of size \(k\)</li>
					<li>\(V \setminus S\) has size \(n-k\) (it is the complement)</li>
					<li>Consider any edge \((u,v) \in E\)</li>
					<li>Because S is independent, either \(u \not \in S\) or \(v \not \in S\) or both \(\implies u \in V \setminus S\) or \(v \in V \setminus S\).</li>
					<li>Thus \(V \setminus S\) covers all \((u,v)\). \(\triangleright\)</li>
				</ul>
				</p>
				<p>
					Proving \(\impliedby\):
				<ul>
					<li>Let \(V \setminus S\) be a vertex cover of size \(n-k\)</li>
					<li>S is then size \(k\)</li>
					<li>Consider any edge \((u,v) \in E\)</li>
					<li>\(V \setminus S\) is a vertex cover \(\implies\) etiher \(u \in V \setminus S \lor v \in V \setminus S\) or both,</li>
					<li>Meaning \(v \not\in S \lor u \not\in S\),</li>
					<li>Thus every edge has at most 1 vertex in S, and S is an independent set.</li>
				</ul>
				$$\tag*{$\Box$}$$
				</p>
			</div>

			<h4>Set Cover</h4>

			<p class="side">
				<b><i>Problem.</i></b> <b>Set Cover</b>: given set U, and a collection S of subset s of U, and an integer \(k\), are there \(\leq k\) of these subsets, whose union \(= U\)?
			</p>

			<button class="collapsible">Example... </button>
			<div class="ccontent">
				<p>
					<b><i>Example</i></b>. Take \(U = \{1,2,3,4,5,6,7\};\; k=2\)
					\begin{align}
						S : &\{3,7\} &\{2,4\} \\
						&\{3,4,5,6\} &\{5\} \\
						&\{1\} &\{1,2,6,7\}
					\end{align}

					We can see that yes, there is, since (reading horizontally) the third and last one union to make U.
				</p>
				

			</div>

			<p class="side">
				<b><i>Theorem 2.</i></b> Vertex cover \(VC \leq_p SC\) set cover.
			</p>
			<button class="collapsible active">Proof 2... </button>
			<div class="ccontent" style="display: block;">
				<p>
					<b><i>Proof.</i></b> Given VC instance \(G=(V,E), k\) we can construct an SC instance \(U, S, k\) that has set cover size \(k\) <b>iff</b> G has vertex cover size \(k\).
				</p>
				<p>
					Let \(U=E\). Let \(S = \{S_v | \forall v \in V\}\), where \(S_v = \{e \in E | e \textrm{ incident to } v\}\).	
				</p>
				<p>
					<b><i>Lemma.</i></b> \(G=(V,E)\) contains a VC of size \(k \iff (U,S,k)\) has a SC size \(k\).
				</p>
				<p>
					Proving \(\implies\):
					<ul>
						<li>Let \(X \subseteq V\) be the vertex cover size \(k\) in G.</li>
						<li>Then \(Y = \{S_v | v \in X\} \) is a set cover size \(k\), because every edge is covered by at least 1 vertex. \(\triangleright\)</li>
					</ul>
					Proving \(\impliedby\):
					<ul>
						<li>Let \(Y \subseteq S\) be a set cover size \(k\).</li>
						<li>Then \(X = \{v | S_v \in Y\} \) is a vertex cover size \(k\) in G,</li>
						<li>because if the collection of sets (of edges) unions to a whole, then all edges must be covered.</li>
					</ul>
					$$\tag*{$\Box$}$$
				</p>
			</div>

			<h4>Constraint Satisfaction</h4>

			<p>
				<span class="grey">AI flashbacks</span> (Boolean) constraint satisfaction satisifies a CSP with boolean values. We define 
				<ul>
					<li>A <b>literal</b> \(x, \overline{x} (\lnot x)\),</li>
					<li>A <b>clause</b> as a disjunction of literals,</li>
					<li>A <b>Conjunctive Normal Form</b> \(\Phi\),</li>
					<li>And a CSP, or <b>SAT</b>, which is a CNF and the question: is it satisfiable?</li>
				</ul>
				<b>3-SAT</b> is a SAT where clauses have 3 literals each, and is a hard problem. Many things are reducible to 3SAT
			</p>
			
			<p class="side">
				<b><i>Theorem 3.</i></b> \(3SAT \leq_p IS\).
			</p>

			<button class="collapsible active">Proof 3...</button>
			<div class="ccontent" style="display: block;">
				<p>
					<b><i>Proof.</i></b> Given \(\Phi\) of 3SAT, construct instance \((G,k)\) of IS : there is a set size \(k = |\Phi | \iff \Phi\) satisfiable. 
				</p>
				<p>
					To construct G,
					<ul>
						<li>G has 3 nodes per clause.</li>
						<li>Connect all literals in a clause.</li>
						<li>Connect a literal to its negative literals.</li>
					</ul>
				</p>
				<p>
					<b><i>Lemma.</i></b> \(\Phi\) satisfiable <b>iff</b> G has independent set size \(k=| \Phi |\).
				</p>
				<p>
					Proving \(\implies\)
				<ul>
					<li>Consider any satisfying configuration of \(\Phi\) (select 1 literal from each triangle).</li>
					<li>This is an independent set size \(k = |\Phi |\), because</li>
					<ul>
						<li>Since vertices are in distinct triangles, the only inter-triangle connection is from a literal to its negative.</li>
						<li>It is impossible to have \(x = T\) and \(\lnot x = T\).</li>
					</ul>
					<li>So no vertices touch. \(\triangleright\)</li>
				</ul>
					Proving \(\impliedby\)
				<ul>
					<li>Let S be an independent set of size \(k\)</li>
					<li>S must have 1 node per triangle, since there are \(k\) triangles, and a triangle is \(K_3\) so only one point can be picked,</li>
					<li>Set all literals in S to true, this is the satisfying assignment of \(\Phi\).</li>
				</ul>
					$$\tag*{$\Box$}$$
				</p>
			</div>

			<h4>Reduction Strategies</h4>

			<ol>
				<li>Finding the <b>equivalence of problems</b></li>
				<li>Showing one problem is a <b>special case</b> whilst the other is a more <b>general case</b></li>
				<li><b>Encoding</b> one problem into another problem with a special setup so it works</li>
			</ol>

			<p class="blue">
				<b><i>Transitivity</i></b>. If \(X \leq_p Y\) and \(Y \leq_p Z\) then \(X \leq_p Z\), so can also reduce to an intermediate reducible.
			</p>

			<p>
				\(3SAT \leq_p IS \leq_p VC \leq_p SC\). Since 3SAT is hard, the others must be hard too.
			</p>

			<h3 id="int-2">Numerical Problems</h3>

			<h4>Subset Sum</h4>

			<p class="side">
				<b><i>Problem.</i></b> Given \(n\) natural numbers \(w_1 .. w_n\), along with another \(W \in \mathbb{N}\), is there a subset of numbers that adds to exactly W?
			</p>
			<p>
				<b>Note Well</b> with arithmetic problems, inputs are in <b>binary</b>, thus a poly-time algorithm must be <b>polynomial in binary</b>.
			</p>
			<p class="side">
				<b><i>Theorem 4.</i></b> 3SAT \(\leq_p\) Subset-Sum.
			</p>
			<button class="collapsible active">Proof 3... </button>
			<div class="ccontent" style="display: block;">
				<p>
					<b><i>Proof.</i></b> Given instance of 3SAT \(\Phi\), construct instance of subset sum that has a solution \(\iff \Phi\) satisfiable.
				</p>
				<p>
					The construction goes: given a 3SAT instance with \(n\) variables and \(k\) clauses, form \(2n+2k\) decimal integers, each with \(n+k\) digits:
					<ul>
						<li>1 digit per variable \(x_i\), 1 digit per clause \(c_i\)</li>
						<li>Two numbers per var \(x_i\)</li>
						<li>Two numbers per clause \(c_j\)</li>
						<li>Sum of each \(x_i\) digits is 1, and each \(c_j\) digits is 4</li>
					</ul>
					The <b>key property</b> of this is that no carries are possible when we sum.
				</p>
				<p>
					Create a table based on the following (see example):
				</p>
				<ul>
					<li>For the variable/variable boxes, assign 1 where the variable matches to the literal (\(x\) matches to \(x, \lnot x\))</li>
					<li>For the variable/clause boxes, assign 1 where the variable appears positively \(x\) and 0 where negatively \(\lnot x\)</li>
					<li>For the clause/variable boxes, assign 0</li>
					<li>For the clause/clause boxes, where the clauses match up assign 1 and 2 respectively "dummy integers"</li>
				</ul>
				<button class="collapsible nul">Example Deconstruction.</button>
				<div class="ccontent cnul">
					<p>
						Let
						\begin{align}
							c_1 &= \lnot x_1 \lor x_2 \lor x_3 \\
							c_2 &= x_1 \lor \lnot x_2 \lor x_3 \\
							c_3 &= \lnot x_1 \lor \lnot x_2 \lor \lnot x_3
						\end{align}
						Thus we make a table (get ready for this)
					</p>
					<table>
						<tr>
							<th></th>
							<th>\(x_1\)</th>
							<th>\(x_2\)</th>
							<th>\(x_3\)</th>
							<th>\(c_1\)</th>
							<th>\(c_2\)</th>
							<th>\(c_3\)</th>
							<th>Number</th>
						</tr>
						<tr>
							<td>\(x_1\)</td>
							<td>1</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>0</td>
							<td>100 010</td>
						</tr>
						<tr>
							<td>\(\lnot x_1\)</td>
							<td>1</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>0</td>
							<td>1</td>
							<td>100 101</td>
						</tr>
						<tr>
							<td>\(x_2\)</td>
							<td>0</td>
							<td>1</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>1</td>
							<td>10 100</td>
						</tr>
						<tr>
							<td>\(\lnot x_2\)</td>
							<td>0</td>
							<td>1</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>1</td>
							<td>10 011</td>
						</tr>
						<tr>
							<td>\(x_3\)</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>1</td>
							<td>1</td>
							<td>0</td>
							<td>1110</td>
						</tr>
						<tr>
							<td>\(\lnot x_3\)</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>1001</td>
						</tr>
						<tr>
							<td rowspan="2">\(c_1\)</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>0</td>
							<td>0</td>
							<td>100</td>
						</tr>
						<tr>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>2</td>
							<td>0</td>
							<td>0</td>
							<td>200</td>
						</tr>
						<tr>
							<td rowspan="2">\(c_2\)</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>0</td>
							<td>10</td>
						</tr>
						<tr>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>2</td>
							<td>0</td>
							<td>20</td>
						</tr>
						<tr>
							<td rowspan="2">\(c_3\)</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>1</td>
							<td>1</td>
						</tr>
						<tr>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>0</td>
							<td>2</td>
							<td>2</td>
						</tr>
						<tr>
							<td>\(W\)</td>
							<td>1</td>
							<td>1</td>
							<td>1</td>
							<td>4</td>
							<td>4</td>
							<td>4</td>
							<td>111 444</td>
						</tr>
					</table>

					<hr>
				</div>
				<p>
					<b><i>Lemma.</i></b> \(\Phi\) satisfiable \(\iff \exists\)subset that sums to W.
				</p>
				<p>
					Proving \(\implies\)
				<ul>
					<li>Suppose 3SAT has satisfying assignment \(x* \).</li>
					<li>If \(x_i* = T\), select integer in row \(x_i\), else select integer in row \(\lnot x_i\).</li>
					<li>Each of the \(x_i\) digits will sum to 1 (since each column will only have one 1 digit and all 0s)</li>
					<li>Since \(\Phi\) satisfiable, each \(c_j\) digit will sum to at least 1 (from either the \(x_i\) or \(\lnot x_i\) rows), and at most 3 (from all 3 vars).</li>
					<li>Select dummy integers 1, or 2, or both (for 3), to ensure it makes 4 at the end. \(\triangleright\)</li>
				</ul>
					Proving \(\impliedby\)
				<ul>
					<li>Suppose there exists a subset \(S* \) that sums to W for the configuration given.</li>
					<li>Digit \(x_i\) forces \(S* \) to select either var \(x_i\) or \(\lnot x_i\), but not both.</li>
					<li>Digit \(c_j\) forces \(S* \) to select at least one literal per clause (otherwise dummy integers don't sum),</li>
					<li>This satisfies 3SAT.</li>
				</ul>
				$$\tag*{$\Box$}$$
				</p>
			</div>
			<p>
				Subset-sum reduces to <b>knapsack problem</b>.
			</p>

			<h3 id="int-3">P vs NP</h3>

			<!-- <p>
				<b><i>Definition.</i></b> <b>P</b> is the class of all problems solveable in polynomial time. 
			</p>
			<p>
				We can further restrict to a <b>decision problem</b>: problem X is a set of strings (of all true instances). Given a single string \(s\), algorithm A solves problem X \(\iff A(s) = \begin{cases} T &\textrm{if }s \in X \\ F &\textrm{if }s \not \in X \end{cases} \).
			</p>
			<p>
				<b><i>Definition.</i></b> Algorithm A is <b>poly time</b> if \(\forall S,\, A(s)\) terminates in \(\leq p(|s|)\) steps, where \(p(x)\) is a polynomial function.
			</p> -->
			<p class="blue">
				<b><i>Definition.</i></b> <b>P</b> is the set of decision problems where there exists a polynomial time algorithm to solve the problem.
			</p>
			<p>
				This can be thought of as a <b>decision problem</b>: problem X is a set of strings (of all true instances). Given a single string \(s\), algorithm A solves problem X \(\iff A(s) = \begin{cases} T &\textrm{if }s \in X \\ F &\textrm{if }s \not \in X \end{cases} \). A is polynomial if A solves in \(p(|s|)\) time, where \(p(x)\) is a polynomial algorithm.
			</p>

			<div class="blue">
				<p><b><i>Definition.</i></b> <b>NP</b> is the set of all problems, where there is a poly-time <b>certifier</b> which can <b>verify</b> a solution is true, but cannot be <b>solved</b> in poly-time.</p>
				<ul>
					<li>Let \(C(s, t)\) be the poly-time certifier, where</li>
					<li>\(s\) is the instance to check, and \(t\) is a <b>certificate</b> of poly-size: \(|t| \leq p(|s|)\).</li>
				</ul>
			</div>

			<p>
				<b>3SAT</b> is in NP:
				<ul>
					<li>Let \(s = \Phi\), and \(t =\) an assignment of truth values.</li>
					<li>The certifier checks that each clause in \(\Phi\) has at least 1 true literal, which is clearly polynomial.</li>
				</ul>
			</p>
			<p>
				<b>Hamiltonian Path</b> is also in NP:
				<ul>
					<li>For a graph, let the certifier be a list of nodes. </li>
					<li>The certifier checks that each node in the path is unique, and there is an arc between two adjacent nodes.</li>
				</ul>
			</p>

			<p class="blue">
				<b><i>Definition.</i></b> There also exists <b>EXP</b> (EXPTIME), the set of all problems solvable in exponential time.
			</p>
			<p>
				\(P \subseteq NP\) and \(NP \subseteq EXP\), <b>but</b> \(P \subset EXP\), thus \(P \neq NP\) or \(NP \neq EXP\). (Millenium problem: P vs NP)
			</p>

			<h3 id="int-4">NP Completeness</h3>

			<p class="blue">
				<b><i>Definition.</i></b> A problem Y is <b>NP-Complete</b> if \(Y \in NP\), and for <b>every other problem</b> \(X \in NP\), \(X \leq_p Y\). Y can also be referred to as <b>NP-hard</b>.
			</p>

			<p>
				To solve P vs NP, finding a poly algorithm for any NP-complete algorithm is sufficient (\(P = NP\))... or prove that this cannot be done (\(P \neq NP\)).
			</p>

			<p>
				It is proven that <b>SAT is NP-Complete</b>. 
			</p>

			<div class="side">
				<p>
					To prove Y is NP-complete:
				</p>
				<ol>
					<li>Show \(Y \in NP\)</li>
					<li>Choose an NP-complete problem X (usually some SAT)</li>
					<li>Prove X is reducible to Y: \(X \leq_p Y\)</li>
				</ol>
			</div>

			<p>
				Incidentally then, we also know Vertex Cover, Independent Set, Set Cover, Subset Sum, Knapsack Problem* are all NP-hard. 
			</p>
			<p>
				*The <i>decision problem</i> of knapsack: given these items can we make a value V without exceeding weight W? is NP-hard. The optimisation problem is not, and has a dynamic programming solution.
			</p>
		</div>

	</div>

	<div class="colourband">
		<h2 id="networks">Network Flows</h2>
	</div>

	<div class="cbox">
		<p>
			As quick as possible then:
		</p>

		<ol>
			<li><a href="#net-1">Flow Networks</a></li>
			<li><a href="#net-2">Min Cut Max Flow</a></li>
			<li><a href="#net-3">Bipartite Matching</a></li>
		</ol>

		<h3 id="net-1">Flow Networks</h3>

		<p class="blue">
			<b><i>Definition.</i></b> A flow network is a <b>directed graph</b> modelled as a tuple \(G = (V,E,s,t,c)\), where \(s, t\) are the <b>source</b> and <b>sink</b> nodes respectively, and capacity is an array such that \(c(e) \geq 0\) is the capacity of edge \(e\).
		</p>

		<p class="blue">
			<b><i>Definition.</i></b> An <b>st-flow</b> \(f\) is a function (i.e. it assigns a number to an edge) that satisfies
			\begin{align}
				&\forall e \in E &0 \leq f(e) \leq c(e) \\
				&\forall v \in V \setminus \{s,t\} &\sum_{\textrm{into }v} f(e) = \sum_{\textrm{out of } v}f(e)
			\end{align} 
			Which in english are (1) <b>flow along edge not greater than cap</b> and (2) <b>flow in = flow out</b>
		</p>

		<p class="blue">
			<b><i>Definition.</i></b> The <b>value</b> of a flow \(val(f) = \sum_{\textrm{into }s} f(e) - \sum_{\textrm{out of } s}f(e)\): the <b>net flow</b> of the source.
		</p>

		<p class="side">
			<b><i>Main Problem.</i></b> Find the flow of maximum value.
		</p>

		<p>
			The main method to solve this is the <b>Ford-Fulkerson Algorithm</b>, which is taught in the Discrete section of A Level Further Maths. It relies on the idea of <b>augmenting flows</b>. 
		</p>
		<p>
			The idea is to have a <b>residual graph \(G_f\)</b>, which is the backwards graph and has values equal to \(c(e) - f(e)\) for the current flow of an edge. Then, go through the graph repeatedly, finding a passable flow along the arrows (using the backwards arrows if needed) until \(s\) and \(t\) are partitioned into two separate <b>cutsets</b>, with the cut going through all saturated (\(f(e) = c(e)\)) edges.
		</p>
		<p>
			
			<a href="https://www.youtube.com/embed/oWjXF_SztWI">This seems like a decent enough video.</a>
		</p>
		<button class="collapsible">Ford Fulkerson Algorithm... </button>
		<div class="ccontent">
			<p>
				See the pseuduocde for <code>FORD-FULKERSON</code>, and the helper function <code>AUGMENT</code>
			</p>
			<p>
				Note the <b>bottleneck</b>, as name implies, is the least roomy edge on the path.
			</p>
			<div class="codediv">def AUGMENT(\(f,c,P\)):
	\(\delta \longleftarrow\) bottleneck of augmenting path \(P\)
	for each edge \(e \in P\):
		if \(e \in E\):  # right way edge
			\(f(e) \longleftarrow f(e) + \delta\)
		else:  # residual edge
			\(f(e^{\textrm{reverse}) \longleftarrow f(e^{\textrm{reverse}) - \delta\)
	return \(f\)</div>
			<p></p>
			<div class="codediv">def FORD_FULKERSON(\(G\)):
	for each edge \(e \in E\):
		\(f(e) \longleftarrow 0\)
	\(G_f \longleftarrow\) residual network of \(G\) with respect to \(f\)
	while exists \(s \leadsto t\) path \(P\) in \(G_f\):
		\(f \longleftarrow\) AUGMENT(\(f,c,P\))
		update \(G_f\)
	return \(f\)</div>
		</div>

		<p>
			The idea of cuts is very important:
		</p>

		<h3 id="net-2">Min Cut, Max Flow</h3>

		<p class="blue">
			<b><i>Definition.</i></b> an <b>st-cut</b> is a partition \((A,B)\) where \(s \in A,\; t \in B\). The <b>capacity</b> of this cut is the sum of the capacities of the edges \(cap(A,B) = \sum_{e\textrm{ leaving }A}c(e)\).
		</p>
		

		<p class="side">
			<b><i>Lemma 1.</i> (Flow Value)</b> let \(f\) be a flow and \((A,B)\) be an st-cut. The value of flow \(f\) is the net flow across the cut:
			\[val(f) = \sum_{e \textrm{ out of }A} f(e) - \sum_{e\textrm{ into }A}f(e).\]
		</p>
		<p>
			Proofs are omitted at this current stage for speed.
		</p>

		<p>
			As a corollary, <b>max flow = min cut</b>. So, find the min cut to find the max flow. 
		</p>
		
		<p class="side">
			<b><i>Theorem 1.</i> (Augmenting Path Thm.)</b> Flow \(f\) is a max flow <b>if and only if</b> there are no available augmenting paths. <span class="grey">This proves correctness of the FF algorithm</span>.
		</p>

		<p class="side">
			<b><i>Theorem 2.</i></b> Given a max flow \(f\), we can find the min cut in \(O(m)\) time (\(m\) edges).
		</p>

		<h4>Running Time of Ford Fulkerson</h4>

		<p>
			<b>Assume</b> that all capacities are positive integers between 1 and some \(C\).
		</p>
		<p>
			<b>Define</b> the <b>integrality invariant</b>: that throughout FF, every edge's flow and residual capacity will remain integers. Then,
		</p>
		<p class="side">
			<b><i>Theorem 3.</i></b> Ford Fulkerson terminates in at most \(val(f*)\) iterations (\(f*\) means max flow). And also,
			\[val(f*) \leq nC.\]
		</p>
		<p>
			This leads to the corollary that the running time of FF is \(O(mnC)\), because FF finds the max flow value, from which we can use DFS/BFS to find the min cut.
		</p>
		<p class="side">
			<b><i>Theorem 4.</i></b> If we assume the integrality constraint, we can guarantee that our max flow is also integral.
		</p>
		<p>
			Note that the generic FF is <b>not necessarily polynomial</b>, for FF to be poly time, it must be poly on \(m, n, \log_2 C\) (C is a <b>binary</b> value to the computer).
		</p>
		<p>
			You can engineer graphs that absolutely break a generic, non-deterministic FF algorithm. Suppose
			<table>
				<tr>
					<th>Vertices</th>
					<td>s</td>
					<td>v</td>
					<td>w</td>
					<td>t</td>
				</tr>
			</table>
			<table>
				<tr>
					<th>Edges</th>
					<td>(s,v)</td>
					<td>(s,w)</td>
					<td>(v,w)</td>
					<td>(v,t)</td>
					<td>(w,t)</td>
				</tr>
				<tr>
					<th>Capacities</th>
					<td>C</td>
					<td>C</td>
					<td>1</td>
					<td>C</td>
					<td>C</td>
				</tr>
			</table>
			This will <i>royally </i>████ with a PC randomly picking paths, because it could forever pick the path \((s,v,w,t)\) and increment by 1 each time to some stupidly large number \(C\). Now as humans we can easily see past this, but PCs aren't humans ... yet at least. 
		</p>
		<p>
			Thus, in reality we need some way of choosing path good. 
		</p>

		<h4>Capacity Scaling</h4>

		<p>
			The idea of capacity-scaling is to choose a path with a "large enough" capacity, i.e. have a <b>scaling value</b> \(\Delta\) and only pick paths with a bottleneck larger than it. 
		</p>
		<p>
			Alter the algorithm so we have a \(G_f(\Delta)\) be the <b>subgraph</b> of \(G_f\) with all edges lower than Delta removed. Find augmenting paths. Halve Delta. Rinse and repeat.
		</p>

		<button class="collapsible">Capacity Scaling... </button>
		<div class="ccontent">
			<div class="codediv">def CAPCITY_SCALING(\(G\)):
	for each edge \(e \in E\):
		\(f(e) \longleftarrow 0\)
	\(\Delta \longleftarrow\) largest power of 2 that is \(\leq C\)

	while \(\Delta \geq 1\):
		\(G_f(\Delta) \longleftarrow\) as described above
		while exists \(s \leadsto t\) path \(P\) in \(G_f(\Delta)\):
			\(f \longleftarrow\) AUGMENT(\(f,c,P)\))
			update \(G_f(\Delta)\)
		\(\Delta \longleftarrow \frac{\Delta}{2}\)
	return \(f\)</div>
		</div>

		<p class="side">
			<b><i>Lemma 2.</i></b> There are \(1 + \left\lfloor{\log_2 C}\right\rfloor \) scaling phases.
		</p>
		<p class="side">
			<b><i>Lemma 3.</i></b> There are \(\leq 2m\) augmentations every scaling phase.
		</p>
		<p class="side">
			<b><i>Theorem 5.</i></b> Thus from the above, capacity scaling has a time complexity of \(O(m^2 \log C\).
		</p>
		<p class="side">
			<b><i>Lemma 4.</i></b> Let \(f\) be a flow at the <b>end</b> of a delta-scaling phase,
			\[\textrm{max flow } \leq val(f) + m\Delta\]
			\[\textrm{or }val(f) \geq val(f*) - m\Delta.\]
		</p>

		<h3 id="net-3">Bipartite Matching</h3>

		<p>
			This section looks at bipartite graphs. Bipartite graphs can be defined \(G(L \cup R, E)\) with a distinct left and right side.
		</p>

		<h4>Simple Matching</h4>

		<p class="blue">
			<b><i>Definition.</i></b> A <b>matching</b> in a graph \(G(V,E)\) is a set of edges \(M \subseteq E\) where each node appears in at most 1 edge. 
		</p>
		<p class="blue">
			<b><i>Definition.</i></b> A <b>bipartite matching</b> is a matching in a bipartite graph, and we want to find the <b>max cardinality (largest) matching</b>.
		</p>
		<p>
			The matching problem can actually be solved with (reduced to) flows. Given a bipartite graph \(G(L \cup R, E)\):
		</p>
		<ul>
			<li>Create a digraph \(G' = (L \cup R \cup \{s,t\}, E')\)</li>
			<li>Direct all edges from \(L \longrightarrow R\) and make their capacities \(\infty\)</li>
			<li>Connect \(s\) to all nodes in \(L\) and set the capacity of these edges to 1</li>
			<li>Connect all nodes in \(R\) to \(t\) and set the capacity also to 1</li>
		</ul>
		<p class="side">
			<b><i>Theorem 6.</i></b> There is now a <b>1-1 correspondence</b> between the max matching size \(k\) in \(G\) and the max flow value in \(G'\).
		</p>
		<p>
			A corollary is that we can solve bipartite matching in poly-time with max flow. Granted, non-bipartite matchings are also poly-time, but irrelevant. 
		</p>
		<h4>Perfect Matching</h4>
		<p class="blue">
			<b><i>Definition.</i></b> Given a graph \(G=(V,E)\), a subset \(M \subseteq E\) is a <b>perfect matching</b> if every node appears in \(M\) (M is a vertex cover and a matching).
		</p>
		<p>
			Since we match one left node to one right node in a matching, it reasons that a bipartite graph with a perfect matching must have \(|L| = |R|\).
		</p>
		<p>
			Let us <b>denote</b>, for a set \(S\) of nodes, \(N(S)\) as the neighbours of all nodes in \(S\).
		</p>

		<p class="side">
			<b><i>Theorem 7.</i> (Hall's Marriage)</b> Given a bipartite graph \(G = (L \cup R, E) : |L| = |R|\), G has a perfect matching <b>if and only if</b> \(|N(S)| \geq |S|\; \forall S \subseteq L\).
		</p>

		<p>
			<i>Proofs are left as an excersise to the reader.</i>
		</p>
	</div>

	<footer>
		<div class="cbox">
			<div class="columncontainer ctwo" id="fc2">
			</div>
			<script type="text/javascript" src="../../js/footerGen.js"></script>
		</div>
	</footer>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
</body>
</html>