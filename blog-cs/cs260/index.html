<!DOCTYPE html>
<html>
<head>
	<title>CS260</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0> 
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="../../" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="../../blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="../../about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS260</h1>
                    <p class="subheading">Algorithms</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Introduction</h1>
			</div>
		</header>

		<div class="cbox">
			<p>
                These notes will be added to throughout the year. Check back later for more.
            </p>

            <ol>
                <li><a href="#greedy">Greedy Algorithms</a></li>
				<li><a href="#divide">Divide and Conquer</a></li>
				<li><a href="#dynprogramming">Dynamic Programming</a></li>
				<li><a href="#intractibility">Intractbility</a></li>
            </ol>
		</div>


		<div class="colourband">
			<h2 id="greedy">Greedy Algorithms</h2>
		</div>

		<div class="cbox">
			<h3>Overview</h3>

			<p>
				A <i>Greedy Algorithm</i> is one that builds up a solution from small steps, snatching or discarding the next available one without regard for the bigger picture, based on some simple rule(s).
			</p>

			<ol>
				
				<li><a href="#greedy1">Interval Scheduling</a></li>
				<li><a href="#greedy2">Interval Partitioning</a></li>
				<li><a href="#greedy3">Minimising Lateness</a></li>
				<li><a href="#greedy4">Strategies of Analysis</a></li>
			</ol>

			<h3 id="greedy1">Interval Scheduling</h3>

            <div class="cornell">
                <div class="ir">
					<p>
						Interval Scheduing
					</p>
                    
                </div>
                <div>
					<p>
						You manage a scheduling system for a conference room, and want to schedule as many meetings as possible, with no overlap between meetings and no rearranging of meeting times. 
					</p>
                    <p>
						Let us call meetings <i>jobs</i>, the general term for this problem. Job \(j\) starts at time \(s_j\) and finishes at \(f_j\). Two jobs are <b>compatible</b> [to be scheduled] if they do not overlap.
					</p>
					<p>
						Our goal is to make an algorithm with the following inputs and outputs
						<ul>
							<li><b><span class="sc">in:</span></b> A sequence of jobs (which are just pairs \((s_j,f_j\))</li>
							<li><b><span class="sc">out:</span></b> The <b>maximum subset</b> of mutually compatible jobs</li>
						</ul>
					</p>

					<p>
						We can use a greedy algorithm to solve this, but we need the rules that allow us to implement it. We'll sort the jobs by a specific rule, then take jobs in order, provided they are compatible with all the jobs already selected. 
					</p>

					<p>
						This ordering rule can be many things: earliest start time, earliest finish time, or shortest interval time being some possibilities. (You can try work out which one is correct, but I'm going to immediately reveal it below.)
					</p>
                </div>

				<div class="ir">
					<p>Earliest Finish First</p>
				</div>
				<div>
					<p>
						This is the correct sort rule. We can write an algorithm, for jobs \(j_i\) with starts/finishes \((s_i, f_i)\) for \(i = 1..n\).
					</p>

					<div class="codediv">earliest_finish_first(\(n\), \(s_{1..n}\), \(f_{1..n}\)):
	sort jobs by finish time, and renumber them such that \(f_1 \leq f_2 \leq ... \leq f_n\)
	\(S = \varnothing\)  # set of jobs selected
	for \(j = 1 .. n\):
		if job \(j\) compatible with \(S\):
			\(S\).add(\(j\))
	return \(S\)</div>
				</div>

				<div class="ir">
					<p>Running Time</p>
				</div>
				<div>
					<p>We can prove this algorithm runs with \(O(n \log n)\). This is because:</p>

					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>We know that sorting is at best \(O(n \log n)\). </p>
						<p>The for loop, we can prove is only \(O(n)\). If the for loop is \(O(n)\), then the compatibility check must be \(O(1)\)</p>
						
						<ul>
							<li>If we keep a track of job \(j*\), which is the <i>last job</i> added to \(S\)</li>
							<li>\(j\) would be compatible with \(S\) if and only if \(s_j \geq f_{j*}\)</li>
						</ul>
						<p>Thus comparison is indeed order \(O(1)\).</p>
					</div>

					
				</div>

				<div class="ir">
					<p>Proof of Correctness</p>
				</div>
				<div>
					<p>
						<b><i>Theorem.</i></b> The Earliest Finish First (EFF) algorithm is optimal.
					</p>
					
					<button class="collapsible">Proof...</button>
					<div class="ccontent">
						<p><b><i>Proof.</i></b> We will prove by contradiction. Let us assume EFF is not optimal.</p>
						<p>
							Let \(i_1, i_2, ..., i_k\) be the set of jobs selected by EFF.
						</p>
						<p>
							Let \(j_1, j_2, ..., j_m\) be the optimal set, with \(i_1 = j_1, i_2 = j_2, ... i_r = j_r\) for as large of a value of \(r\) as possible. If EFF is not optimal, then \(m > k\).
						</p>
						<p>
							If job \(i_{r+1}\) does not exist, then by nature of the algorithm all jobs after \(i_r\) are incompatible with it. However, since \(i_r = j_r\), and we know that the optimal must <i>strictly</i> have more jobs than EFF, there must be compatible jobs after \(i_r\), thus we reach a contradiction.
						</p>

						<p>
							If job \(i_{r+1}\) exists, it cannot finish later than \(j_{r + 1}\), because of the sorting rule. Thus we can just replace \(j_{r+1}\) with \(i_{r+1}\), and guarantee that all jobs \(j_r+2\) and afterwards is compatible. Thus the optimal is still optimal, and the condition that we have the <i>largest possible \(r\)</i> has been violated.
						</p>

						$$\tag*{$\Box$}$$
					</div>
				</div>
            </div>

			<h3 id="greedy2">Interval Partitioning</h3>

			<div class="cornell">
				<div class="ir">
					<p>Interval Partitioning</p>
				</div>
				<div>
					<p>You are in charge of scheduling lectures into lecture rooms. Lecture \(j\) starts at \(s_j\) and finishes at \(f_j\), and your goal is to find the <i>minimum</i> number of classrooms needed to schedule all lectures such that no two overlap.</p>
					
					<p>
						<ul>
							<li><b><span class="sc">in:</span></b> A sequence of jobs (which are just pairs \((s_j,f_j\))</li>
							<li><b><span class="sc">out:</span></b> The smallest possible collection of sets (classrooms) of compatible jobs</li>
						</ul>
					</p>

					<p>
						(As the section implies) we use a greedy algorithm, and need to decide the ordering rule. This can be: Earlist start first, Earliest finish first, Shortest lecture first, or something else.
					</p>
				</div>

				<div class="ir">
					<p>Earliest Start First</p>
				</div>
				<div>
					<p>This is the correct sort rule.</p>
					<div class="codediv">earliest_start_first(\(n\), \(s_1 .. s_n\), \(f_1 .. f_n\)):
	sort lectures by start times, renumber s.t. \(s_1 \leq s_2 \leq ... \leq s_n\)
	\(d = 0\)  # number of allocated rooms
	for \(j = 1 .. n\):
		if (lecture \(j\) compat. with all lectures in any classroom \(k\)):
			schedule \(j\) in \(k\)
		else:
			allocate new room \(d+1\)
			schedule \(j\) in room \(d+ 1\)
			\(d = d+1\)
	return the schedule</div>
				</div>

				<div ><p>Running Time</p></div>
				<div>
					<p>
						If we use a suitable data structure to store the rooms in, this algorithm can be \(O(n \log n)\).
					</p>

					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							First of all, sorting is \(O(n \log n)\). If we store all rooms in a <i>Priority Queue</i>, with the key being the <i>finish time of the last lecture</i>:
						</p>
						<ul>
							<li>When we allocate a new room, we insert it into the PQ.</li>
							<li>When we schedule \(j\) in \(k\), we increase the key of \(k\) to \(f_j\).</li>
							<li>To determine whether \(j\) is compatiable with any \(k\), we compare \(s_j\) to <code>findMin</code> of the PQ.</li>
						</ul>

						<p>
							The total number of searches in the priority queue is on order \(O(n)\), where each PQ operation is \(O(\log n)\), thus we get \(O(n \log n)\).
						</p>
					</div>

					<p>
						This implementation will always schedule the next compatible lecture in the room with the earliest finish time.
					</p>
				</div>

				<div>
					<p>Definitions and Observations</p>
				</div>
				<div>
					<p class="blue"><b><i>Definition.</i></b> The <b>Depth</b> of a set of open intervals is the max number of intervals that contain some point. Basically, the point where the most lectures overlap from all rooms determines the depth, which is the number of rooms.</p>

					<img src="./classrooms.svg" alt="at most 3 concurrent lectures = depth 3" style="max-width: 400px; width: 100%;">

					<p>
						Minimum number of rooms would equal the depth (since no lectures can overlap)
					</p>

					<p>
						Also take note that the Earliest Start First (ESF) never schedules two incompatible lectures in one room.
					</p>
				</div>

				<div>
					<p>Proof of Correctness</p>
				</div>
				<div>
					

					<p>
						<b><i>Theorem.</i></b> ESF is optimal.
					</p>

					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Let \(d =\) the number of rooms ESF allocates.
						</p>
						<p>
							Room number \(d\) is opened because we need to schedule a lecture \(j\), which is incompatible with all lectures in rooms \(1 .. d-1\). 
						</p>
						<p>
							Because of the earliest start sort, each incompatible lecture in all prior rooms must have a start time \(\leq s_j\). Furthermore, all \(d\) lectures (including \(j\)) will have ended by \(f_j\).
						</p>
						<p>
							Thus there will be \(d\) lectures overlapping at some time \(s_k + \epsilon\) for a number \(\epsilon\), which is our depth. Since depth = max number of rooms, this demonstrates that ESF is optimal.
							$$\tag*{$\Box$}$$
						</p>
					</div>
				</div>


			</div>

			<h3 id="greedy3">Minimising Lateness</h3>

			<div class="cornell">
				<div>
					<p>Minimising Lateness</p>
				</div>
				<div>
					<p>You are in charge of a single mainframe that can process one job at one time. Job \(j\) requires \(t+j\) units of time to process and is due at \(d_j\) (but can be late). (If \(j\) starts at \(s_j\) it finishes at \(f_j = s_j + t_j\).) Your goal is to schedule jobs to minimise <i>maximum lateness</i>.</p>

					<p>
						Let lateness be defined \(\ell_j = \max(0, f_j - d_j)\), and max lateness is thus \(L = \max(\textrm{all } \ell_j)\).
					</p>

					<p>
						<ul>
							<li><b><span class="sc">in:</span></b> A sequence of jobs (which are just pairs \((t_j,d_j\))</li>
							<li><b><span class="sc">out:</span></b> An ordering of jobs with the least amount of lateness.</li>
						</ul>
					</p>

					<p>
						Some rules we can consider are order by: shortest processing time, earliest deadline, or shortest slack (\(d_j-t_j\)).
					</p>
				</div>

				<div>
					<p>Earliest Deadline First</p>
				</div>
				<div>
					<div class="codediv">
earliest_deadline_first(\(n\), \(t_{1..n} \), \(d_{1..n} \)):
	sort jobs by due time and renumber s.t. \(d_1 \leq d_2 \leq ... \leq d_n\)
	\(t = 0\)
	for \(j = 1..n\):
		assign job \(j\) to interval \([t, t+t_j] \)
		\(s_j = t; f_j = t+t_j\)
		\(t = t+t_j\)
	return intervals \([s_1, f_1] .. [s_n, f_n] \)
					</div>
				</div>

				<div>
					<p>Important Observations and Lemmas</p>
				</div>
				<div>
					<p>
						<b><i>1.</i></b> There exists an optimal schedule with no idle time. If we have a schedule with idle time between jobs, which has no lateness, we can simply remove all idle time and still have no lateness. 
					</p>

					<p>
						<b><i>2.</i></b> Earliest Deadline First (EDF) has no idle time by design.
					</p>

					<p class="blue">
						<b><i>Definition.</i></b> Given a schedule S, an <b>inversion</b> is a pair of jobs \(i, j\) where \(i < j\) (meaning i is due before j) and \(j\) is scheduled before \(i\).
					</p>

					<p>
						<b><i>3.</i></b> The EDF schedule is the (unique) schedule with no inversions (by design).
					</p>

					<p>
						<b><i>4.</i></b> If some schedule with no idle time has an inversion, then it has an adjacent inversion (inverted jobs are next to each other)
					</p>

					<button class="collapsible">Proof of 4...</button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Let \(i-j\) be the <i>closest</i> inversion: <br>
							
						</p>
						<p>
							If \(i, j\) are adjacent, we are done. However, if we have a case like the following:
						</p>
						Schedule: <code>...[ ][j][k][ ][ ][i][ ]...</code>
						<p>
							Then there would exist a job \(k\) between \(j, i\), which is directly after \(j\).
						</p>

						<p>
							If \(j > k\), then \(j-k\) is an adjacent inversion. Else if \(j < k\), then \(k > i\) and \(k-i\) is a <i>closer</i> inversion. Repeat until we encounter an adjacent inversion. $$\tag*{$\Box$}$$
						</p>
					</div>

					<p>
						<b><i>Key Lemma.</i></b> Eschanging two adjacent inverted jobs \(i, j\) reduces the number of inversions by one, and does <b>not</b> increase maximum lateness. 
					</p>

					<button class="collapsible">Proof of Lemma... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Let \(\ell\) denote the lateness <i>before</i>, and \(\ell'\) denote lateness <i>after</i>.
						</p>
						<p>
							\(\ell'_k = \ell_k \; \forall k \neq i, j\), and \(\ell'_i \leq \ell_i\) when \(i\) is moved forward in the schedule.
						</p>
						<p>
							if job \(j\) is not late, we are done. If it is late, then the new lateness \(\ell'_j = f'_j - d_j\) (by definition)
							\begin{align}
								\ell'_j &= f'_j - d_j \\
								&= f_i - d_j \textrm{ (see diagram)} \\
								&\leq f_i - d_i \textrm{ since } d_i \leq d_j \\
								&\leq \ell_i
							\end{align}

							<img src="./inversion.svg" alt="Inverting i and j: new finish time of j is old finish time of i" style="max-width: 400px; width: 100%; display:block;">

							Thus lateness does not increase. $$\tag*{$\Box$}$$
						</p>
					</div>
				</div>

				<div>
					<p>Proof of Correctness</p>
				</div>
				<div>
					<p>
						<b><i>Theorem.</i></b> EDF is optimal.
					</p>

					<button class="collapsible">Proof... </button>
					<div class="ccontent">
						<p>
							<b><i>Proof.</i></b> Let us define \(S*\) to be the optimal schedule with the <i>fewest</i> inversions. (We say that optimal solutions may have inversions).
						</p>

						<p>
							By observation 1 \(S*\) will have no idle time. 
						</p>

						<p>
							<i>Case 1</i> If \(S* \) has no inversions, then the schedule \(S\) generated by EDF will equal \(S*\) by observation 3.
						</p>
						<p>
							<i>Case 2.</i> If \(S* \) has an inversion:
						</p>
						<ul>
							<li>Let \(i-j\) be an adjacent inversion (Obs. 4)</li>
							<li>Exchanging \(i, j\) decreases the number of inversions by 1, and does not increase max lateness (lemma)</li>
							<li>This contradicts the fewest inversion condition on \(S*\), as we can exchange all the way to no inversions and be (more) optimal - which is the schedule generated by EDF. </li>
						</ul>
						$$\tag*{$\Box$}$$
					</div>
				</div>

				
			</div>

			<h3 id="greedy4">Strategies to Analyse Greedy Algorithms</h3>

				<div class="cornell">
					<div></div>
					<div>
						<p>Three strategies were explored for analysing and proving the optimality of greedy algorithms.</p>
					</div>
					
					<div>
						<p>Greedy Stays Ahead</p>
					</div>
					<div>
						<p>
							Demonstrating that after each incremental step, the greedy algorithm solution is at least as good as any other solution. This incremental building was employed in the proof of Interval Scheduling.
						</p>
					</div>

					<div>
						<p>Structural Bound</p>
					</div>
					<div>
						<p>
							Discover a simple bound / principle on the structure of the problem, which gives the lowest (most optimal) bound on possible solutions, and show that the greedy algorithm always reaches that bound. This was employed in Interval Partitioning with the depth bound.
						</p>
					</div>

					<div>
						<p>Exchange Argument</p>
					</div>
					<div>
						<p>
							By gradually transforming a hypothetical optimal solution (which is not the greedy algorithm one) into the greedy algorithm solution without hurting its quality. This was employed by swapping inversions in Minimising Lateness.
						</p>
					</div>
				</div>

		</div>

		<div class="colourband">
			<h2 id="divide">Divide and Conquer</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#div-1">Merge Sort</a></li>
				<li><a href="#div-2">Closest pair of points</a></li>
				<li><a href="#div-3"><b>Master Theorem</b></a></li>
			</ol>

			<div class="cornell">
				<div>
					<p>Divide and Conquer Paradigm</p>
				</div>
				<div>
					<p>
						Divide and conquer is a strategy for solving algorithms where the main problem is <b>divided</b> into <b>independent</b> subproblems, which are then solved (<b>conquered</b>) <b>recursively</b>.
					</p>

					<p>
						In general, this goes as follows:
						<ul>
							<li>Divide a problem of size \(n\) into subgroups, e.g. 2 subproblems of size \(\frac{n}{2}\) - <i>typically \(O(n)\)</i></li>
							<li>Solve the 2 subproblems recursively</li>
							<li>Combine the subproblems into one - <i>typically \(O(n)\)</i></li>
						</ul>
						This can end up faster than "traditional" polynomial time methods. 
					</p>
				</div>
			</div>

			<h3 id="div-1">Merge Sort</h3>

			<p>
				Your classic divide and conquer sorting algorithm that you learned in GCSE. 
			</p>

			<div class="cornell">
				<div>
					<p>Process</p>
				</div>
				<div>
					<ul>
						<li>For an input size (n), divide into a <i>left</i> half and a <i>right</i> half. - \(O(n)\)</li>
						<li><b>Recursively</b> sort left and right halves (refer to above)</li>
						<li>Combine two sorted halves, maintaining the order. - \(O(n)\)</li>
					</ul>
				</div>
				<div>
					<p>Combining halves</p>
				</div>
				<div>
					<p>
						Suppose we have two sorted halves A and B, with
						\[A = [a_1, a_2, a_3], \; B = [b_1, b_2, b_3, b_4]\]
						Maintain two pointers to the sorted arrays. Scan A and B, comparing \(a_i\) and \(b_j\). Append the <i>smaller</i> value to the output list, and increment that pointer only. Finally, append any left over values from one array if the other has reached the end. 
					</p>
					<p>
						<i>Linear for scanning, constant for comparing, constant for appending.</i>
					</p>
					
				</div>
				<div>
					<p>Implementation</p>
				</div>
				<div>
					<p></p>
					<div class="codediv">mergesort(list L):
	if L.length = 1:
		return L
	divide L into two halves, A and B
	A \(\longleftarrow\) mergesort(A)
	B \(\longleftarrow\) mergesort(B)
	L \(\longleftarrow\) merge(A, B)  # implementation of this function 
		# is left as an excersise to the reader
	return L</div>

					<p>
						Let's say mergesort has a procesing time of \(T(n)\). Thus, every subproblem (\(A, B\)) takes \(T(\frac{n}{2})\) time. The <code>merge</code> portion as we know is linear, and is \(\Theta(n)\).
					</p>
					<p>
						Thus we can conclude \(T(n)\) is bounded by \(\Theta(n) + 2T(\frac{n}{2})\).
					</p>
				</div>
				<div>
					<p>Useful recurrence relation</p>
				</div>
				<div>
					<p>
						The above gives rise to a useful recurrence relation. 
					</p>
					<p>
						<b><i>Definition.</i></b> Let \(T(n)\) to be the number of comparisons mergesort makes proportional to list size \(n\). This satisfies the recurrence 
						\[T(n) = \begin{cases} 0 & \textrm{if } n = 1 \\ T(\left \lfloor{\frac{n}{2}} \right \rfloor) + T(\left \lceil{\frac{n}{2}} \right \rceil) + n & \textrm{if } n > 1 \end{cases}\]
					</p>
				</div>
				<div>
					<p>Complexity of merge sort</p>
				</div>
				<div>
					<p>
						<b><i>Claim.</i></b> For mergesort, \(T(n) = O(n \log n)\).
					</p>
					<p><i>
						Assume \(n\) is a power of 2. This does not affect the integrity much but makes proving easier.
					</i></p>
					<p class="small">
						The following proofs demonstrate important techniques so are <i>shown by default</i>. Clicking the drop-down bar (twice) will hide them. 
					</p>
				</div>
				<div>
					<p>Proof by tree</p>
				</div>
				<div>
					<p>
						<b><i>Assertion 1.</i></b> \(T(n) = O(n \log n)\) if \(T(n)\) satisfies the following recurrence (given above assumption):

						\[T(n) = \begin{cases} 0 & \textrm{if } n = 1 \\ 2T(\frac{n}{2}) + n & \textrm{if } n > 1 \end{cases}\]
					</p>

					<button class="collapsible">Proof by Tree... </button>
					<div class="ccontent inv">
						<p>
							<b><i>Proof.</i></b> Represent the recurrence in tree form:
							<figure>
								<img src="./recurrence-tree.png" alt="recurrence tree">
							</figure>
							<ul>
								<li>On level \(i\), there are \(2^i\) nodes. Each node has \(\frac{n}{2^i} \) items so does that amount of work. Thus each level does \(n\) work. </li>
								<li>\(n = 2^k\) (is a power of 2), and we <i>halve</i> \(n\) every time, thus we do \(k\) halvings to get to \(2^0\): 1 item. Thus \(k\) = the number of levels = \(\log_2 n\).</li>
							</ul>
							\(\log_2 n\) times \(n\) work gets \(O(n \log n)\)
							$$\tag*{$\Box$}$$
						</p>
					</div>
				</div>
				<div>
					<p>Proof by induction</p>
				</div>
				<div>
					<p>A proof method that may be easier to use (and certainly is easier to represent in HTML). We use the <b>same proposition</b> as above.</p>

					<button class="collapsible">Proof by Induction...</button>
					<div class="ccontent inv">
						<p>
							<b><i>Proof.</i></b> Induction on the size of \(n\).
							<ul>
								<li><b>Base case:</b> \(n=1\), thus \(T(1) = 0 = 1 \log_2 1\).</li>
								<li><b>Hypothesis:</b> Assume \(T(n) = n \log_2 n\).</li>
							</ul>
						</p>
						<p>
							Let us look at \(T(2n)\).
							\begin{align}
								T(2n) &= 2T(n) + 2n\\
								&= 2n \log_2 n + 2n \textrm{ by ind hyp.} \\
								&= 2n (\log_2 n + 1) = 2n (\log_2 n + \log_2 2) \\
								&= 2n \log_2(2n).
							\end{align}
							$$\tag*{$\Box$}$$
						</p>
					</div>
				</div>

				<div>
					<p>Extending inductive proof</p>
				</div>
				<div>
					<p>
						So far we have been ignoring floors and ceilings. In fact, we can take as granted that <b>floor and ceil may be ignored</b> outright. However, here's an inductive proof for the assertion that uses floor and ceil. 
					</p>
					<p>
						<b><i>Assertion 2.</i></b> \(T(n) \leq n \left \lceil{\log_2 n} \right \rceil\) if \(T(n)\) satisfies the following recurrence. \(n\) is not assumed to be a power of 2. 
					</p>

					<div style="display: none;">
						\[ \require{enclose} \]	
					</div>
					

					<button class="collapsible">Proof by Strong Induction... </button>
					<div class="ccontent inv">
						<p>
							<b><i>Proof.</i></b> Strong induction on \(n\). Instead of just assuming the prior is true, we assume <i>all priors</i> are true. The base case is the same and is true. Please note \(\log\) means \(\log_2\). 
						</p>
						<p>
							Let \(n_1 = \left \lfloor{n} \right \rfloor\) and \(n_2 = \left \lceil{n} \right \rceil\). <i>Note that \(n_1 + n_2 = n\)</i>. The <b>inductive hypothesis</b> is that the recurrence is satisifed for <b>all</b> values up to \(n-1\). 
						</p>
						<p>
							Some easy-to-miss changes are circled. The circles are for emphasis and are otherwise meaningless.
							\begin{align}
								T(n) &\leq T(n_1) + T(n_2) + n \\
								&\leq n_1 \left \lceil{\log n_1} \right \rceil + n_2 \left \lceil{\log n_2} \right \rceil + n \textrm{ by ind hyp.} \\
								\because n_2 \geq n_1,\; T(n) &\leq n_1 \left \lceil{\log 
									{\scriptstyle \enclose{circle}{\kern .06em n_2\kern .06em}}
								} \right \rceil + n_2 \left \lceil{\log n_2} \right \rceil + n \\
								&\leq (n_1 + n_2) \left \lceil{\log n_2} \right \rceil + n \\
								& \leq n \left \lceil{\log n_2} \right \rceil + n \\
								& \leq n (\left\lceil{\log  {\scriptstyle \enclose{circle}{\kern .06em n \kern .06em}} }\right\rceil + 1) + n \; *\\
								& \leq n \left\lceil{\log n}\right\rceil .
							\end{align}
							$$\tag*{$\Box$}$$
						</p>
						<p>
							The jump marked \(*\) is explained as follows:
							\begin{align}
								2^{\log n_2} = n_2 &= \left\lceil{\frac{n}{2}}\right\rceil \\
								&\leq \left\lceil{\frac{2^\left\lceil{\log n}\right\rceil }{2}}\right\rceil \\
								&\textrm{since } n = 2^{\log n} \textrm{, } 2^{\left\lceil{\log n}\right\rceil } \textrm{ must be not lower. } \\
								&\leq \frac{2^{\left\lceil{\log n}\right\rceil }}{2} \textrm{ since RHS must } \in \mathbb{Z} \\
								&\leq 2^{\left\lceil{\log n}\right\rceil -1} \\
								\therefore \log n_2 = \log{2^{\log n_2}} &= \left\lceil{\log n}\right\rceil - 1.
							\end{align}
						</p>
						<p class="small">
							If you're curious, right click -> show math as tex commands, it's nasty to write, thank god for vscode snippets.
						</p>
					</div>
				</div>
			</div>

			<h2 id="div-2">Closest Pair of Points</h2>

			<div class="cornell">
					<div>
						<p>Problem </p>
					</div>
					<div>
						<p>
							Given \(n\) points in a euclidean plane, find the closest pair of points. 
						</p>
					</div>
					<div>
						<p>Possible non-divide and conquer solutions</p>
					</div>
					<div>
						<p>
							Listed are some non-divide and conquer solutions, for interest. 
						</p>
						<button class="collapsible">Expand... </button>
						<div class="ccontent">
							<p>
								<b>Brute force</b>: the "naive" algorithm, I think you can guess what this does. Solves in \(\Theta(n^2)\) but we can do better. 
							</p>
							<p>
								<b>Dimension collapsing</b>: we can consider only 1 dimension (say, x), sort the points and find the closest. This can be done in \(\Theta(n \log n)\) by sorting points, and comparing only consecutive ones. The only problem is it has counterexamples and may not always work depending on your configuration of points.
							</p>
							<p>
								This relies on an assumption of <i>non-degeneracy</i> (see later).
							</p>
							<p>
								We can extend this idea to doing dimension collapsing for 2D by repeating the process for both x and y, finding the points which are closest together on those two axes.
							</p>
							<p>
								However this is also flawed and has counterexamples. 
							</p>

							<p>
								<b>Division:</b> We can try a technique more akin to divide and conquer - dividing the space up into halves or quadrants, then considering each quadrant separately, then the boundaries of quadrants. Ideally we'd want all quadrants to have an equal number of dots, but this is hard to guarantee. 
							</p>
						</div>
						
					</div>

					<div>
						<p>The divide and conquer algorithm</p>
					</div>
					<div>
						<p>
							<b><i>Definition.</i></b> the <b>non-degeneracy</b> assumption asserts that each point should have a unique \(x\) coordinate. 
						</p>
						<p>
							We <b>divide</b> by drawing a vertical line \(L : \frac{n}{2}\) points on each side. We <b>conquer</b> by recursively finding the closest point in each "half". We <b>combine</b> by checking if there's a closer pair that lies across the boundary. 
						</p>
						<p>
							It seems initially hard to do this more efficiently than the brute force \(n^2 \), but there are a few tricks we can employ. 
						</p>
						<p>
							Take the minimum distance from the pairs on either side, let it be \(\delta\). Say our two halves find closest points with length 12 and 21, \(\delta = \min(12, 21)\). It is the sufficient to <b>only check points within \(\delta\) distance from L</b>.
						</p>
						<p>
							We need only sort the boundary points by their \(y\) coordinate, then compute the closest points in \(y\). This much may be quite easy to understand, but that still leaves us with quadratic time if we have to compute distances to all other points, but in fact, we only need to compute <b>7</b>.
						</p>
						<p>
							Every point checks against up to 7 others, and the minimum distance found is kept track of as the boundary points are iterated through. This actually will reduce our combine step down to <b>linear time</b>: \(O(n)\). 
						</p>
					</div>

					<div>
						<p>Why 7!?</p>
					</div>
					<div>
						<p>
							To prove this, first <b>let</b> all boundary points be sorted by y coordinate, and numbered in increasing order, \(s_1, s_2, \dots, s_n\).  \(s_i\) is the point with the \(i\)<sup>th</sup> largest y value. 
						</p>
						<p>
							<b><i>Proposition.</i></b> If \(|i-j| \geq 7\), the distance between \(s_i, s_j\) is <b>at least \(\delta\)</b>.
						</p>
						<button class="collapsible">Proof...</button>
						<div class="ccontent inv">
							<p>
								Let us take any point \(s_i\) with some \(y\) coordinate. Consider a rectangle \(\delta\) tall and \(2\delta\) wide, spanning the boundary region with its bottom being the \(y\) coord of \(s_i\). Split this rectangle into 8 squares, \((0.5\delta)^2\).
							</p>
							<img src="./7-rectangles.png" alt="rectangle split into 8 squares" style="max-width: 200px;">
							<img src="./7-furthest-points.png" alt="furthest away points in one square are delta over root 2 apart" style="max-width: 300px;">
							<p>
								The furthest away two points could be in one single square is at opposite diagonals, \(\frac{\delta}{2}\) away. \(\frac{\delta}{2 } < \delta\) - this means that <b>this is an impossible configuration</b>: both points would be on the same side, with the distance between less than \(\delta\), which is <i>defined</i> to be the shortest distance from both sides. 
							</p>
							<p>
								Thus, there can only be <b>one point per square</b>. At most 7 other points <i>may</i> be valid options (have distances closer than \(\delta\)), so we need only check 7. 
							</p>
							$$\tag*{$\Box$}$$
						</div>
					</div>

					<div>
						<p>Implementation</p>
					</div>
					<div>
						<p></p>
						<div class="codediv">closestPair(points \(p_1, p_2, \dots, p_n\)):
	computer vertical line \(L\) such that half the points are on either side  # (1)
	\(\delta_1 \longleftarrow\) closestPair(left half)  
	\(\delta_2 \longleftarrow\) closestPair(right half)  # (2)
	\(\delta \longleftarrow \min(\delta_1, \delta_2)\)
	delete all points further than \(\delta\) from \(L\)  # (3)
	sort remaining points by \(y\) coordinate  # (4)
	for \(p \in \) remaining points:  # (5)
		work out the distances up to the next 7 points 
		if the smallest of those \(\leq \delta\):
			\(\delta \longleftarrow \) that distance 
	return \(\delta\)</div>
					</div>

					<div>
						<p>Running time</p>
					</div>
					<div>
						<p>
							The labelled sections are as follows: (1) is \(O(n)\), (2) is \(2T(\frac{n}{2})\), (3) is \(O(n)\), (4) is \(O(n \log n)\), and 5 is \(O(n)\).
						</p>
						<p>
							As to \(T(n)\), knowing the running time of dividing and combining steps we can get the recurrence 

							\[ T(n) = \begin{cases} \Theta(1) & n = 1 \\ T(\left\lfloor{\frac{n}{2}}\right\rfloor )  + T(\left\lceil{\frac{n}{2}}\right\rceil ) + \Theta(n \log n) & \textrm{otherwise} \end{cases} \]
						</p>
						<p>
							This resolves down to a running time of \(\Theta(n \log^2 n)\).
						</p>
					</div>

					<div>
						<p>Improvements on running time</p>
					</div>
					<div>
						<p>
							Later work has been done to push the algorithm down to \(O(n \log n\). This involves techniques such as preserving sorting of delta strip, returning lists of points sorted by x and y, and sorting points by merely doing one merge, which is linear time. 
						</p>
					</div>
				</div>

				<h2 id="div-3">Master Theorem</h2>

				<div class="cornell">
					<div>
						<p>Premise</p>
					</div>
					<div>
						<p >
							The <b>master theorem</b> is a single method to solve all common recurrences of the form
							<span style="border-spacing: 0px 0px;">\[T(n) = aT(\frac{n}{b}) + f(n)\]</span>
							
							Where \(a, b \in \mathbb{Z}^+\), \(b \geq 2\), and with \(T(0) = 0, \; T(1) = \Theta(1)\). 
						</p>
						<p>
							An input of size \(n\) makes \(a\) recursive calls of size <span style="border-spacing: 0px 0px;">\(\frac{b}{n}\)</span>. We also assume that \(f(n) \geq 0\). 
						</p>
					</div>

					<div>
						<p>Deriving via recursion trees</p>
					</div>
					<div>
						<p></p>
						<button class="collapsible">Expand... </button>
						<div class="ccontent">
							<p>
								We can solve such recurrences with a recursion tree. (We assume that \(n\) is a power of \(b\).)
								<ul>
									<li>Let  \(a=\) branching factor</li>
									<li>\(a^i = \) number of subproblems at layer \(i\)</li>
									<li>\(1+\log_b n\) levels</li>
									<li>\(\frac{n}{b^i} =\) size of subproblems at layer \(i\).</li>
								</ul>
							</p>
	
							<p>
								Suppose \(T(n)\) satisfies \(T(n) = aT(\frac{n}{b}) + n^c\) for some c: i.e. \(f(n)\) is a polynomial. 
							</p>
							<p>
								Then at we have \(n^c\) work at layer 0, \(a(\frac{n}{b})^c\) at 1, \(a^2 \frac{n}{b^2}^c\) at 2, ..., \(a^i \frac{n}{b^i}^c\) at level \(i\), for a total of \(\log_b (n) \) levels. 
							</p>
							<p>
								The last \(\log_b n\)<sup>th</sup> level has \(a^i = a^{\log_b n} = n^{\log_b a}\):
								\[a^{\log_b n} = (b^{\log_b a})^{\log_b n} = b^{\log_b a \cdot \log_b n} = (b^{\log_b n})^{\log_b a} \]
							</p>

							<p>
								At level \(i\), the work is \(a^i (\frac{n}{b^i})^c = a^i \frac{n^c}{b^{ic}} = n^c \frac{a^i}{b^{ic}} = n^c (\frac{a}{b^c})^i\).
							</p>

							<p>
								Thus we can say \(T(n) = n^c \sum_{i=0}^{\log_b n} r^i\) where \(r = \frac{a}{b^c}\) (which is a constant).
							</p>

							<p>
								Looking at this sum, we can get that 
								\[
								T(n) = n^c \sum^{\log_b n}_{i=0} r^i = \begin{cases} 
								    \Theta(n^c) &\textrm{if }r&lt;1 &c &gt; \log_b a\\
									\Theta(n^c \log n) &\textrm{if } r = 1 &c = \log_b a \\
									\Theta(n^{\log_b a}) &\textrm{if } r &gt; &c &lt; \log_b a 1
								\end{cases}
								\]
								Since it is a gemetric series thus follows geometric series rules. 
							</p>
						</div>

						
					</div>
					<div>
						<p>Master Theorem</p>
					</div>
					<div class="blue">
						<p >
							<b><i>Theorem.</i></b> Let \(a \geq 1, b \geq 2, c &gt; 0.\) Let \(T(n)\) be a function on \(\mathbb{Z}_{\geq 0} \) that satisfies the recurrence
							<span style="border-spacing: 0px 0px;">\[T(n) = aT(\frac{n}{b}) + \Theta(n^c)\]</span> 
							With \(T(0) = 0, T(1) = \Theta(1),\) and <span style="border-spacing: 0px 0px;">\(\frac{n}{b}\) is either \(\left\lceil{\frac{n}{b}}\right\rceil \) or \(\left\lfloor{\frac{n}{b}}\right\rfloor \).</span>   Then the following cases apply:
							<ol>
								<li>\(c &lt; \log_b a \implies T(n) = \Theta(n^{\log_b a})\) - recurring part dominates</li>
								<li>\(c = \log_b a \implies T(n) = \Theta(n^c \log n)\) - balances</li>
								<li>\(c &gt; \log_b a \implies T(n) = \Theta(n^c)\) - processing part dominates</li>
							</ol>
						</p>
					</div>

					<div>
						<p>Overview of proof</p>
					</div>
					<div>
						<ul>
							<li>
								Prove theorem when \(b \in \mathbb{Z}\) and \(n\) is a power of \(b\)
							</li>
							<li>
								Extend the domains of reasoning to rationals or reals
							</li>
							<li>
								Deal with floor and ceil for <i>at most 2 levels</i>, e.g. 
								\begin{align}
									\left\lceil{\frac{\left\lceil{\frac{\left\lceil{\frac{n}{b}}\right\rceil }{b}}\right\rceil }{b}}\right\rceil & &lt; \frac{n}{b^3} + (\frac{1}{b^2} + \frac{1}{b} + 1) \\
									&\leq \frac{n}{b^3} + 2 \;\; (\because b \geq 2)
								\end{align}
							</li>
						</ul>
					</div>

					<div>
						<p>Theorem extension</p>
					</div>
					<div>
						<p>
							We can replace \(\Theta\) with \(O, \Omega\) with no problem. 
						</p>
						<p>
							We can replace the initial conditions with \(T(n) = \Theta(1)\) for all \(n \leq n_0\) (which is a constant) and only require the recurrence holds for \(n > n_0\). 
						</p>
					</div>

					<div>
						<p>Examples</p>
					</div>
					<div>
						<p>
							\(T(n) = 3T(\left\lfloor{\frac{n}{2}}\right\rfloor + 5n\)
						</p>
						<button class="collapsible">Expand... </button>
						<div class="ccontent">
							<p>
								\(a = 3, b = 2\) (and ignore floor). \(c = 1, 5n = O(n)\). \(\log_b a = \log_2 3 \approx 1.58\)
							</p>
							<p>
								Case 1 of 3, thus \(T(n) = \Theta(n^{\log_2 3} )\)
							</p>
						</div>
						<p>
							\(T(n) = T(\left\lfloor{\frac{n}{2}}\right\rfloor) + T(\left\lceil{\frac{n}{2}}\right\rceil ) + 17n \)
						</p>
						<button class="collapsible">Expand... </button>
						<div class="ccontent">
							<p>
								Can "merge" the floors and ceils, thus \(a=2, b=2, c=1\). 
							</p>
							<p>
								\(T(n) = O(n \log n)\)
							</p>
						</div>
					</div>

					<div>
						<p>Gaps in master theorem</p>
					</div>
					<div>
						<p>
							Master theroem will not work if the number of branches is not constant: \(T(n) = nT(\frac{n}{2 }) + n^2 \),
						</p>
						<p>
							The number of subproblem branches is less than 1: \(T(n) = \frac{1}{2} T(\frac{n}{2}) + n^2\),
						</p>
						<p>
							And cannot directly work if work done by subproblem is not \(\Theta(n^c)\): \(T(n) = 2T(\frac{n}{2 }) + n \log n\).
						</p>
					</div>
				</div>
			</div>

			<div class="colourband">
				<h2 id="dynprogramming">Dynamic Programming</h2>
			</div>
	
			<div class="colourband">
				<h2 id="intractibility">Intractibility</h2>
			</div>

		</div>

		

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="../../js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
</body>
</html>