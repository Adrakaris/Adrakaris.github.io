<!DOCTYPE html>
<html>
<head>
	<title>CS331</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="./" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="./blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="./about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS331</h1>
					<p class="subheading">Neural Computing</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Contents</h1>
			</div>
		</header>

		<!-- REMEMBER TO DO! -->

		<div class="cbox">
			<ol>
				<li><a href="#learn">Learning</a></li>
				<li><a href="#anns">Artificial Neural Networks (inc. MP Neuron)</a></li>
				<li><a href="#perceptron">Perceptron</a></li>
				<li><a href="#prop">Activations and Backpropagations</a></li>
			</ol>

			\( 
				\renewcommand{\vec}{\mathbf}
				\renewcommand{\epsilon}{\varepsilon}

				\DeclareMathOperator*{\argmax}{argmax}
				\DeclareMathOperator*{\argmin}{argmin}

				\def\xtrain{{\vec{x}_i}}  
				\def\xtest{{\vec{\tilde{x}}_i}}  
				\def\ytrain{{y_i}} 
				\def\ytest{{\tilde{y}_i}} 
				\def\ypred{{\hat{y}_i}} 
				\def\yhat{{\hat{y}}} 
				\def\wtrans{{\vec{w}^\top}} 
				\def\bb#1{{\mathbb{#1}}}
				
				\def\x{{\vec{x}}}
				\def\X{{\vec{X}}}
				\def\w{{\vec{w}}}
				\def\W{{\vec{W}}}
				\def\y{{\vec{y}}}
				\def\Y{{\vec{Y}}}
				\def\t{{^\top}}
				\def\z{{\vec{z}}}
				\def\Z{{\vec{Z}}}
				\def\v{{\vec{v}}}

				\def\lra{{\longrightarrow}}
				\def\lla{{\longleftarrow}}
				\newcommand{\fr}{\frac}
				\newcommand{\vecu}{\underline}
				\newcommand{\rm}{\textrm}
				\newcommand{\bb}{\mathbb}
				\newcommand{\nm}{\overline}
				\newcommand{\tl}{\tilde}

			\)
		</div>

		
		<div class="colourband">
			<h2 id="learn">Learning</h2>
		</div>

		<div class="cbox">
			<div class="md-conv">
				### AI 

				AI is a very broad category. It describes everything from tree search to deep learning.

				Good old fashioned AI (**GOFAI**) describes anything which does not use neuron-based models to learn: that is, statistical machine learning models, constraint satisfaction, etc. 

				Neural computing then further encompasses deep learning.
				
				### Learning paradigms

				There are three learning paradigms:

				* Supervised learning, where you have labelled data and model tries to find correct labels 
				* Unsupervised learning, where there are no labels
				* Reinforcement learning, where model has states and actions, and feedback given based on that

				Supervised learning tries to approximate a function $f:X \lra Y$. Example is classification.

				Unsupervised learning tries to *find* a function given an $X$. Example is clustering.

				Reinforcement learning is like [Little Albert and the Rat](https://www.simplypsychology.org/little-albert.html)

				This is a form of **pavlovian conditioning** (associative learning) e.g. Albert being conditioned to fear the rat. 

				### Transfer Learning 

				Transfer learning is taking a pre-trained model, and retraining it to perform a similar task. I.e. taking an ImageNet classifier (knows 1000 types of objects) and training it further to recognise new categories of your own choosing. 

				This concept of "incremental learning" builds on previous work done, and is faster and less effort than going from zero.

				Advantage:

				* Allows training with less data since you reuse models
				* Takes less time
				* Takes advantage of existing good architecture (not reinvent the wheel)

				The idea is to modify and retrain only the later stages/later layers. Naturally you have to set your own learning parameters still. 
			</div>
		</div>

		<div class="colourband">
			<h2 id="biology">Biology Class</h2>
		</div>

		<div class="cbox">
			<div class="md-conv">
				### Neurons 

				Your central nervous system (Brain, spine) has neurons. Neurons make think. Yes, this is relevant and examined. 
				
				Neurons generate signals (**action potentials**), which pass to other neurons.

			</div>
				<figure>
					<img src="./assets/neuron-diagram-researchgate.png" alt="Labelled neuron diagram" style="max-width: 500px;">
					<figcaption>Diagram of a neuron (ResearchGate)</figcaption>
				</figure>

			<div class="md-conv">
				**Dendrites** are tree-like structures with small spines which the synapses (ends) of other neurons connect to. They receive information.

				The **Soma** is the cell body, and is where all the signals (charges) received go. They are put in the **axon hillock**, and if the signal exceeds a threshold, the neuron passes on the signal. 

				The **axon** is the long thing which connects the axon terminals (synapses) to the rest of the cell. The larger the diameter the axon has, the fast it transmits data. Axons have a fatty myalin sheath insulating it, and if this is degraded axon transmission effectiveness worsens. 

				Altogether, a neuron takes in input, has an **activation function** which transforms the signal, and then makes output. 

				There are three classes of neurons: **sensory, motor, relay/inter**.

				Sensory neurons take input, motor neurons make output, and relay neurons relay data between them. 


				### Signal propagation

				Signals are propagated electrically, called "membrane potential". 

				It comes from the voltage potential across the nerve fibre -- the cell membrane. 

				Inside the cell are lots of $K^+$ ions. Outside are a lot of $Na^+$ ions. Their inequality forms the voltage potential, which is at rest around -70 mV. This is the **resting potential**.

				Across the cell membrane there are **channels** and **pumps**:

				* The **leaky channels**, which allows the free travel of $Na^+$ and $K^+$ through (is not gated)
				* The **volt-gated channels**, which only opens at certain voltages, and are generally closed at resting potential.
				* The **Sodium-Potassium Pump**, which removes 3$Na^+$ and takes in 2$K^+$.

				Neurons obey the "all or nothing" principle of firing; any stimulus under the threshold, no matter how big, does not fire, but anything that reaches it produces a full response. 

				See the graph for the phases.
				
			</div>

			<figure>
				<img src="./assets/neuron-action-potential.jpg" alt="Membrane potential diagram during activation">
				<figcaption>Voltage graph during stages of activation (byui.edu)</figcaption>
			</figure>

			<div class="md-conv">
				A neuron is **polarised** if it is more negative on the inside. **hyperpolarisation** is when the membrane potential is below resting potential, whilst **depolarisation** is when it's above. 

				The threshold is around -55 mV. 

				A stimulus causes some Na+ volt-gated gates to open, but if it's not enough it'll go back again. 
				
				When the threshold is reached it causes a rush of volt-gated gates to open, giving a spike of voltage up to 30 +mV.
				
				When it closes it hyperpolarises, and corrects itself. 

				### Synaptic transmission

				Synapses transfer chemically, via **neurotransmitters**, which are released from vesicles, and travel to receptors on the opposing synapse/dendrite.

				They can either be **excitatory** (causing action) or **inhibitory** (preventing action)

				Note that axon terminals don't have to connect to dendrites. They can connect to dendrites, somas, and other axons. Dendrites can also connect to dendrites. 
			</div>
		</div>

		<div class="colourband">
			<h2 id="anns">Artificial Neural Networks</h2>
		</div>

		<div class="cbox">
			<div class="md-conv">
				### Introduction

				Artificial Neural Networks are computer algorithms which are modelled off biological neuron learning. 

				Neurons are arranged in layers, which connect through to one another. (Note the input layer is not counted)

				Propagation of data goes in one direction, from input to output. This is called a **feed-forward NN**. 

				Of course there are many varieties, 
				
				like the convolutional neural net CNN, which works off filters (think: a function over an $n\times n$ block of an image);

				or a recurrent neural network, with a feedback system that allows it to deal with time-series or sequential data (see image).

				<figure>
					<img src="https://images.ctfassets.net/3viuren4us1n/fTI7OlJ6DrkeE05St3uf0/810c6207f985495c2b1b37cb7fc2d9db/2020-02-21_difference-between-cnn-rnn-1.webp" alt="RNN and expansion" style="max-width: 400px;">
					<figcaption>A RNN, and its expanded form through time.</figcaption>
				</figure>

				The latter is not called feed-forward, rather feed-back.

				ANNs use high performance hardware which are very fast and parallel (i.e. graphics cards). Because of the intense computing power needed for large NNs, it is very hard to scale up. They have limited fault tolerance capacity.  
			</div>

			<div class="md-conv">
				### MP-Neuron

				An MP neuron is one of the simplest models. It has one neuron, which accepts binary inputs, and gives binary outputs, based on a user-set threshold $\theta$.

				<figure>
					<img src="./assets/mp-nulong.png" alt="MP-neuron" style="max-width: 350px;">
					<figcaption>MP Neuron. It first sums the inputs, then will output if it is $\geq$ a set threshold.</figcaption>
				</figure>

				We can extend the MP neuron with two types of inputs: excitatory and inhibitory. Excitatory inputs add to the value, and there must be enough excitatory neurons activated to meet the threshold before activation.

				Inhibitory neurons however have **absolute veto power**.

				We can also draw the MP neuron in **Rojas Diagram** form. 

				<figure>
					<img src="./assets/mp-rojas.png" alt="" style="max-width: 350px;">
					<figcaption>Rojas diagram.</figcaption>
				</figure>

				Where the $\theta$ still refers to the threshold, and inhibitory neurons are indicated by a circle after the arrow (see $x_3$). One half of the neuron is black. 

				We can also write the MP neuron in **vector representation**:
			</div>

			<div class="md-conv side">
				Let $\vec{1} = (1, 1, \dots 1)$. Let inputs $\x = (x_1, x_2, \dots x_n)$. 

				The sum $z = \vec{1} \cdot \x$.

				The **haviside function** $H_\theta(z) = \begin{cases} 1 & z \geq \theta \\ 0 & z &lt; \theta \end{cases}$

				Thus $y = H_\theta(\vec{1} \cdot \x)$
			</div>

			<div class="md-conv">
				### To simulate logic gates

				We have basic gates AND, OR, NOT; and derived gates NAND, NOR, XOR, IMPLY.

				We can use an MP neuron to represent the basic gates, by writing out the truth table and finding threshold. 

				<figure>
					<img src="./assets/mp-logic-basic.png" alt="AND OR NOT gates in mp gate" style="max-width: 600px;">
					<figcaption>Basic logic gates as MP neurons</figcaption>
				</figure>
			</div>

			<div class="md-conv side">
				***Exercise.*** Consider the function $f: \{0,1\}^3 \lra \{0,1\}$, given as

				\[
				f(x_1, x_2, x_3) = \begin{cases} 0 & \rm{if } (x_1, x_2, x_3) = (0,0,0) \\
				1 & \rm{otherwise.} \end{cases}
				\]
			</div>

			<button class="collapsible">Answer...</button>
			<div class="ccontent md-conv">
				Hint: This is a 3-input OR gate. The rest is left to the reader (because I cba rn).
			</div>
		</div>

		<div class="colourband">
			<h2 id="perceptron">Perceptron</h2>
		</div>

		<div class="cbox">

			<div class="md-conv">
				### Introduction 

				The MP Neuron is limited: 

				* It only accepts binary ins and outs 
				* Every input is equally important / no weighting
				* $\theta$ must always be set manually / no learning

				A advancement of this is the Rosenblatt **perceptron:**

				<figure>
					<img src="./assets/rosenblatt-perceptron-1.png" alt="" style="max-width: 600px;">
					<figcaption>Rosenblatt perceptron. $\theta$ is learned.</figcaption>
				</figure>

				We can move the $\theta$ from being a part of the perceptron node, and into a **bias**, so that we can use a standard Haviside "step" function: 

				\[
				H = \begin{cases} 1 & z \geq 0 \\ 0 & z &lt; 0 \end{cases}
				\]

				Thus getting 

				<figure>
					<img src="./assets/rosenblatt-perceptron-2.png" alt="" style="max-width: 600px;">
					<figcaption>Bias is moved outside and set to $-\theta$. The step looking thing is the step function.</figcaption>
				</figure>

				Which is representable as 
				\[y = H(\w \cdot \x + b)\]

				We can even pretend the bias is another weight on a constant 1 input, i.e.
				\begin{align}
					\x &= (x_1, x_2, \dots, x_n, 1) \\
					\w &= (w_1, w_2, \dots, w_n, b) \\ 
					y &= H(\vec{w} \cdot \vec{x})
				\end{align}
			</div>

			<div class="md-conv side">
				**Exercise.** A perceptron has two inputs, which have weights $[3,1]$, and a bias of $-3$. Draw the diagram of the perceptron, and predict the output on $\x = [0,1]$ and $\x = [1,1]$
			</div>

			<div class="md-conv">
				### The Perceptron Learning Rule

				(This may not be part of the exam)

				In general:
			</div>
			<div class="codediv">LABEL start:
	choose random \(\w, b : \w\t\x + b = 0\)
	if $\exists$ misclassified node:
		update $\w, b$ 
		GOTO start
	output $\w, b$</div>
				
			<div class="md-conv">
				Recall: Given two vectors $\vec{a}, \vec{b}$:

				\[
				\vec{a} \cdot \vec{b} \begin{cases} = 0 & \rm{Orthogonal } (90^\circ) \\ &gt; 0 & \rm{Acute} \\ &lt; 0 & \rm{Obtuse or reflex} \end{cases}
				\]

				Given $\w \cdot \x$, and a vector $\vec{v}$, $\w \cdot \vec{v} \geq 0 \iff \rm{Angle}(\w, \vec{v}) \leq 90^\circ$.

				We can use this property to detect misclassified nodes, if a $\vec{v}$ has an angle greater than 90. 

				For any vector $\vec{v}$ that is misclassified, we do $\w \lla \w + \alpha y \vec{v}$, where $\alpha$ is the learning rate, and $y$ is 1 or -1 based on which category they're supposed to be in. i.e. add misclassified + node, minus misclassified - node.
			</div>

			<div class="md-conv">

				### Non-linear separability 

				A perceptron can only separate things linearly: it can only draw a straight line in a plane, or a straight plane in a 3D space. 

				XOR is a logic gate which is not linearly separable in 2D. 

				We can solve this by:

				* Replacing the threshold with a more complex function (defeats the point of simplifying it in the first place)
				* Increase the number of layers

				Generally we prefer the second option.

				Since we know that $x_1 \oplus x_2 = (x_1 \lor x_2) \land \lnot (x_1 \land x_2)$, we can split this into two layers:

				<figure>
					<img src="./assets/xor-mlp.png" alt="">
					<figcaption>The XOR gate</figcaption>
				</figure>

				Since we have multiple layers, this is a **Multi-layer Perceptron** (MLP).

				Since a perceptron draws a straight line in 2D space, any polygon can be decomposed into an MLP.
			</div>

			<div class="md-conv left">
				<img src="./assets/mlp-shape-bound.png" alt="" style="float: right; max-width: 200px;">
				***Excersise.*** Given the picture on the right;

				Represent the shape $R$ as a multi-layer perceptron.

				The axes are $x_1$ and $x_2$ respectively.
			</div>

			<button class="collapsible">Answer...</button>
			<div class="ccontent md-conv">
				***Answer.*** This can be decomposed into four lines, going anticlockwise from the top-right:

				\begin{align}
					(1) \; \; 0 &\leq -3x_1 -2x_2 + 6 \\ 
					(2) \; \; 0 &\leq 3x_1 - x_2 + 3 \\ 
					(3) \; \; 0 &\leq x_2 + 2 \\
					(4) \; \; 0 &\leq -x_1 + x_2 + 2
				\end{align}

				They are intersected (ANDed) together. 

				Draw the MLP as appropriate.
			</div>
		</div>

		<div class="colourband">
			<h2 id="prop">Activations and Backpropagations</h2>
		</div>
		

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="./js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
	<script type="text/javascript" src="../../js/markdown.js"></script>
</body>
</html>