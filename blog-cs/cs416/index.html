<!DOCTYPE html>
<html>
<head>
	<title>CS416</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	
	
	<script async id="MathJax-script" type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script defer type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script defer type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
	<script defer type="text/javascript" src="../../js/arrange.js"></script>
	<script defer type="text/javascript" src="../../js/prism.js"></script>
</head>
<body>

<div class="hidden">
	<header>
		<div class="parallax parsmaller">
			<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 7fr 1fr 1fr; grid-column-gap: 10px; padding: 5px; ">
				<div class="column tinycolumn">
					<a href="../../" class="nav">Home</a>
				</div>
				<div class="column tinycolumn">
					<a href="../../blog.html" class="nav">Blog</a>
				</div>
				<div class="column tinycolumn">
					
					<a href="../../about.html" class="nav">About</a>
				</div>
				<div></div>
				<div class="column tinycolumn">
					<a href="https://ko-fi.com/yijunhu" class="nav">Donate</a>
				</div>
				<div class="column">
					<button class="nav dark-light">Dark Mode</button>
				</div>
			</div>
			<div class="cbox"> 		
				<h1>CS416</h1>
				<p class="subheading">
					Optimisation Methods
				</p>
			</div>
		</div>
	</header>

<header>
	<div class="cbox">
		<h1>Contents</h1>
	</div>
</header>


<div class="cbox">
<div class="md-conv">

Similar situation to quantum.

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\t}{^\top}
\renewcommand{\vec}{\mathbf}
\renewcommand{\rm}{\textrm}
\newcommand{\x}{\vec{x}} 
\newcommand{\y}{\vec{y}} 
\newcommand{\p}{\vec{p}} 
\newcommand{\d}{\vec{d}} 
\newcommand{\z}{\vec{z}} 
\newcommand{\w}{\vec{w}} 
\newcommand{\0}{\vec{0}} 
\newcommand{\b}{\vec{b}} 
\newcommand{\dd}{\rm{d}} 
\newcommand{\lra}{\longrightarrow} 
\newcommand{\nm}[1]{\left\lVert #1 \right\rVert}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\det}{det}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\dim}{dim}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\null}{null}
$$

</div>

</div>


<div class="colourband">
	<h2>Linear Regression</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Linear Equations

Series of linear equations are of the form 

\begin{align}
	a_{11} x_1 + \cdots + a_{1n}x_n &= b_1 \\\\
	&\vdots \\\\
	a_{m1} x_1 + \cdots + a_{mn} x_n &= b_m 
\end{align}

Or in matrix form:
$$
\underset{m \times n}{A} \underset{n \times 1}{\x} = \underset{m \times 1}{\vec{b}}
$$

Let the notation $A_{.k}$ represent column $k$, i.e. `A[:, k]`. Therefore 
$$ A\x = x_1 A\_{.1} + \cdots + x_n A\_{.n} $$

Hence $A\x \in \range(A)$, (the dimension of columns), $\forall x \in \R^n$.

Therefore, the solution set $S_{A, \vec{b}} \neq \varnothing$ **if and only if** 
- $b \in \range(A)$
- $\rank(\begin{bmatrix}
	A & \vec{b}
\end{bmatrix} = \rank(A))$ : the rank criterion (matrix concatenation)

> *rank* is the dimension of the vector space generated by a matrix's columns (or rows, whichever is fewer), i.e. the number of **linearly independent** columns/rows.
>
> *full rank* is if the rank is the same as the number of columns (or rows, whichever smaller).

Generally, if a solution $\x^* \in S_{A, b}$, then $\forall \x \in \R^n$: 

\begin{align}
	\x \in S\_{A, b} &\iff A(\x - \x^*) = 0 \\\\ 
	&\iff A\x = A\x^\* \\\\
	&\iff \x - \x^\* \in \null(A)
\end{align}

> Note that the null space of A is the set $\\{ \forall \y : A\y = 0 \\}$.
> 
> Hence $S_{A, b}$ is the "affine set"
> $$ S_{A, \vec{b}} = \\{ \x^* + \z : \z \in \null(A) \\} $$ 
> for all solutions $\x^\*$. You can write this as $\x^\* + \null(A)$.

So $A\x + \vec{b}$ has a unique solution **if and only if** $\null(A) = \\{\0\\}$ -- the trivial null space (no non zero vectors make zero).

***Classes of linear equations.*** Suppose $A$ is **full rank** (since it is not interesting to consider otherwise)

Then $A\x + \vec{b}$ is:
- **over-determined** if $m \geq n$.
	- $\dim(\null(A)) = 0$
	- and if $b \not \in \range(A)$ there are no solutions. In this case we can seek the *best approximate solution* 
- **under-determined** if $m \lt n$
	- $\dim(\null(A)) = n - m \gt 0$ there are *infinite* solutions 
	- We could try to seek a solution with the *minimum norm*
- **square** if $m = n$ 
	- and so the inverse $A^{-1}$ is unique, and there is a unique solution $\x^* = A^{-1}\vec{b}$.

<s-side>

***Fact.*** Let $A \in \R^{m \times n}.$
1. $\rank(A) = n \iff A\t A$ is invertible
2. $\rank(A) = m \iff A A\t$ is invertible.

</s-side>

### Linear Regression 

**Input.** List of data and observations: $(t_1, b_1) \cdots (t_m, b_m)$ 

**Output.** An $\alpha, \beta \in \R$ s.t. a linear function $f(t) = \alpha + \beta t$ best predicts the data labels at hand, i.e. it minimises the error / loss function 
$$
L(\alpha, \beta) = \sum_{i=1}^m \varepsilon_i^2 
$$
Where $\varepsilon_i = f(t\_i) - b\_i$. 

***Necessary conditions for a minimiser.*** 

**(1)** $\nabla L(\alpha, \beta) = \0$, where 

\begin{align}
	\nabla L(\alpha, \beta) &= \begin{bmatrix}
		\frac{\partial L }{\partial \alpha } \\\\ \frac{\partial L }{\partial \beta  }
	\end{bmatrix} = \begin{bmatrix}
		\frac{\partial  }{\partial \alpha  } \sum (\alpha + \beta t_i - b_i)^2 \\\\
		\frac{\partial  }{\partial \beta } \\;-"-
	\end{bmatrix}	\\\\
	&= \begin{bmatrix}
		2 \sum (\alpha + \beta t\_i - b\_i) \\\\
		2 \sum (\alpha + \beta t\_i - b\_i) \cdot t_i
	\end{bmatrix}
\end{align}

**(3)** Let 
$$
\begin{matrix}
	A = \begin{bmatrix}
		1 & t_1 \\\\ 
		\vdots & \\\\ 
		1 & t_m
	\end{bmatrix} &
	\vec{b} = \begin{bmatrix}
		b_1 \\\\ \vdots \\\\ b_m
	\end{bmatrix} & 
	\x = \begin{bmatrix}
		\alpha \\ \beta
	\end{bmatrix} 
\end{matrix}
$$

Then 
$$
A\x = \vec{b} \equiv A\t A \x = A\t \vec{b}
$$
Is the normal equation for $A\x = \vec{b}$.

Thus (3) has a unique solution $\x^* = (A\t A)^{-1} A\t \vec{b}$.

### Linear Least Squares 

<b-blue>

***Thm.*** For $A \in \R^{m \times n}, \b \in \R^n$,

Let $\varepsilon = \varepsilon(\x) = A\x - \b$,

The **General least squares problem** is: find an $\x$ that **minimises** 
$$
\sum_{i=1}^m \varepsilon_i ^2 = \varepsilon \t \varepsilon = (A\x - \b)\t (A\x - \b)
$$

$\x$ is called a **least squares solution**, and 

- The set of all LS solutions == the set of solutions to the system of normal equations $A\t A \x = A\t \b$. 
- There exists a unique LS soln **iff** $\rank(A) = n$, then $\x = (A\t A)^{-1}A\t \b$. 
- If $A\x - \b$ is *consistent* then the solution set is the same as the LS set. 

</b-blue>

> Consistent: there is at least one set of values of unknowns that satisfies each equation in the system.
</div>

<button class="collapsible">Proof...</button>
<div class="ccontent md-conv">

***Proof.*** (x minimises $\varepsilon\t\varepsilon \implies$ x satisfies normal eqns.)

We know $\x\t A\t \b = \b\t A \x$ due to scalar symmetry / dot product symmetry. 

So $\varepsilon \t \varepsilon = (A\x - \b)\t (A\x - \b)$, which is 
$$
\x\t A\t A \x - 2 \x\t A\t \b + \b\t\b
$$
Let this be $f(\x)$. 



Solve by finding $\nabla f$ for each $x_i$:

$$
\frac{\partial f }{\partial x_i  } = \frac{\partial \x\t }{\partial x_i  } A\t A \x + x\t A\t A \frac{\partial \x  }{\partial x_i  } - 2\frac{\partial \x\t  }{\partial x_i  } A\t \b.
$$

Note that $\frac{\partial \x  }{\partial x_i }$ is the unit vector $e_i$ which has a 1 in the ith place and 0 everywhere else, so 

\begin{align}
\frac{\partial f }{\partial x_i  } &= e_i\t A\t A \x + \x \t A\t A e\_i - 2e\_i\t A\t \b \\\\
&= 2e_i \t A\t A \x - 2e\_i \t A\t \b 
\end{align}

Note that $e_i\t A\t$ is just the vector row $i$ of matrix A $A\t_{i*}$, and so this just becomes 

\begin{align}
	\implies 2 A\t_{i*} A\x - 2A\t_{i*} A \b &= \0 \pod{\rm{for minimum}} \\\\
	\implies A\t_{i*} A\x &= A\t_{i*} A \b \\\\ 
	\implies A\t A \x &= A\t \b 
\end{align}

We can show any minimum of this system satisfies LS, but we want to demonstrate that *all* solutions are LS solutions.

If $\z$ is a soln to the normal eqns, then  let
$$ f(\z) = \b\t\b - \z\t A\t \b 
$$
For any other $\y \in \R^{n \times 1}$ let $\y = \z + \vec{u} $ i.e. $\vec u = \y - \z$, then we would get that 
$$
f(\y) = f(\z) + (A\vec u)\t(A \vec u) = f(\z) + \vec v\t\vec v$ 
$$
for some vector $\vec v$, and since the dot product $\vec v \t \vec v$ must be positive, we know that $$f(\z) \leq f(\y) \forall \y \in \R^{n \times 1}$$
Hence it is a minimum. $\Box$
</div>

<div class="md-conv">

> Differentiating a matrix is just like differentiating scalars, and so:
> $$ \begin{bmatrix}
	\frac{\partial U  }{\partial x }
\end{bmatrix}\_{ij} = \frac{\partial u\_{ij}  }{\partial x } $$ 
> Similarly
> $$ \frac{\partial U + V  }{\partial x } = \frac{\partial U  }{\partial x  } + \frac{\partial V }{\partial x } $$ 
> $$ \frac{\partial UV  }{\partial x  } = \frac{\partial U  }{\partial x  }V + U \frac{\partial V  }{\partial x  } $$ 
> i.e. a useful one to know is that the gradient of $\x\t M \x$ is given as just $2M\x$.

</div>

</div>

<div class="colourband">
	<h2>Smooth Optimisation</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Gradient and Descent

<b-blue>

***Definition.*** (Partial Derivatives, gradient, smoothness).

Let $f : \R^n \lra \R$, and let $x \in \R^n = (x_1 .. x_n)$

The **partial derivative** $\nabla_{x_i} f(\w) = \frac{\partial f(\w)  }{\partial x_i  }$ 
- is defined to be a derivative of a function $\phi$, $\phi_i' (w)$, where 
$$ \phi_i(z) = f(w_1 .. w_{i-1}, z, w_{i+1}.. w_n)$$ 
- i.e. the partial derivative for an element of a vector, fix all other elements and derive that one only.

The **gradient** $\nabla_\x f(\w)$ (often without the subscript) 
$$
\nabla f(\w) = \begin{bmatrix}
	\nabla_{x_1} f(\w) \\\\ 
	\vdots \\\\ 
	\nabla_{x_n} f(\w)
\end{bmatrix}
$$
- So $\nabla f : \R^n \lra \R^n$, if defined for all elements in a domain (**well defined**)

$f$ is **smooth** if its gradient is *continuous* / it is continuously differentiable.

</b-blue>
<p></p>
<b-blue>

***Thm.*** (First order taylor theorem) **IMPORTANT** 

Let $f: \R^n \lra \R$ be smooth, then we can use first order taylor approximations:

For a small vector distance $\p$:

- The **integral form**:
$$
f(\x + \p) = f(\x) + \int_0^1 \nabla f (\x + \gamma \p)\t \p \\;\dd \gamma 
$$
- The **mean value form**:
$$
f(\x + \p) = f(\x) + \nabla f(\x + \gamma \p)\t\p
$$
For some $\gamma \in (0,1)$

</b-blue>

<s-side>

***Corollary.*** (Local linear approximation)

If $f : \R^n \lra \R$ is smooth at $\x$, then for some small $\p$ 
$$ f(\x + \p) = f(\x) + \nabla f(\x)\t\p + o(\nm{p})$$

Where little $o(\nm p)$ notation means that the "residual" ratio $\frac{ r(\p) }{ \p }$ for some residual function (the "error" I guess) is vanishingly small compared ot the norm of $\p$

</s-side>

</div>

<button class="collapsible">Proof (corrolary)</button>
<div class="ccontent md-conv">

***Proof.*** 

- By mean value form 
$$f(\x + \p) = f(\x) + \nabla f(\x + \gamma \p) \t \p$$
- Adding and subtracting the same thing
$$= f(\x) + \nabla f (\x)\t \p + (\nabla f(\x + \gamma \p) - \nabla f(\x))\t \p$$ 
- which is 
$$ = f(\x) + \nabla f (\x)\t \p + O(\nm{\nabla f(\x + \gamma \p) - \nabla f(\x)} \nm{\p})$$ 
- And we claim this big-O term is just little-o norm of p, by claiming that 
$$ \lim_{\nm p \lra 0} O(\cdots) \longrightarrow\longrightarrow 0$$
As $\nm p \lra 0$, the gradient term tends to $\nabla f(\x)$, and since the gradient is continuous we can do this. Thus the ratio 
$$
\frac{ \nm{\nabla f(\x + \gamma \p - \nabla f(\x))} \nm{\p} }{ \nm \p } \lra \nm{\nabla f(\x) - \nabla f(\x)} \lra 0
$$
Therefore 
$$
f(\x + \p) = f(\x) + \nabla f(\x)\t\p + o(\nm{p}) \Box
$$

</div>


<div class="md-conv">
<p></p>
<b-blue>

***Def.*** A vector $\d \in \R^n$ is a **descent direction** if for a function at a point $f(\x)$, 
$$
f(\x + t\d) \lt f(\x)
$$
For all sufficiently small values of $t$. 

</b-blue>
<p></p>
<s-side>

***Fact.*** If $f$ is smooth in a neighbourhood of $\x$ (close to x), then every direction s.t. $\nabla f(\x)\t \d \lt 0$ is a valid descent direction.
</s-side>
</div>

<button class="collapsible">Proof (fact)</button>
<div class="ccontent md-conv">

***Proof.*** Since the gradient is continuous, there is some $t^* \gt 0$ where 
$$
\nabla f(\x + t\d) \t \d \lt 0 \pod{\forall t \in [0, t^*]}
$$
By the mean value form of taylor theorem, 
$$
f(\x + t\d) = f(\x) + t \nabla f (\x + \gamma t \d) \cdot \d
$$
Since $\gamma t \lt t$ as gamma is between 0 and 1, $f(\x + t\d) \lt f(\x)$. Thus it is a descent vector. $\Box$

</div>

<div class="md-conv">
<p></p>
<s-side>

***Corrolary.*** If $\x^\*$ is a **local minimiser** for a smooth $f$, then 
$$
\nabla f(\x^*) = \0
$$

</s-side>

### Gradient Descent 
> not to be confused with the previous section "Gradient and Descent"

<s-side>

***Fact.*** (On steepest descent directions for a point where the gradient is nonzero)

Among directions $\d$ with unit length, the one that minimises $\nabla f(\x) \cdot \d$ is 
$$
-\frac{ \nabla f(\x)  }{ \nm{\nabla f(\x)} }
$$

</s-side>

<b-blue>
	
***Algorithm.*** Gradient Descent $GD(\alpha_k, T)$:
1. Pick $\x_0 \in \R^n = \dom(f)$
2. **for** $k = 0..T-1$ steps:
	1. choose step size $\alpha_k \gt 0 $
	2. set $\x\_{k+1} := \x\_k - \alpha\_k \nabla f(\x\_k)$
3. **return** $\x_T$

</b-blue>

Also written as $GD(\alpha_k)$ and the algorithm runs until convergence.

In terms of the step length, too small and it is too slow, too large and it overshoots. 

<b-blue>

***Def.*** $f$ is **L-Lipschitz continuous** if
$$
|f(\x) - f(\y)| \leq L \nm{\x - \y} 
$$
For some constant $L$.

***Def.*** $f$ is **L-smooth** if its gradient is L-lipschitz.

</b-blue>

<s-side>

***Lemma.*** (Quadratic UB for L-smooth functions)

If $f$ is L-smooth, then 
$$
f(\x + \p) \leq f(\x) + \nabla f(\x)\t\p + \frac{ L }{ 2 } \nm{\p}^2
$$
($\nm\p^2 \equiv \p\t\p$)

</s-side>
</div>

<button class="collapsible">Proof (Lemma)</button>
<div class="ccontent md-conv">
***Proof.*** 

Take by integral form of Taylor's theorem and taking away gradient:

$$
f(\x + \p) - f(\x) - \nabla f(\x)\t\p = \int_0^1 (\nabla f(\x + \gamma \p) - \nabla f(\x))\t\p \dd\gamma
$$

By the *cauchy schwarz inequality*:
$$
\cdots \leq \int_0^1 \nm{\nabla f (\x + \gamma\p) - \nabla f(\x)} \nm{\p} \dd\gamma
$$

Then by the L-smoothness of $f$:
\begin{align}
	&= \int_0^1 L \nm{\x + \gamma \p - \x}\nm\p \dd \gamma \\\\
	&= L \lVert \p^{2} \rVert \int_0^1 \gamma \dd \gamma = \frac{L}{2} \lVert \p^{2} \rVert &\Box
\end{align}
</div>


<div class="md-conv">

***For L-smooth functions, pick $GD(\frac 1 L)$.***

The motivation for this goes as:
- via the lemma, the improvement is 
$$ F(\x + \alpha\d) - f(\x) \leq \alpha \nabla f(\x)\t\d + \alpha^2 \frac L 2 \nm{\d}^2$$
- And for the descent direction $\d$ to be $\nabla f(\x)$, then the choice $\alpha = \frac 1 L$ minimises the RHS of the function

<s-side>
	
***Fact.*** If $f$ is L-smooth, then $\x_0, \x_1, \dots$ iterates of GD with fixed step 1/L, the improvement over 1 step is 
$$ f(\x_{k+1}) - f(\x_k) \leq -\frac{1}{2L} \nm{\nabla f(\x ^k)}^2 $$ 

</s-side>

The proof of this fact can be found by subbing $\alpha = \frac 1 L$ into the improvement equation.

The one step improvement of GD is *at least as much* as this bound.

<b-blue>

***Theorem.*** (Convergence rate of $GD(\frac 1 L)$ for L-smooth functions)

- Let $f$ be L-smooth 
- Let the global minimiser $f^* \leq f(\x) \\;\forall \x \in \dom(f)$. 
- Let $G = f(\x_0) - f^*$ be the initial optimality gap.

Then, after $t$ steps of $GD(\frac 1 L)$, we can find some $\x_k \in \dom(f)$ such that 
$$
\nm{\nabla f(\x_k)} \leq \sqrt{\frac{2LG}{t}} $$

Hence we can find a $\x_k : \nm{\nabla f(\x_k)} \leq \varepsilon$ for a small $\varepsilon$, which is an **approximate stationary point** after $\frac{2LG}{\varepsilon^2}$ steps.

</b-blue>

</div>

<button class="collapsible">Proof (theorem)</button>
<div class="ccontent md-conv">
***Proof.*** We can sum up all one step improvements:

\begin{align}
	\sum_{k=1}^{t-1} f(\x_{k+1}) - f(\x_k) &= f(\x_t) - f(\x_0) \\\\
	f(\x_t) - f(\x_0) &\leq - \frac{1}{2L} \sum_{k=0}^{t-1} \nm{\nabla f (\x_k)}^2 
\end{align}

Hence 
$$
\sum_{k=0}^{t-1} \nm{\nabla f(\x_k)}^2 \leq 2L(f(\x_0) - f(\x_t)) \leq 2LG 
$$
So as $t$ increases, we are always bound by a constant. Therefore the gradient converges to zero:
$$
\lim_{k \lra \infty} \nm{\nabla f(\x_k)} = 0
$$

So after $t$ steps, 
$$
\min_{k = 0..t-1} \nm{\nabla f(\x_k)}^2 \leq \frac{1}{t} \sum_{k= 1}^{t-1} \nm{\nabla f(\x_k)}^2 \leq \frac{2LG}{t} 
$$
And hence the square root form. Note The minimum of a set is never more than the average.

The second point follows on from the first point. 

</div>

<div class="md-conv">

So far, we only ensure that *some* point the gradient is smallest, what is guaranteed is that the function decreases monotonically. 

We do not guarantee GD gets the exact minimum, only the approximate minimum. Moreover, technically GD only finds stationary points, convexity of functions guarantees it is a minimum. 


### Convex Sets and functions

<b-blue>

***Def.*** A set $\Omega \subseteq \R^n$ is a **convex set** IF for any $x, y, \in \Omega$, and any $\alpha \in [0,1]$, we have that the convex combination (i.e. linear interpolation) always satisfies:
$$
(1-\alpha)x + \alpha y \in \Omega
$$

</b-blue>
<b-blue>

***Def.*** A function $f: \R^n \lra \R$ is **convex** IF $\forall \x,\y \in \dom(f),\; \alpha \in [0,1]$, that 

$$
f((1-\alpha)\x + \alpha\y) \leq (1-\alpha)f(\x) + \alpha f(\y)
$$

</b-blue>
<p></p>
<b-blue>

***Thm.*** Suppose $f : \Omega \lra \R$ is convex, and $\Omega$ is convex and closed. Then 
1. Every **local minimiser** of $f$ is also the **global minimiser**.
2. The set of all global minimisers is convex. 

</b-blue>
<b-blue>

***Thm.*** (First order characterisation of convexity)

If $f:\R^n \lra \R$ is smooth and convex, then $\forall \x, \y \in \R^n$
$$
f(\y) \geq f(\x) + \nabla f(\x)\t (\y - \x)
$$
This is the *first order linear approximation of $f(\x)$* via the taylor's theorem.

</b-blue>
<p></p>
</div>

<button class="collapsible">Proof</button>
<div class="ccontent md-conv">

***Proof.*** Condition for convexity:
\begin{align}
	f((1-\alpha)\x + \alpha \y ) &\leq (1- \alpha) f(\x) + \alpha f(\y). \\\\
	f((1-\alpha)\x + \alpha \y ) &= f(\x + \alpha \y - \alpha \x) \\\\
	&= f(\x + \alpha(\y - \x)) \\\\
	&= f(\x) + \alpha \nabla f(\x)\t (\y - \x) + o(\nm{\alpha (\y - \x)}) 
\end{align}

We fix $\x, \y$ and vary $\alpha$: 

$$
 f(\x) + \alpha \nabla f(\x)\t (\y - \x) + o(\nm{\alpha})
$$

So 

\begin{align}
	f(\x) + \alpha \nabla f(\x)\t (\y - \x)  + o(\alpha) &\leq (1-\alpha)f(\x) + \alpha f(\y) \\\\
	&\leq f(\x) - \alpha f(\x) + \alpha f(\y) \\\\
	\nabla f(\x)\t (\y - \x) + o(1) &\leq -f(\x) + f(\y) \\\\
	f(\y) &\geq f(\x) + \nabla f(\x)\t (\y - \x) & \Box 
\end{align}

</div>
<div class="md-conv">
<p></p>
<b-blue>

***Thm.*** For convex functions, a 0 gradient is both **necessary** and **sufficient** for a local minimum.

</b-blue>

### Fixed Step GD for Convex Functions 

<b-blue>

***Thm.**(Convergence rate of $GD(\frac 1 L)$ for L-smooth convex functions)* 

Let $f$ be L-smooth and convex. Let $\x^\*$ be a minimiser, and let the minimum value $f^\* = f(\x^\*)$. 

Let the *distance* $D = \nm{\x_0 - \x^*}$ be the "initial gap". $\x_0 .. \x_t$ are iterates of gradient descent.

**Then** at step $t$ of gradient descent,
$$
f(\x_t) - f^* \leq \frac{L \cdot D^2}{2t}
$$

**Hence** $f(\x_t) - f^* \leq \varepsilon$ for a small $\varepsilon$ after $t \geq \frac{LD^2}{2\varepsilon}$ steps.

</b-blue>

Therefore iterations is proportional to $\frac 1 \varepsilon$. 

</div>
<button class="collapsible">Proof... </button>
<div class="ccontent md-conv">

***Proof.*** Recall for gradient descent:

$$
f(\x_{k+1}) \leq f(\x_k) - \frac{ 1 }{ 2L } \nm{\nabla f (\x_k)}^2 
$$

Recall for convexity for any points $\z,\x$:

$$
f(\z) \geq f(\x) + \nabla f(\x)\t(\z-\x) \geq f(\x)
$$

Therefore since $f$ is convex 
$$
f(\x^*) \geq f(\x_k) + \nabla f(\x_k) \t (\x^* - \x_k)
$$
Reverse and sub in for $f(\x_k)$ to get
\begin{align}
	f(\x_{k+1}) &\leq f(\x^\*) + \nabla f(\x\_k)\t (\x_k - \x^\*) - \frac{ 1 }{ 2L  }\nm{\nabla f(\x\_k)}^2   \\\\ 
	&\leq f(\x^\*) + \frac{L}{2} \left( 2(\x_k - \x^\*) \cdot \frac 1 L \nabla f(\x\_k) - \frac{ 1 }{ L^2  }\nm{\nabla f(\x\_k)}^2 \right) \\\\
	&\leq f(\x^\*) + \frac{L}{2} \left( 2(\x_k - \x^\*) \cdot \frac 1 L \nabla f(\x\_k) - \lVert \frac{ 1 }{ L } \nabla f(\x\_k) \rVert ^2 \right) \\\\ 
	&\leq f(\x^\*) + \frac{L}{2} \left( \nm{\x_k - \x^\*}^2 - \lVert \x_k - \x^\* - \frac 1 L  \nabla f(\x_k) \rVert^2 \right)
\end{align}

Where the last step comes from an identity over vectors $\vec{a}, \vec{b}$ where 
$$
\nm{\vec{a}}^2 - \nm{\vec{a} - \vec{b}}^2 = 2\vec{a}\t\b - \nm{\b}^2
$$

Now shift $f(\x^*)$ over to the left hand side, and sum over all $k = 0..t-1$:


$$
\begin{align*}
\sum\limits_{k=0}^{t-1} (f(\x_{k+1}) - f^\*) &\leq \frac{L}{2} \sum\limits_{k=0}^{t-1} ( \nm{\x_k - \x^\*}^2 - \nm{\x_{k+1} + \x^\*}^2)\\
&\leq \frac{L}{2} (\nm{\x_0 - \x^\*}^2 - \nm{\x_t - \x^\*}^2)\\
&\leq \frac{LD^2}{2}
\end{align*}
$$

Hence because $f(\x_0) \leq f(\x\_1( \leq \cdots \leq f(\x\_t)))$
$$
t(f(\x_t) - f^\*) \leq \sum\limits_{k=0}^{t-1} (f(\x_{k+1}) - f^\*) \leq \frac{LD^2}{2}
$$
Therefore 
$$
f(\x_t) - f^* \leq \frac{LD^2}{2t} \\;\\;\\;\Box 
$$

</div>
<div class="md-conv">

However the bound $t \geq \frac{ LD^2  }{ 2\varepsilon }$ is still not good. We would like ideally something that would produce a new digit of precision every step, i.e. running in time $\log \frac{1}{\varepsilon}$.

</div>

</div>

	

	<footer>
		<div class="cbox">
			<div class="columncontainer ctwo" id="fc2">
			</div>
			<script type="text/javascript" src="../../js/footerGen.js"></script>
		</div>
	</footer>

</div>

</body>
</html>