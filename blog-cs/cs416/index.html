<!DOCTYPE html>
<html>
<head>
	<title>CS416</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	
	
	<script async id="MathJax-script" type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script defer type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script defer type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
	<script defer type="text/javascript" src="../../js/arrange.js"></script>
	<script defer type="text/javascript" src="../../js/prism.js"></script>
</head>
<body>

<div class="hidden">
	<header>
		<div class="parallax parsmaller">
			<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
				<div class="column tinycolumn">
					<a href="../../" class="nav">Home</a>
				</div>
				<div class="column tinycolumn">
					<a href="../../blog.html" class="nav">Blog</a>
				</div>
				<div class="column tinycolumn">
					
					<a href="../../about.html" class="nav">About</a>
				</div>
				<div></div>
				<div class="column">
					<button class="nav dark-light">Dark Mode</button>
				</div>
			</div>
			<div class="cbox"> 		
				<h1>CS416</h1>
				<p class="subheading">
					Optimisation Methods
				</p>
			</div>
		</div>
	</header>

<header>
	<div class="cbox">
		<h1>Contents</h1>
	</div>
</header>


<div class="cbox">
<div class="md-conv">

Similar situation to quantum.

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\t}{^\top}
\renewcommand{\vec}{\mathbf}
\renewcommand{\rm}{\textrm}
\newcommand{\x}{\vec{x}} 
\newcommand{\y}{\vec{y}} 
\newcommand{\p}{\vec{p}} 
\newcommand{\d}{\vec{d}} 
\newcommand{\z}{\vec{z}} 
\newcommand{\0}{\vec{0}} 
\newcommand{\b}{\vec{b}} 
\newcommand{\nm}[1]{\left\lVert #1 \right\rVert}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\det}{det}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\dim}{dim}
\DeclareMathOperator{\range}{range}
\DeclareMathOperator{\null}{null}
$$

</div>

</div>


<div class="colourband">
	<h2>Linear Regression</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Linear Equations

Series of linear equations are of the form 

\begin{align}
	a_{11} x_1 + \cdots + a_{1n}x_n &= b_1 \\\\
	&\vdots \\\\
	a_{m1} x_1 + \cdots + a_{mn} x_n &= b_m 
\end{align}

Or in matrix form:
$$
\underset{m \times n}{A} \underset{n \times 1}{\x} = \underset{m \times 1}{\vec{b}}
$$

Let the notation $A_{.k}$ represent column $k$, i.e. `A[:, k]`. Therefore 
$$ A\x = x_1 A\_{.1} + \cdots + x_n A\_{.n} $$

Hence $A\x \in \range(A)$, (the dimension of columns), $\forall x \in \R^n$.

Therefore, the solution set $S_{A, \vec{b}} \neq \varnothing$ **if and only if** 
- $b \in \range(A)$
- $\rank(\begin{bmatrix}
	A & \vec{b}
\end{bmatrix} = \rank(A))$ : the rank criterion (matrix concatenation)

> *rank* is the dimension of the vector space generated by a matrix's columns (or rows, whichever is fewer), i.e. the number of **linearly independent** columns/rows.
>
> *full rank* is if the rank is the same as the number of columns (or rows, whichever smaller).

Generally, if a solution $\x^* \in S_{A, b}$, then $\forall \x \in \R^n$: 

\begin{align}
	\x \in S\_{A, b} &\iff A(\x - \x^*) = 0 \\\\ 
	&\iff A\x = A\x^\* \\\\
	&\iff \x - \x^\* \in \null(A)
\end{align}

> Note that the null space of A is the set $\\{ \forall \y : A\y = 0 \\}$.
> 
> Hence $S_{A, b}$ is the "affine set"
> $$ S_{A, \vec{b}} = \\{ \x^* + \z : \z \in \null(A) \\} $$ 
> for all solutions $\x^\*$. You can write this as $\x^\* + \null(A)$.

So $A\x + \vec{b}$ has a unique solution **if and only if** $\null(A) = \\{\0\\}$ -- the trivial null space (no non zero vectors make zero).

***Classes of linear equations.*** Suppose $A$ is **full rank** (since it is not interesting to consider otherwise)

Then $A\x + \vec{b}$ is:
- **over-determined** if $m \geq n$.
	- $\dim(\null(A)) = 0$
	- and if $b \not \in \range(A)$ there are no solutions. In this case we can seek the *best approximate solution* 
- **under-determined** if $m \lt n$
	- $\dim(\null(A)) = n - m \gt 0$ there are *infinite* solutions 
	- We could try to seek a solution with the *minimum norm*
- **square** if $m = n$ 
	- and so the inverse $A^{-1}$ is unique, and there is a unique solution $\x^* = A^{-1}\vec{b}$.

<s-side>

***Fact.*** Let $A \in \R^{m \times n}.$
1. $\rank(A) = n \iff A\t A$ is invertible
2. $\rank(A) = m \iff A A\t$ is invertible.

</s-side>

### Linear Regression 

**Input.** List of data and observations: $(t_1, b_1) \cdots (t_m, b_m)$ 

**Output.** An $\alpha, \beta \in \R$ s.t. a linear function $f(t) = \alpha + \beta t$ best predicts the data labels at hand, i.e. it minimises the error / loss function 
$$
L(\alpha, \beta) = \sum_{i=1}^m \varepsilon_i^2 
$$
Where $\varepsilon_i = f(t\_i) - b\_i$. 

***Necessary conditions for a minimiser.*** 

**(1)** $\nabla L(\alpha, \beta) = \0$, where 

\begin{align}
	\nabla L(\alpha, \beta) &= \begin{bmatrix}
		\frac{\partial L }{\partial \alpha } \\\\ \frac{\partial L }{\partial \beta  }
	\end{bmatrix} = \begin{bmatrix}
		\frac{\partial  }{\partial \alpha  } \sum (\alpha + \beta t_i - b_i)^2 \\\\
		\frac{\partial  }{\partial \beta } \\;-"-
	\end{bmatrix}	\\\\
	&= \begin{bmatrix}
		2 \sum (\alpha + \beta t\_i - b\_i) \\\\
		2 \sum (\alpha + \beta t\_i - b\_i) \cdot t_i
	\end{bmatrix}
\end{align}

**(3)** Let 
$$
\begin{matrix}
	A = \begin{bmatrix}
		1 & t_1 \\\\ 
		\vdots & \\\\ 
		1 & t_m
	\end{bmatrix} &
	\vec{b} = \begin{bmatrix}
		b_1 \\\\ \vdots \\\\ b_m
	\end{bmatrix} & 
	\x = \begin{bmatrix}
		\alpha \\ \beta
	\end{bmatrix} 
\end{matrix}
$$

Then 
$$
A\x = \vec{b} \equiv A\t A \x = A\t \vec{b}
$$
Is the normal equation for $A\x = \vec{b}$.

Thus (3) has a unique solution $\x^* = (A\t A)^{-1} A\t \vec{b}$.

### Linear Least Squares 

<b-blue>

***Thm.*** For $A \in \R^{m \times n}, \b \in \R^n$,

Let $\varepsilon = \varepsilon(\x) = A\x - \b$,

The **General least squares problem** is: find an $\x$ that **minimises** 
$$
\sum_{i=1}^m \varepsilon_i ^2 = \varepsilon \t \varepsilon = (A\x - \b)\t (A\x - \b)
$$

$\x$ is called a **least squares solution**, and 

- The set of all LS solutions == the set of solutions to the system of normal equations $A\t A \x = A\t \b$. 
- There exists a unique LS soln **iff** $\rank(A) = n$, then $\x = (A\t A)^{-1}A\t \b$. 
- If $A\x - \b$ is *consistent* then the solution set is the same as the LS set. 

</b-blue>

> Consistent: there is at least one set of values of unknowns that satisfies each equation in the system.
</div>

<button class="collapsible">Proof...</button>
<div class="ccontent md-conv">

***Proof.*** (x minimises $\varepsilon\t\varepsilon \implies$ x satisfies normal eqns.)

We know $\x\t A\t \b = \b\t A \x$ due to scalar symmetry / dot product symmetry. 

So $\varepsilon \t \varepsilon = (A\x - \b)\t (A\x - \b)$, which is 
$$
\x\t A\t A \x - 2 \x\t A\t \b + \b\t\b
$$
Let this be $f(\x)$. 



Solve by finding $\nabla f$ for each $x_i$:

$$
\frac{\partial f }{\partial x_i  } = \frac{\partial \x\t }{\partial x_i  } A\t A \x + x\t A\t A \frac{\partial \x  }{\partial x_i  } - 2\frac{\partial \x\t  }{\partial x_i  } A\t \b.
$$

Note that $\frac{\partial \x  }{\partial x_i }$ is the unit vector $e_i$ which has a 1 in the ith place and 0 everywhere else, so 

\begin{align}
\frac{\partial f }{\partial x_i  } &= e_i\t A\t A \x + \x \t A\t A e\_i - 2e\_i\t A\t \b \\\\
&= 2e_i \t A\t A \x - 2e\_i \t A\t \b 
\end{align}

Note that $e_i\t A\t$ is just the vector row $i$ of matrix A $A\t_{i*}$, and so this just becomes 

\begin{align}
	\implies 2 A\t_{i*} A\x - 2A\t_{i*} A \b &= \0 \pod{\rm{for minimum}} \\\\
	\implies A\t_{i*} A\x &= A\t_{i*} A \b \\\\ 
	\implies A\t A \x &= A\t \b 
\end{align}

We can show any minimum of this system satisfies LS, but we want to demonstrate that *all* solutions are LS solutions.

If $\z$ is a soln to the normal eqns, then  let
$$ f(\z) = \b\t\b - \z\t A\t \b 
$$
For any other $\y \in \R^{n \times 1}$ let $\y = \z + \vec{u} $ i.e. $\vec u = \y - \z$, then we would get that 
$$
f(\y) = f(\z) + (A\vec u)\t(A \vec u) = f(\z) + \vec v\t\vec v$ 
$$
for some vector $\vec v$, and since the dot product $\vec v \t \vec v$ must be positive, we know that $$f(\z) \leq f(\y) \forall \y \in \R^{n \times 1}$$
Hence it is a minimum. $\Box$
</div>

<div class="md-conv">

> Differentiating a matrix is just like differentiating scalars, and so:
> $$ \begin{bmatrix}
	\frac{\partial U  }{\partial x }
\end{bmatrix}\_{ij} = \frac{\partial u\_{ij}  }{\partial x } $$ 
> Similarly
> $$ \frac{\partial U + V  }{\partial x } = \frac{\partial U  }{\partial x  } + \frac{\partial V }{\partial x } $$ 
> $$ \frac{\partial UV  }{\partial x  } = \frac{\partial U  }{\partial x  }V + U \frac{\partial V  }{\partial x  } $$ 
> i.e. a useful one to know is that the gradient of $\x\t M \x$ is given as just $2M\x$.

</div>

</div>

	

	<footer>
		<div class="cbox">
			<div class="columncontainer ctwo" id="fc2">
			</div>
			<script type="text/javascript" src="../../js/footerGen.js"></script>
		</div>
	</footer>

</div>

</body>
</html>