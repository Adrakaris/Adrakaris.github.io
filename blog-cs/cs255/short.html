<!DOCTYPE html>
<html>
	<head>
		<title>CS255</title>
		<meta charset="utf-8">
		<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
		<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
		<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
		<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="./index.html" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="../../blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="../../about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS255 Abridged</h1>
                    <p class="subheading">Artificial Intelligence</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Introduction</h1>
			</div>
		</header>

		<!-- <div class="buttonwrapper beside" >
			<a href="./index-zh.html">简体中文</a>
		</div> -->
		<!-- REMEMBER TO DO! -->

		<div class="cbox">
			<p>
				AI is a massive module, filled with a lot of important things and redundant fluff. My aim here is to rewrite my notes for the module to remove all of the fluff, which not only makes the task easier for me, but also means there's less filler content. <b>This page assumes some inherent knowledge from AI and its keywords</b>, as well as knowledge of logic from CS130 or CS262 -- it will not do to read it from scratch.
			</p>
			<p>
				<b>These are also not guaranteed to be complete</b>.
			</p>
			<p>
				The old long one can be found <a href="./">here</a>.
			</p>

			<p>
				Looking through the past exam papers, there are 7 overall topics that are included, some more than others, being
			</p>

			<ul>
				<li><b>Conditional Probability and Bayes' Theorem</b>
				</li>
				<li>
					<b>CSPs</b>
				</li>
				<li>
					<b>Graph Searching and Heuristics</b>
				</li>
				<li>
					Reinforcement (Q-) Learning 
				</li>
				<li>
					Knowledge Bases and Rule-based systems
				</li>
				<li>
					Partial Order Planning 
				</li>
			</ul>

			<p>
				... I may also do multiagent systems.
			</p>

			<h2>Contents</h2>

			<ul>
				<li><a href="#search">Search</a></li>
				<li><a href="#csp">Constraint Satisfaction</a></li>
				<li><a href="#probability">Conditional Probability and Bayes</a></li>
				<li><a href="#kbs">Knowledge Bases and Rule Based Systems</a></li>
				<li><a href="#pop">Partial Order Planning</a></li>
				<li><a href="#reinforcement">Reinforcement Learning</a></li>
			</ul>
		</div>

		
		<div class="colourband">
			<h2 id="search">Search</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#sea-1">Uninformed Search</a></li>
				<li><a href="#sea-2">Informed Search</a></li>
				<li><a href="#sea-3">Cycle Checking and Path Pruning</a></li>
				<li><a href="#sea-4">More Searching</a></li>
				<li><a href="#sea-5">Finding Heuristics</a></li>
				<li><a href="#sea-6">Adversarial Search</a></li>
				<li><a href="#sea-7">Minimax</a></li>
				<li><a href="#sea-8">Games with Chance</a></li>
			</ol>

			<p>
				<b>Search</b> is one of the most essential forms of problem solving. It entails making moves along a problem space (usually nodes in a graph) in order to try get to a goal. 
			</p>
			
			<p>
				There are two main methods of searching: <i>Informed</i> and <i>Uninformed</i>.
			</p>

			<p>
				In all cases, assume we have reduced the problem down to a graph of nodes, where searching algorithms is at home. Specifically, often we want to talk about a tree graph. Trees have a root node and some goal nodes further down.
			</p>

			<p>
				Graph searching is pretty much no different from tree searching, except that we use \(\langle x, y, z \rangle\) to represent paths.
			</p>

			<h3 id="sea-1">Uninformed Search</h3>

			<p>
				Uninformed does not use information from the question - it is a brute force method.
			</p>

			<h4>Generic Tree Search</h4>

			<p>
				In the general case, a search, starting from the root node of a tree (or from a given node in a graph) can be described as follows:
			</p>

			<ol class="side">
				<li>
					While there are still nodes yet to be expored (candidates for expansion):
					<ol type="a">
						<li>
							Expand a node according to your searching strategy
						</li>
						<li>
							Is it a goal? If so, return success. Else, carry on.
						</li>
					</ol>
				</li>
				<li>
					Return failure
				</li>
			</ol>

			<p>
				The unexpanded nodes that we can immediately expand to are called the <b>frontier</b>, and are stored in a <b>queue</b> structure -- our search strategy determines how this queue is ordered.
			</p>

			<h4>Breadth First Search</h4>

			<figure >
				<img src="https://upload.wikimedia.org/wikipedia/commons/5/5d/Breadth-First-Search-Algorithm.gif" alt="" style="max-width: 300px;" align="right">
				<figcaption>Process of BFS (Wikimedia Commons) -></figcaption>
			</figure>

			<p>
				In short: expand the <b>shallowest</b> node first. 
			</p>
			<p>
				Frontier queue is ordered by distance from the origin/root. Successor nodes are added to the <b>end</b> of the queue.
			</p>

			<p>
				For a branching factor \(b\) and the <i>depth of least cost solution</i> \(d\) the <b>time</b> complexity is \(O(b^d)\)... i.e. \(O(n)\) in number of nodes. The <b>space</b> complexity is the <b>same</b> - which can be a big problem if there are lots of nodes.
			</p>
			<p>
				BFS will always find a solution if \(b\) is finite (complete).
			</p>

			<h4>Depth First Search</h4>

			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/7/7f/Depth-First-Search.gif" alt="" style="max-width: 300px;" align="right">
				<figcaption>Process of DFS (Wikimedia Commons) -></figcaption>
			</figure>

			<p>
				In short: expand the <b>deepest</b> node first.
			</p>
			<p>
				Frontier queue is a stack, put successors at the start. 
			</p>

			

			<p>
				DFS has a time complexity of \(O(b^m)\), where \(m\) is the <i>maximum</i> depth, rather than that of the nearest solution. This can be bad if \(m\) is large. However, DFS's main advantage is its <b>space</b> complexity -- \(O(bm)\) -- only one path needs to be stored, making DFS better where memory is tighter.
			</p>

			<p>
				DFS is <b>incomplete</b> -- if \(d\) is infinite (or graph has loops) then DFS may never terminate. 
			</p>

			<h4>Lowest Cost First Search</h4>

			<p>
				As the name implies, select a node on the <b>path with the lowest cost</b> first. 
			</p>
			<p>
				The path cost is the sum of all the arcs from the origin to the newly expanded node: \(cost(\langle n_0 \dots n_k \rangle) = \sum_{i=1}^k cost(\langle n_{i-1}, n_i \rangle \).
			</p>
			<p>
				Frontier is priority queue ordered by cost. The first path to a goal found is the least cost goal. Note this reduces to breadth first when all arcs are of equal cost.
			</p>

			<h3 id="sea-2">Informed Search</h3>

			<p>
				Informed search uses "problem specific knowledge", such as the location of the goal, an estimate of distance, etc., to help inform its search choices. They are usually much better than brute force uninformed search.
			</p>

			<h4>Best First Search</h4>

			<p>
				Best first search uses a <b>heuristic</b> - some estimate of the final distance for each path to determine the choice of exploration. Heuristics come up <b>a lot</b>, since AI is all about "good enough". There are two variants:
			</p>

			<p class="side">
				<b>Heuristic DFS</b> picks the <i>node</i> with the best possible heuristic estimate.
			</p>
			<p class="side">
				<b>Greedy Best First Search</b> picks the <i>path</i> with the best possible heuristic.
			</p>
			<p>
				And what is the heurisitc? Well, it depends on the situation, but say you're in a maze, and the nodes are intersections / turns. Perhaps the heuristic is the euclidean (i.e. straight diagonal distance) between that corner and the goal square. 
			</p>
			<p>
				However, you might quickly notice that for a maze, the closest "as the crow flies" might be a massive dead end. This is the problem with BFS, where the heuristic <i>may</i> just lead to the wrong path, and if the algorithm is not programmed well enough, forever looping.
			</p>
			<p class="blue">
				The crucial thing is that <b>heuristics are underestimates</b>.
			</p>

			<p>
				There seems to not be much difference between the two apart from naming, so just default to "Greedy BFS I guess".
			</p>

			<p>
				Greedy BFS has a time and space complexity of \(b^n\) for a <b>branching factor</b> \(b\) and a <b>path length</b> \(n\). It may not ever find a solution, and thus is <b>incomplete</b>.
			</p>

			<h4>A* Search</h4>

			<p>
				<b>The very important significant one</b>. A* takes into account both path cost and remaining heuristic when it does its searching. 
			</p>
			<p class="blue">
				Let \(g(p)\) or \(cost(p)\) be the cost of the current path \(p\), and the heuristic from the end of p to the goal as \(h(p)\).
				<br><br>
				Let \(f(p) = h(p) + g(p)\), the total estimate of a path's cost from start to finish, so to say. This is our final <b>evaluation function</b>
			</p>
			<p>
				A* orders the frontier by \(f(p)\), and picks like that. In this way, it is a mix of Lowest-cost first and Best-first, and is actually pretty damn good.
			</p>

			<p class="blue">
				An algorithm is <b>ADMISSIBLE</b> if a solution existing \(\implies\) the <b>optimal solution</b> is found.
			</p>
			<p>
				A* is an <b>admissible algorithm</b>.
			</p>

			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/9/98/AstarExampleEn.gif" alt="" style="max-width: 50%;">
				<figcaption>A* Search example (wikipedia)</figcaption>
			</figure>

			<p>
				A good heuristic is better, an overestimate can be really bad but too far of an underestimate is also bad - A* explores every path with an estimate less than the optimal cost, so if there are a lot of paths here, A* can still take a while.
			</p>
			<p>
				A* thus has a time complexity relative to \(\textrm{error of }h(p) \cdot \textrm{length of solution}\) - which is pretty good, the only problem is that A* is <b>exponential in space</b>, because it needs all nodes in memory. 
			</p>

			<h3>Cycle Checking and Path Pruning</h3>

			<p>
				This is how you stop algorithms which may not halt from not halting.
			</p>
			<p>
				<b>Pruning</b> a path means removing it from consideration entirely, which saves memory holding unnecessary paths.
			</p>
			<p>
				<b>Cycle Checking:</b> If your explored path reaches an already explored node in memory, for example if you went <code>a -> b -> c -> d -> b</code>, you can prune the <code>d -> c -> b</code> bit without losing an optimal path solution, since it forms a closed cycle. 
			</p>
			<p>
				<b>Path pruning:</b> If you have a path in memory that goes like <code>s -> b -> x -> y -> z -> u -> h -> m -> p</code>, and later on in searching you find a different path <code>s -> i -> m -> p</code> through a new node <code>i</code>, then you can prune away the first path, since you can use the second one to get to the same destination.
			</p>
			<p>
				<b>Note</b> though how I didn't say <i>longer</i> and <i>shorter</i> - that's because it's not, it's recency. <b>This is why you have underestimate heuristics</b> - otherwise you won't necessarily get the most optimal solution.
			</p>
			<p class="blue">
				A <b>Monotone Heuristic</b> is where the heuristic is an underestimate for all arcs across a graph. These heuristics are also called <b>consistent</b>, and will never overestimate. 
			</p>
			<p>
				A* Search with a consistent heuristic is needed to find an optimal path.
			</p>

			<h3 id="sea-4">More Searching</h3>

			<p>
				Searching backwards from the goal is effectively the same as searching forward from the start. 
			</p>
			<p>
				Of course if the <i>backwards</i> branching factor is much different from the <i>forwards</i> one, the efficiency of one direction vs another may be vastly different. Sometimes, even, one direction is not available, if the graph is being constructed dynamically.
			</p>
			<p>
				Provided not, we can get <b>bidirectional search</b>: search from both start and end simultaneously. \(2b^{\frac{k}{2}} < b^k\) after all (b branch. factor, k depth of goal). Of course, the frontiers must somehow meet, so one side is usually a BFS.
			</p>
			<p>
				Extend to <b>island-driven search</b>, where we pick \(M\) "interesting locations/checkpoints" and search simultanously from all of those. \(Mb^{\frac{k}{M}}\) is faster still -- the problem is choosing said "interesting locations".
			</p>

			<h4>Iterative Deepening and Depth First Branch and Bound</h4>

			<p>
				We like DFS because low memory, but DFS might follow its own tail into the abyss, so how do we prevent that? We can limit how deep DFS will go - making <b>bounded DFS</b>.
			</p>
			<p>
				But what if the bound is too shallow? Then, we gradually increase the bound, and rerun DFS, until we get to the goal. This is now <b>Iterative Deepening</b> of the bound. 
			</p>
			<p>
				But this is still a dumb algorithm. What if we add h e u r i s t i c s to find an optimal solution? Suppose we already have a path to the goal. Let's set a bound \(p\) as its cost. If we have a path where the cost + heuristic \(&gt; p\), then clearly, it will never be shorter, and thus is immediately pruned. 
			</p>
			<p>
				Basically, given a known goal at depth \(p\), no nodes with depth \(&gt; p\) will even be encountered. Set the bound to <b>less than</b> \(p\) and keep searching at a decreasing depth until we can't find goal nodes any more.
			</p>
			<p>
				Rinse and repeat until we have no more shorter paths than our \(p\). That is our optimal solution, and this is <b>Depth First Branch and Bound</b>.
			</p>

			<p>
				(For a known state space with unknown goals, then just put the bound at the max depth, to save researching anything)
			</p>

			<h3 id="sea-5">Finding Heuristics</h3>

			<p>
				Heuristics are underestimates, but the closer the underestimate the better. A heuristic of 0 is no better than a dumb search. Finding heuristics however is difficult, good ones even more so, but there are a few approaches:
			</p>
			<p>
				<b>Relax the problem</b>: try a less restrictive version of the problem. If it's a maze, imagine there's no walls and you can fly. If it's a 15-tile game, imagine you can move tiles through others. 
			</p>
			<p>
				<b>Combine heuristics</b>: If you have several different admissible heuristics, combine them and use the best one out of them for each individual state as your final value.
			</p>
			<p>
				<b>Statistics</b>: Actually run simulations to try get data estimates. This is however <b>NOT ADMISSIBLE</b>, but can be good enough.
			</p>

			<h3 id="sea-6">Adversarial Search</h3>

			<p>
				Often we have games where multiple agents compete. In a competitive game, agents must deal with <b>contingencies</b>
			</p>
			<p>
				Time is of the essence: there is often not enough time to think through everything properly.
			</p>
			<p>
				Thus we have uncertainty. Whether from opponent moves, insufficient time, or even the random chance nature of a game (such as dice). This makes them more interesting. 
			</p>
			<p>
				Formally, we view a game as 
				<ul>
					<li><b>An Initial State:</b> board position, and player to move</li>
					<li><b>Operators</b> set, for legal moves</li>
					<li><b>Terminal Test</b> for game conclusion</li>
					<li><b>Payoff function</b> to numerically evaluate terminal states. For noughs and crosses, we could have +1 win, 0 draw, -1 lose.</li>
				</ul>
			</p>

			<h3 id="sea-7">Minimax</h3>

			<p>
				Consider a game of noughts and crosses, between two players, Min and Max. If Max wins, Max gains a point. If Min wins, Max loses a point. In this way, this is a <b>zero sum game</b>.
			</p>
			
			<p>
				Max must form a strategy that will try to maximise its points, whilst Min does the opposite. 
			</p>
			<p>
				Since there is no random chance, and the search space is small, we can afford to make <b>perfect decisions</b>. This considers the <i>entire</i> state space. 
			</p>
			<p>
				Create a search tree simulating Min minimising, Max maximising, repeat until you reach terminal states. The terminal values propagate up by:
				<ul>
					<li>If the node is a <i>min</i> node, take the minimum of the children</li>
					<li>If the node is a <i>max</i> node, take the maximum of the children</li>
				</ul>
				Repeat until you reach the root node of the current state. Then, (as Max) just make (one of) the move(s) that yields the highest score. 
			</p>
			<p>
				Hence, <b><i>Minimax</i></b>.
			</p>
			<p>
				For <b>two</b> players, a minimax search tree is a <b>two-ply game tree</b>, since two "ply" (layers) make one full round. 
			</p>
			
			<div style="display: flex; flex-direction: row;">
				<figure>
					<img src="./twoply-gametree.png" alt="" style="max-width: 300px;">
					<figcaption>2-ply game tree</figcaption>
				</figure>
				<figure>
					<img src="./minimaxing.png" alt="" style="max-width: 300px;">
					<figcaption>Minimax process</figcaption>
				</figure>
			</div>

			<h4>\(\alpha\beta\)-pruning</h4>

			<p>
				\(\alpha \beta\)-pruning is based on <b>minimax</b>, and gets its name from the utilities on the min-max tree. 
				<ul>
					<li>
						\(\alpha\) is the best choice along the path for <b>Max</b>
					</li><li>
						\(\beta\) is the best choice along the path for <b>Min</b>
					</li>
				</ul>
			</p>

			<p>
				The pruning part comes from when we search through the search tree. In short, if we discover a node which makes a path <b>objectively worse</b> than any other already found, we don't bother to search more on that path, and prune it out entirely - terminate the recursive call.
			</p>
			<p>
				The efficiency of this depends on how we search -- using good heuristics may help \(\alpha\beta\)-pruning to become very efficient. 
			</p>

			<h4>Imperfect Decisions</h4>

			<p>
				The biggest problem we have left is that sometimes, we're not afforded the time to search the whole tree, which \(\alpha\beta\) still has to do. Thus we need some sort of way to accomodate this, usually including 
				<ul>
					<li>A <b>heuristic evaluation function</b> to get values for non-terminals</li>
					<li>A <b>cutoff test</b> to determine when to stop.</li>
				</ul>
			</p>
			<p>
				The evaluation function gives an <b>estimate</b> of expected utility -- usually by calculating "features", which are game dependent.
			</p>
			<p>
				The simplest cutoff apprach is fixed depth, a slightly better method would be to <b>iteratively deepen</b> until time runs out. 
			</p>
			<p>
				Alternatively, we can base everything off <b>quiescent</b> states -- those which are <b>not likely to change in value</b>. Non-quiescent states are expanded until quiescent ones are found, which are resolved with an evaluation function. This extra search is called the <b>quiescent search</b>.
			</p>
			<p>
				Quiescent Search's main problem is the <b>horizon problem</b> -- faced with unavoidable damage, it thinks stalling is advancing -- we can mitigate this by instead picking the least worst non-stalling move.
			</p>

			<h3 id="sea-8">Games with Chance</h3>

			<p>
				Many games, like Backgammon, contain chance. We must alter our tree to include <b>chance nodes</b>, and take the <b>expected value</b> of all possible moves for each possibility. 
			</p>
			<p>
				Generally represented as 
				\[
						\textrm{expMinMax}(n) = \begin{cases} 
							\textrm{utility}(n) & n \textrm{ is terminal node} \\
							\max_{s \in \textrm{successor}(n)}(\textrm{expMinMax}(s)) & n \textrm{ is node for Max} \\
							\min_{s \in \textrm{successor}(n)}(\textrm{expMinMax}(s)) & n \textrm{ is node for Min} \\
							\sum_{s \in \textrm{successor}(n)}(P(s) \cdot \textrm{expMinMax}(s)) & n \textrm{ is chance node}
						\end{cases}
						\]
			</p>

			<h4>Monte Carlo</h4>
			
			<p>
				Monte Carlo tree search is an improvement on \(\alpha \beta\)-pruning which addesses some of the latter's limitations, namely

				<ul>
					<li>High branching factors reducing depth of search</li>
					<li>difficulty defining an evaluation function</li>
				</ul>

				Monte Carlo addresses these via numerical methods: 
			</p>
			<p>
				<b>Estimate the value of a state from the average utility of many simulated games (playouts) from the start state.</b> 
			</p>
			<p>
				By playing games against itself Monte Carlo can learn which states often lead to which outcomes, rather than the programmer having to come up with some function.
			</p>
			<p>
				The <i>way</i> moves are made during playouts is called <b>playout policy</b>, and can vary depending on games. 
			</p>

			<hr>

			<p>
				<b>Pure MTCS</b> performs N simulations from the current state, track which moves from current position has highest win %. 
			</p>
			<p>
				As N increases, this converges to optimal play. 
			</p>
			<p>
				A pure MTCS is usually infeasible, given the large amount of processing power needed to run. Thus, a <b>selection policy</b> is applied to focus simulation on important parts of the game tree.
			</p>
			<p>
				To balance <b>exploration</b> (simulating from states with few playouts) and <b>exploitation</b> (simulating from known good states to increase accuracy of estimate).s
			</p>
			<p>
				steps:
			</p>
			<ul>
				<li><b>Selection</b>: starting at the root, choosing a move via policy, and repeat moving to a leaf</li>
				<li><b>Expansion</b>: grow tree by generating successor</li>
				<li><b>Simulation</b>: Performing a playout from the newly generated successor node (determine the outcome, do not record moves)</li>
				<li><b>Backpropagation</b>: Use the result to update values going back up to the root.</li>
			</ul>
			<p>
				Repeat for either a fixed no. of iterations, or until we are out of time, and return the move with the <b>highest number</b> of playouts. 
			</p>

			<hr>

			<p>
				<b>Upper confidence</b> is a selection policy based off the <b>upper confidence bound</b> formula UCB1:

				\[
				UCB1(n) = \frac{U(n)}{N(n)} + C \sqrt{\frac{\log (N \cdot \textrm{parent}(n))}{N(n}}
				\]

				\(U(n)\) is the <b>total utility</b> of all playouts with \(n\) as the starting node (from n), \(N(n)\) as the <b>number</b> of playouts from n, parent is self explanatory, and \(C\) is a set constant, which is used to balance exploration with exploitation.
			</p>
			<p>
				The computation time of a playout is linear in depth of tree.
			</p>

		</div>

		<div class="colourband">
			<h2 id="csp">Constraint Satisfaction</h2>
		</div>

		<div class="cbox">
			<h3>Variables and Constraints</h3>

			<p class="blue">
				Given a set of variables, an <b>assignment</b> is a function from variables to their domain. A <b>total assigment</b> is a function that assigns all variables in the set, and partial is thus self-explanatory.
			</p>

			<p>
				Constraints limit what combination of variables are possible.
			</p>
			<p class="blue">
				A <b>constraint</b> \(C\) is a relation \(R: S \longleftarrow \{T, F\} \) over a scope \(S\), which is a set of variables. 
			</p>
			<p>
				Any assignment \(A\) on the superset \(2^S\) <b>satisfies</b> \(C\) if all variables in S then map to true. 
			</p>
			<p>
				A constraint can be <b>unary</b> (1 var), <b>binary</b> (2 vars), or more. 
			</p>

			<h3>CSPs</h3>

			<p class="blue">
				A <b>Constraint Satisfaction Problem</b> has a set of variables, a domain for every variable, and a set of constraints. <span class="grey">We will be working on finite CSPs only.</span>
			</p>

			<p>
				Many things are reducible to CSPs, notably <i>3-SAT</i>, which is covered in CS260. CSPs are usually NP-hard, thus difficult to solve efficiently, and thus, like the rest of AI, our algorithms are heuristic based and "good-enough" based.
			</p>

			<h4>Generate and Test</h4>

			<p>
				The most naive method of solving CSPs. As the name suggests, generate a total assignment (randomly or systematically), and test if it violates any constraints.
			</p>
			<p>
				Whilst this is stupid easy, it has a complexity of \(O(\textrm{horrible})\).
			</p>

			<h4>Backtracking</h4>
			
			<p>
				So generate and test is horrible, maybe we do something smarter. CSPs can be reduced to trees, where nodes are <b>assignments of values</b> and neighbours are gotten by selecting a different node, and assigning it to something else. 
			</p>
			<p>
				As soon as we hit an invalid (partial) assignment, backtrack one and try a different one, like the example below.
			</p>

			<button class="collapsible active">Backtracking... </button>
			<div class="ccontent" style="display: block;">
				<p>
					<b><i>Example.</i></b> Suppose we have a CSP with variables \(A, B, C\), each has domain \(\{1..4\}\), and our contraints are \(A &lt; B; B &lt; C\). A backtracking tree might look like the following:
				</p>

				<figure>
					<img src="./csp-backtrack.png" alt="" style="max-width: 70%;">
					<b>Full backtracking tree. Depending on the search algorithm the partially searched tree might look different.</b>
				</figure>
				
			</div>

			<p>
				Backtracking is \(O(\textrm{less horrible})\), and is usually good enough for most applications <i class="grey">*cough* something beginnning with c</i>.
			</p>

			<button class="collapsible nul">Algorithm... </button>
			<div class="ccontent nul">
				<p></p>
				<div class="codediv">backtrackCSP(CSP):
	return backtrack_r(\(\{\}\), CSP)

backtrack_r(assignment, CSP):
	if assignment complete: 
		return assignment
	\(X \longrightarrow\) unassigned variable in CSP.vars
	for each \(v \in\) CSP.domain(\(X\)):
		if \(v\) consistent with assignment given CSP.constraints:
			assignment.add(\(X = v\))
			result \(\longrightarrow\) backtrack_r(assignment, CSP)
			if result \(\neq\) fail: 
				return result
			assignment.remove(\(X = v\))
	return fail</div>
			</div>

			<p>
				If dumb backtracking is <i>not</i> good enough, we can use a heuristic-based search:
			</p>
			<ul>
				<li>
					<b>MRV | Minimum Remaining Value</b>: pick the variable with the fewest possible values; "fail first"
				</li>
				<li>
					<b>Degree</b>: (backup to MRV) pick the variable with most constraints
				</li>
				<li>
					<b>LCV | Least Constraining Value</b>: for a variable, pick the value that rules out the least other values
				</li>
			</ul>

			<h3>Consistency Algorithms</h3>

			<p>
				Sometimes, some assignments will always fail nomatter what. For example, if \(A, B = \{1..4\}\) and \(A < B\), the assignment \(A=4\) is always invalid. Thus to speed up backtracking we can strip these domain values out as we go along.
			</p>

			<p>
				These algorithms work on a <b>constraint network</b>, where nodes are variables (annotated by domain) and arcs are constrains connecting variables. 
			</p>
			<p class="small">
				If you have more than just binary constraints you can have constrains as their own <b>square</b> nodes and connect <b>round</b> variable nodes instead.
			</p>

			<h4>Arc Consistency</h4>

			<p>
				(Generic) <b>Arc Consistency</b> (GAC) is the one algorithm, and the goal is to make everything "arc consistent":
			</p>

			<p class="blue">
				A variable is <b>domain consistent</b> if all unary constraints (loops) are satisfied. A variable is <b>arc consistent</b> if the constraints on all arcs between it and other variables are satisfied as well. 
			</p>

			<ul class="side">
				<li>
					GAC keeps track of a <b>to-do list</b> of unconsidered arcs, which are initialised to all arcs in the graph.
				</li>
				<li>
					Every iteration until the todo list is empty, we pick and remove an arc from a variable X to a constraint \(c\) and make it consistent. If, however, this changes the domain of X, do the next step:
				</li>
				<li>
					For all other constraints \(c'\), we need to make sure that every other variable apart from X (\(Z \neq X\)) are satisfied again -- i.e. adding them back to the to-do list (if not there already).
				</li>
			</ul>

			<button class="collapsible nul">Algorithm... </button>
			<div class="ccontent nul">
				<p></p>
				<div class="codediv">generic_arc_consistency(vars \(V\), domains \(dom\), constraints \(C\)):
	return gac2(\(V\), \(dom\), \(\{\langle X, c \rangle : c \in C, x \in scope(C)\} \))
				
gac2(vars \(V\), domains \(dom\), constraints \(C\), paths \(toDo\)):
	while \(toDo \neq \varnothing\):
		remove and select \(\langle X, c \rangle\) from \(toDo\)
		\(\{Y_1, \dots, Y_k\} \longrightarrow scope(C) \setminus \{X\}\)
		\(ND \longleftarrow \begin{align} \{X &: 
			x \in dom[x] \land \exists y_1, \dots, y_k \in dom[Y_1], \dots, dom[Y_k] 
			\\ &: c(X=x, Y_1=y_1, \dots, Y_k=y_k) \textrm{ true}\} \end{align}\)
		if \(ND \neq dom[x]\):
			\(toDo \longleftarrow toDo \cup \{\langle Z, c' \rangle 
				: \{X, Z\} \subseteq scope(c'), c' \neq c, Z \neq X\}\)
			\(dom[x] \longleftarrow ND\)
		return \(dom\)</div>
			</div>

			<p>
				<b>Regardless</b> of arc selection order, the algorithm will terminate with the same arc-consistent graph and set of domains. Three cases will happen:
				<ol>
					<li>A domain becomes \(\varnothing \implies\) no solution to CSP</li>
					<li>Each domain has 1 value \(\implies\) unique solution</li>
					<li>All or some domains have multiple values \(\implies\) we do not know the solution and will have to use another method to find it</li>
				</ol>
			</p>

			<p>
				The <b>time complexity</b> of such an algorithm is \(O(cd^3)\) for \(c\) constraints and a \(d\) domain size ~ linear in \(c\), which is efficient enough.
			</p>
			<p>
				The <b>space complexity</b> is \(O(nd)\) for \(n\) total variables.
			</p>
			
			<h4>Domain Splitting</h4>

			<p>
				Domain splitting splits a problem into two separate (disjoint) cases, solving them separately, and combining them.
			</p>
			<p>
				Take a variable \(X \in \{T, F\} \), we can split this into two "worlds" where \(X=T\) and \(X=F\), solve those worlds, and combine all the valid results. With a domain of \(\{1, 2, 3, 4\} \) however, we can split into 4 worlds, or just 2 where \(X =\{1,2\}; X=\{3, 4\} \). If we only need one solution, we can just stop as soon as one world finds valid-- heeyyy wait, isn't this just "backtracking"?
			</p>
			<p>
				Well, yes. Or rather, backtracking is a special case of domain splitting.
			</p>
			<p>
				Interleave arc consistency here between every split/expand for best results.
			</p>

			<button class="collapsible">Algorithm... </button>
			<div class="ccontent">
				<p></p>
				<div class="codediv">cspSolver(vars \(V\), domains \(dom\), constr's \(C\)):
	return csp2(\(V, dom, C, \{\langle X, c \rangle : c \in C \land x \in scope(c)\}\))

csp2(\(V, dom, C\), paths \(toDo\)):
	\(dom_0 \longleftarrow\) gac2(\(V, dom, C, toDo\))  # gac alg from above
	if \(\exists X \in V : X = \varnothing\):
		return false
	else if \(\forall X \in V,\; |dom_0[x]| = 1\):
		return solution where \(X\) set to value in \(dom_0[x]\)
	else:
		select \(X \in V : |dom_0[x]| > 1\)
		partition \(dom_0[x]\) into \(D_1, D_2\)
		\(dom_1 \longleftarrow\) copy(\(dom_0\)); \(dom_1[x] \longleftarrow D_1\)
		\(dom_2 \longleftarrow\) copy(\(dom_0\)); \(dom_2[x] \longleftarrow D_2\)
		\(toDo \longleftarrow \{(Z, c') : \{X, Z\} \subseteq scope(c') \land Z \neq X\}\)
		return csp2(\(V, dom_1, C, toDo\)) 
			or if false return csp2(\(V, dom_2, C, toDo\))</div>
			</div>

			<h4>Variable Elimination: Trees</h4>

			<p>
				A regular CSP takes \(O(d^n)\) to solve (\(d\) domain size \(n\) variables), but if we split this into \(\frac{n}{c}\) subproblems of \(c\) variables, suddenly this becomes drastically quicker (\(O(\frac{n}{c}d^c)\))
			</p>

			<p>
				Specifically, an <b>acyclic</b> CSP can be solved in \(O(nd^2)\) by: 
			</p>
			
			<ol class="side">
				<img src="./csp-tree-to-line.png" alt="A CSP tree being a line" style="max-width: 50%;" align="right">
				<li>Choose a variable as root, and order from root to leaves such that all nodes' parents <b>preceeds</b> them in ordering
					
				</li>
				<li>for \(j = 1 .. n\), make node\(x_j\) consistent with its parents</li>
			</ol>

			<p>
				If we don't have a tree, but can make a tree by removing one or two nodes, we can instantiate those variables, prove their consistency, and then remove them from consideration.
			</p>
			
			<p class="blue">
				<img src="./csp-conditioning.png" alt="A graph where one node removed makes a tree" style="max-width: 40%; display: block;" >
				<b>Conditioning</b> is removing one variable by the above method. <br>
				<b>Cutset Conditioning</b> is conditioning done to a set of variables (nodes).
			</p>

			<h4>Variable Elimination and Cutset Conditioning</h4>

			<p>
				More generally, we can eliminate a variable by passing its effects off onto the constraints of its neighbours. 
			</p>
			<p>
				The idea is to collect all constrains on a variable X and <b>natural join</b> them to make one \(, c_x(X, \bar{Y})\) (Y-bar are neighbours to X), then <b>project</b> only those \(\bar{Y} \) variables. <span class="grey">(CS258 Relational Algebra)</span>
			</p>
			<button class="collapsible">Algorithm... </button>
			<div class="ccontent">
				<p></p>
				<div class="codediv">ve_csp(vars \(V\), constr's \(C\)):
	if \(|V| = 1\):
		return \(\bowtie\)join(\(\forall c \in C\))
	else:
		select variable \(X \in V\) to eliminate
		\(V' \longleftarrow V \setminus \{X\}\)
		\(C_X = \{t \in C : t \textrm{ involves } X\}\)
		\(R_1 \longleftarrow \bowtie\)join(\(\forall t \in C_X\))
		\(R' \longleftarrow R\) projected onto everything except \(X\)
		\(S \longleftarrow\) ve_csp(\(V', (C \setminus C_X) \cup \{R'\}\))</div>
			</div>

			<p>
				Efficiency depends on selection, and variable elimination only works when the graph is <b>sparse</b>.
			</p>

			<p class="blue">
				The <b>treewidth</b> is the number of vars in the <b>largest</b> relation. The <b>graph treewidth</b> is the minimum treewidth of any ordering. VE is exponential in treewidth.
			</p>

			<p>
				Cutset conditioning is the process of doing this with <b>cutsets</b> (sets of variables) 
			</p>
			<p>
				The efficiency of cutset elimination is O(number of cutsets &times; time per tree), that is,
			</p>
			<p class="blue">
				Given domain size \(d\), number of nodes \(n\), and a cutset size \(c\), the efficiency of cutset conditioning is \(O(d^c (n-c) d^2 )\).
			</p>

		</div>

		<div class="colourband">
			<h2 id="probability">Conditional Probability and Bayes</h2>
		</div>

		<div class="cbox">
			<ul>
				<li><a href="#pro-1">Probability and Bayes</a></li>
				<li><a href="#pro-2">Decision Making</a></li>
			</ul>

			<h2 id="pro-1">Probability</h2>

			<p>
				First of all, see <a href="../cs130#probability">Probability</a> from CS130 since this needs probability spaces \(\Omega\) annd expected values of random vars \(E[X] = \sum_{s_i \in \Omega} P(s_i) X[s_i] \).
			</p>
			<p>
				Recall that for conditional probability: \(P(A|B) = \frac{P(A \land B)}{P(B)}\). If \(P(A|B) = P(A)\) and \(P(B|A) = P(B)\) then they're independent.
			</p>

			<p class="blue">
				<span class="nopad">
				From which we get <b>Bayes' Theorem</b> \(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\) (provided neither \(P(A), P(B) = 0\)). <br>
				Where \(P(B) = P(B|A)P(A) + P(B|\lnot A)P(\lnot A)\) -- the possibility of B given all occurences of A.
				</span>
			</p>
		
			<p>
				If A is instead split into <i>mutually exclusive, exhaustive</i> states \(a_1 \cdots a_n\) rather than a true and false, then we can extend the equation for \(P(B)\) to be a sum \(\sum_{i=1}^{n} P(B|a_i) P(a_i)\)
			</p>
			<p class="blue">
				Note also the chain rule:
				\[P(a_1 \land a_2 \land \dots \land a_i) = P(a_1) \times P(a_2|a_1) \times P(a_3|a_1 \land a_2) \times \cdots \times P(a_i|a_1 \land a_2 \land \cdots \land a_{i-1} )\]
			</p>
			<p class="blue">
					The <b>joint probability distribution</b> \(P(A,B)\) is a probability dist over A, which has \(n\) states, and B, which has \(m\) states. Thus, for every A state \(a_i\), we have \(m\) entries in the prob. dist for every possible B state (i.e. it's \(A \land B\)). 
			</p>

			<p>
				To work out \(p(a_i)\) from \(p(A, B)\) do \(p(a_i) = \sum_{j=1}^{m} p(a_i, b_j)\). This is referred to as <b>marginalising</b> B out of \(P(A,B)\)
			</p>

			<p>
				Naturally this is difficult for continuous probabilities. In that case, we have to use integration (that I doubt will <i>actually</i> come up).
			</p>

			<p class="blue">
				Note that <b>conditional independence</b> for some \(X, Y\) given \(Z\) means that \(P(X, Y|Z) = P(X|Z) \cdot P(Y|Z)\).
			</p>

			<h3>Inference</h3>

			<p>
				<b>Probabilistic inference</b> is about computing <i>values</i> for queried <i>propositions</i> given observed <i>evidence</i>.
			</p>

			<ul class="side">
				<li>
					Let \(X\) be our query variable, and \(E\) be our evidence variables with observed values \(e\).
				</li>
				<li>
					Let \(Y\) be other, unobserved variables, with some values \(y\)
				</li>
				<li class="nopad">
					<!-- <span class="nopad"> -->
					We want \(p(X|e)\):
					\[p(X|e) =\frac{p(X, e)}{p(e)} = \alpha \sum_{y}p(X, e, y)\]
					Where the \(\frac{1}{p(e)}\) multiplier is called the <b>normalisation constant</b> (sometimes written \(\alpha\)), \(p(e)_= \sum_{x, y} p(x, e, y)\).
					<!-- </span> -->
				</li>
			</ul>
			<p>
				Now, this is probably meaningless when just presented in theory, so below is an example.	
			</p>

			<button class="collapsible active">Simple Example... </button>
			<div class="ccontent" style="display: block;"><span class="nopad">
				<p>
					<b><i>Example.</i></b> Given the following table, we want to calculate P(cavity|toothache).
				</p>
				<table>
					<tr>
						<th></th>
						<th colspan="2">Toothache</th>
						<th colspan="2">¬Toothache</th>
					</tr>
					<tr>
						<th></th>
						<th>catch</th>
						<th>¬catch</th>
						<th>catch</th>
						<th>¬catch</th>
					</tr>
					<tr>
						<th>cavity</th>
						<td>0.108</td>
						<td>0.012</td>
						<td>0.072</td>
						<td>0.008</td>
					</tr>
					<tr>
						<th>¬cavity</th>
						<td>0.016</td>
						<td>0.064</td>
						<td>0.144</td>
						<td>0.576</td>
					</tr>
				</table>

				<p>
					Note we have an "unobserved" (irrelevant variable) catch. Then, to work everything out:
				</p>
				<ul>
					<li>p(cavity \(\land\) toothache) = 0.108 + 0.012 = 0.12 (marginalising catch)</li>
					<li>p(cavity \(\land\) ¬toothache) = 0.008 + 0.072 = 0.08</li>
					<li>Then p(cavity) = 0.108 + 0.012 + 0.016 + 0.064 = 0.2</li>
					<li>p(toothache) = 0.108 + 0.012 + 0.016 + 0.064 = 0.2</li>
					<li>Thus p(cavity|toothache) = \(\frac{0.12}{0.2}\) = 0.6.</li>
				</ul>
			</span>
			</div>

			<p>
				For more complex examples, this type of enumeration may become incredibly difficult - thus we need some sort of <i>graphical method</i> to represent and take advantage of conditional independence.
			</p>
			<p>
				<img src="./infer-tree.png" alt="" align="right" style="size: 100px;">
				Since often events lead to other events which lead further in a chain, we can represent this graphically. 
			</p>
			<p>
				We want an acyclic graph, where each node corresponds to one variable in our probability distribution. 
			</p>
			<p>
				This graph should then satisfy the <b>markov condition</b>:
			</p>
			<p class="blue">
				<b>Markov Condition:</b> For all variables \(V\), \(V\) is <b>conditionally independent</b> of all its <b>non</b>descendants, given its parents: \(P(B,C|H) = P(B|H) \cdot P(C|H)\) because C does not descend from B.
			</p>
			<p>
				This reduces vastly the number of conditional probabilities we need to work out, and simplifies the ones we do -- only the conditional probabilities \(P(X|PA_X)\) (given the parents of X) need to be calculated.
			</p>
			<p>
				This type of graph is a <b>Bayesian Belief Network</b> -- directed, acyclic, showing influence, and in total representing a full joint prob. dist.
			</p>

			<p>
				When building a BBN, order of nodes matters (different order = different complexity) -- we really only want a node to depend on only those before it. Then, we can use chain rule principles and only the parents function \(PA_X\) to define the joint probability:
				\[P(X_1, X_2, \dots, X_n) = \prod_{i=1}^n P(X_i | PA_{X_i})\]
			</p>
			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/SimpleBayesNet.svg/400px-SimpleBayesNet.svg.png" alt="">
				<FIGCaption>Bayesian Belief Network example (Wikiepdia)</FIGCaption>
			</figure>

			<h3>Noisy OR</h3>

			<p>
				Even still, local probability distributions tend to grow very quickly (\(O(2^n)\)) -- simplify using "canonical interaction" models like Noisy OR.
			</p>
			<div class="blue">
				<p>Noisy OR:</p>
				<ul>
					<li>Describes set of \(n\) clauses \((x_1 .. x_n\) and their common effect (descendant) \(y\)</li>
					<li><b>Assume</b> each \(x_i\) is sufficient to cause \(y\) regardless of state of other clauses: <b>independence</b>.</li>
					<li>Thus \(p_i\) (prob. of causing \(y\) rel. to \(x_i\)) \(= P(y|\lnot x_1, \dots, x_i, \lnot x_{i+1}, \dots, \lnot x_n)\)</li>
				</ul>
			</div>

			<p>
				<img src="./infer-noisy-or.png" alt="" align="right">

				Shown on the right is a BBN concerning <b>F</b>atigue, <b>B</b>ronchitis, <b>L</b>ung cancer, and <b>O</b>ther causes of fatigue. 
			</p>

			<p>
				<b>Causal inhibition</b> is where each of the three causes F, O, L, has an inhibitor (not shown): Bronchitis will cause fatigue <b>if and only if</b> the inhibior mechanism for that cause is absent.
			</p>

			<p>
				<b>Exception Independence</b> is where the inhibiting mechanism for each cause is independent.
			</p>

			<p>
				And of course, the effect F can only happen if at least one cause is present.
			</p>

			<p>
				A noisy-OR model adds new <b>deterministic nodes</b>, whose values are exactly specified by parents. We can extend the above BBN into a Noisy-OR tree, add in the inhibitors, and <i>voila</i>.
			</p>

			<figure>
				<img src="./noisy-or-tree.png" alt="" title="Noisy OR tree" style="max-width: 70%;">
			</figure>

			<p>
				Course a BBN won't actually show this, and condenses it down into a single probability / probability table. 
			</p>
			<p>
				When constructing a network, only draw causal links from the new node to already existing ones - depending on the order new nodes are added, this wildly changes the number of arcs and thus the complexity.
			</p>

			<h3>Reasoning</h3>

			<p>
				Different types of reasoning can be done on a BBN:
			</p>
			<ol>
				<li><b>Diagnostic Reasoning</b> from symptop to cause: backwards along the arcs. From our evidence reason <i>up</i> until we work out our query -- often involves Bayes' rule.</li>
				<li><b>Causal Reasoning</b> from cause to symptom: forwards along the arcs -- often involves <i>just reading from the graph</i>.</li>
				<li><b>Intercausal Reasoning</b>: If a cause and an immediate symptom are observed together, this concerns all other causes of a symptom</li>
				<li><b>Combined Reasoning</b> if we have to go backwards and forwards simultaneously</li>
			</ol>

			<p>
				A lot of this entials enumeration of possibilities.
			</p>

			<button class="collapsible">Reasoning Over 3 Nodes... </button>
			<div class="ccontent" style="display: none;">
				<p><b><i>Example.</i></b>; note that \(\alpha\) is the \(1/p\) normalisation constant.</p>
				<img src="./reasoning-3-node.png" alt="">
			</div>

			<h4>Variable Elimination</h4>

			<p>
				VE is more efficient than enumeration. VE works over factors, which:
			</p>
			<p class="blue">
				A <b>factor</b> is a <b>function</b> on a set of variables, called the <b>scope of the factor</b>.
				<br><br>
				Conditional Probability \(P(x|y,z)\) can be described as a factor with scope \(x,y,z\).
			</p>
			<p>
				If we can (1) order variables, (2) index them, then we can uniquely represent each factor as a <b>1d array</b>. Ex. the table 
				\begin{array} {|r|r|}\hline x & y & f_0 = P(z=T | x, y) \\ \hline T & T & 0.1 \\ \hline T & F & 0.2 \\ \hline F & T & 0.3 \\ \hline F & F & 0.4 \\ \hline  \end{array}
				Can be represented as the list \(f_0 = [0.1, 0.2, 0.3, 0.4] \). We can then <b>condition, sum,</b> and <b>multiply</b> these.
			</p>
			<p>
				<b>Conditioning:</b> If we have observed a var with a value, we can define a new factor with reduced scope: Starting with \(P(x|y,z\) then observe \(z=T\), define new factor \(P(x|y,z=t)\) with a scope \((x, y)\), since \(z\) is now known.
			</p>
			<p>
				<b>Multiplying:</b> If factor 1 has scope \(A, B\) and factor 2 has scope \(B, C\) then we can multiply these together, which is doing a <code>natural join</code> multiply. If \(f_1: A=T, B=F \implies 0.9\) and \(f_2: B=F, C=T \implies 0.6\) then \(f_{new}: A=T, B=F, C=T \implies 0.6 \times 0.9 = 0.54\).
			</p>
			<p>
				<b>Summing:</b> Eliminating a chosen variable by adding together possible outcomes. Given \(f(X,Y,Z)\) (factor over X, Y, Z) eliminate Y by doing \(f_{new}(X, Z) = f(X, Y=T, Z) + f(X, Y=F, Z)\).
			</p>
			<p>
				Thus, the VE algorithm goes as
			</p>
			<ol class="side">
				<li>Construct factor for each conditional probability</li>
				<li>Eliminate each non-query variable:
					<ol type="a">
						<li>If variable has observed value, set it to that and condition</li>
						<li>Otherwise sum it out</li>
					</ol>
				</li>
				<li>Multiply remaining factors and normalise</li>
			</ol>

			<h2 id="pro-2">Decision Making</h2>

			<p>
				BBNs model probabilities, and often these probabilities are used to make decisions. Decision making can also be formalised.
			</p>
			<p>
				First though, given a set of outcomes \(O=\{O_i ..\} \) of an action \(A\), a <b>utility function</b> \(U(O_i|A)\) assigns a utility to each outcome -- desirability. 
			</p>
			<p>
				The expected utility is a weighted sum of outcomes' probabilities and their utils,
				\[\sum_i P(O_i|A) \times U(O_i|A)\]
				And we assume that a decision maker always wants to <b>maximise utility</b>.
			</p>

			<p>
				The decision maker is an agent, and it must know how good outcomes are <i>relative</i> to each other. These relations should be complete and transitive.
			</p>

			<div class="blue">
				<p>Two outcomes \(\sigma_1, \sigma_2\) are</p>
				<ul>
					<li><b>Weakly preferred</b> \(\sigma_1 \succeq \sigma_2\), 1 is at least as good as 2</li>
					<li><b>Indifferent</b> \(\sigma_1 \sim \sigma_2\), 1 and 2 are the same</li>
					<li><b>Strictly preferred</b> \(\sigma_1 \succ \sigma_2\), 1 is better than 2, we do not weakly prefer 2</li>
				</ul>
			</div>

			<h3>Decision Trees</h3>

			<p>
				Decision trees have 2 types of nodes. <b>Chance Nodes</b> are circles, and are random variables. <b>Decision Nodes</b> are squares, and represent a set of mutually exclusive choices.
			</p>

			<button class="collapsible active">Buying stock...</button>
			<div class="ccontent" style="display: block;">
				<p>
					<img src="./decision-tree.png" alt="" align="right" style="max-width: 30%;">
					<b><i>Example.</i></b> Suppose you have £1000 to spend. You can put it in the bank, with a guaranteed 0.5% return in a month, or buy 100 shares of OCRCompnay @ $10 a share. The tree looks as follows:
				</p>
				<p>
					\(EU(X) = 0.25 \times 500 + 0.25 \times 1000 + 0.5 \times 2000 = £1375\).
				</p>
				<p>
					\(EU(D) = \max(EU(X), 1005) = £1375\)
				</p>
				<p>
					Thus buy stonks.
				</p>
				
			</div>
			<p>
				Naturally real people have different views on risk vs reward than an agent which goes off dumb numbers and a max function. We can also model this however. 
			</p>
			<p>
				We can change our utility function to instead of being just the straight number, account for the risk involved as well:
				\[U_R(x) = e^{\frac{-x}{R}}\]
				Thus with a higher R value for certain options over others, we can represent risk accordingly.
			</p>

			<p>
				If we have non-numeric outcomes in our decision tree, such as whether or not a suit gets ruined, we can still model this by <b>assigning</b> utility values, often 0 for the least desiresd, 1 for the most desired, and something in between for the others that reflects their undesirability vs their chance to mitigate disaster.
			</p>

			<h3>Influence Diagrams</h3>

			<p>
				Decision trees can grow exponentially with the problem, and influence diagrams help prevent that. 
			</p>
			<p>
				Influence diagrams have three types of nodes: <b>chance</b> (circle), <b>decision</b> (square), <b>utility</b> (diamond).
			</p>
			<p>
				An edge <b>to</b> a chance node means its value is <b>probabilistically dependent</b> on value of parent.
			</p>
			<p>
				An edge <b>to</b> a decision node means value of parent is <b>known</b> when decision made. If parent is decision, then this is chain of decisions. If parent is chance, means the chance has resolved before we decide. 	
			</p>

			<button class="collapsible active">Buying Stock (II)...</button>
			<div class="ccontent" style="display: block;">
				<img src="./influence-stonks.png" alt="" title="Influence tree for stocks example" align="right" style="max-width: 50%;">
				<p>
					<b><i>Example.</i></b> We recreate the buying stock example as an influence diagram. We want to work out the same thing, \(\max(EU(d_1), EU(d_2))\).
				</p>

				<p>
					\(EU(d_1) = E(U|d_1)= \) <br>
					\(P(X=£5) \times U(d_1, £5) + \)
					\(P(X=£10) \times U(d_1, £10) + \)
					\(P(X=£20) \times U(d_1, £20) = £1375.\)
				</p>

				<p>
					\(EU(d_2) = E(U|d_2) = £1005.\)
				</p>
			</div>

			<p>
				Links from chance -> decision are also known as <b>information links</b> : chance must be known before decision can be made.
			</p>
			<p>
				For a diagram with these links, 
				<ul>
					<li>Add any observed evidence by changing that chance node to \(p=1\) for that outcome,</li>
					<li>For every possible parent occurence, calculate the expected utility for that specific setting,</li>
					<li>Return the final stored table of decision values.</li>
				</ul>
				I.e. if a decision node depends on a chance node with 4 outcomes, calculate an EU for each outcome, and record that final table.
			</p>
		</div>

		<div class="colourband">
			<h2 id="kbs">Knowledge Bases and Rule-Based Systems</h2>
		</div>

		<div class="cbox">

			<h3>Introduction</h3>

			<ol>
				<li><a href="#kbs-1">Knowledge Bases</a></li>
				<li><a href="#kbs-2">Rules as Knowledge</a></li>
				<li><a href="#kbs-3">Debugging and Conflict Resolution</a></li>
				<li><a href="#kbs-4">Assumption Reasoning</a></li>
			</ol>

			<h3 id="kbs-1">Knowledge Bases</h3>

			<p>
				Knowledge bases are stores of <b>facts</b>, in propositional logic. Usually they're stored as <i>horn clauses</i> (of form \(a_1 \land a_2 \land \cdots \land a_n \implies x\)). Knowledge bases are usually denoted \(KB\).
			</p>
			<p>
				<b>Reasoning</b> is the bridge between the facts, and what can be deduced to be true from the facts. After all, it is difficult to store <i>every</i> true thing. If a proposition \(\alpha\) can be deduced from the KB, we write \(KB \models \alpha\) (KB entails alpha).
			</p>
			<p>
				That being said, KBs can be represented by different languages, and the expressiveness of the language can determine what we can represent.	
			</p>
			<p>
				The <i>knowledge representation hypothesis</i> is that any mechanical, intelligent agent will have a <b>collection of true propositions</b>, and be able to do <b>formal reasoning</b> on them to deduce new facts. 
			</p>
			<p>
				As mentioned, boolean algebra (and propositional logic) is often used, because it is easy to work with.
			</p>
			<p>
				Note that the variables themselves have no intrinsic meaning, the user must decide the <b>intended implementation</b>, and thus each atomic proposition would represent one thing the user wants to describe about the world. 
			</p>

			<h3 id="kbs-2">Expert Systems</h3>

			<p>
				The <i>Expert System</i> was one of the older in-fashion AI models. It's basically a program with a KB that <b>represents</b> knowledge, and can <b>reason</b> (logically) with it. 
			</p>
			<p>
				An expert, a human who has knowledge in the subject, is responsible for entering the base data that the system has to go one (hence, expert system). The user then interacts with the "other side" of the system to input data and query facts. 
			</p>
			<button class="collapsible active">MYCIN... </button>
			<div class="ccontent" style="display: block;">
				<p>
					<b>MYCIN</b> was an old expert system used (well, intended to be used) in medicine. Its knowledge base was made up of rules, of the form <blockquote>if conditions \(c_1, c_2, \dots\), then, with probability (tally) \(t\), draw conclusions \(x_1, x_2, \dots\)</blockquote>
				</p>
				<p>
					Data is stored in a tree, and then the system will search through the tree. Each node is a "goal", a condition to be met, and all / some subgoals must be met for the parent node to be. 
				</p>
				<p>
					This type of tree is an <b>and-or tree</b>. In an AND/OR tree, if a nodes' children are connected with an arc between their lines to the parent, it is an AND. Otherwise, it is an OR. 
				</p>
			</div>

			<h3 id="kbs-3">Rules as Knowledge</h3>

			<p>
				Rational people act consistently, thus consistent behaviour is seen as intelligent behaviour. For an AI system we can describe these in the form of <b>production rules</b> (i.e. from this state you can do these things).
			</p>
			<p>
				A <b>canonical</b> system is a formal grammar system based on an alphabet, some "axiom" strings (variables), and a set of production rules - in all effect, a <i>context sensitive language</i> (see 255).
			</p>

			<button class="collapsible">Example language... </button>
			<div class="ccontent">
				<ul>
					<li>The alphabet \(A = \{a,b,c\} \)</li>
					<li>The axioms \($ \in \{a, b, c, aa, bb, cc\} \)</li>
					<li>
						The production rules
						\begin{align}
							$ &\longrightarrow a$a \\
							$ &\longrightarrow b$b \\
							$ &\longrightarrow c$c 
						\end{align}
					</li>
				</ul>
				<p>
					Where, as seen, $ belongs to one of the starting axioms.
				</p>
			</div>
			<p>
				In a KB, we can represent knowledge through our "alphabet", which conists of three sets of words: O of objects, A of attribtes, and V of values of attributes.
			</p>
			<p>
				Our grammar here is based on <b>object-value-attribute triples</b>, where we can have
				<ul>
					<li>(object attribute value) or</li>
					<li>(object (attribute value) (attribute value) ... )</li>
				</ul>
			</p>
			<p>
				Our <b>working memory</b> is the knowledge base. It stores strings of facts (axioms), which define the initial state. It also stores production rules, to allow transitions from one state to another.
			</p>
			<p>
				Each fact is a <b>working memory element</b>, and can be interpreted as an existential first-order logic: (student (name John) (department computerScience)) means the same as \(\exists x : student(x) \land (name(x) = \textrm{John}) \land (department(x) = \textrm{computerScience})\)
			</p>

			<p>
				Production rules are then effectively <b>if-then</b> statements. 
			</p>
			<p class="blue">
				The <b>CLIPS</b> syntax is one of these rule-representation syntaxes -- <i>and does appear on the exam</i>
			</p>
			<button class="collapsible active">CLIPS example... </button>
			<div class="ccontent" style="display: block;">
				<p>
					<b><i>Example.</i></b> Assume the working memory has the rules: <span class="grey">(organism means the patient is infected with that organism)</span>
				</p>
				<div class="codediv">(patient (name Jones) (organism organism-1))
(organism (name organism-1) (morphology rod) (aerobicity aerobic))</div>

				<p>
					And has the rule 
				</p>
				<div class="codediv">(defrule diagnosis
	(patient (name ?) (organism ?org))
	(organism (name ?org) (morphology rod) (aerobicity aerobic))
	=>
	(assert 
		(organism (name ?org) (identity enterobacteriacae) (confidence 0.8))
	)
)</div>
			</div>

			<p>
				The rule interpreter applies the rules, and it works in a <b>recognise-act cycle</b>. I.e, match the front, and then apply the rule. If multiple things match, pick one according to some strategy (e.g. at random, or through h&nbsp;e&nbsp;u&nbsp;r&nbsp;i&nbsp;s&nbsp;t&nbsp;i&nbsp;c&nbsp;s)
			</p>
			<p>
				When no actions are left, then the recognise-act cycle stops.
			</p>
			<p>
				Controlling inference behaviour is deciding what rules to fire if we have multiple. Control can be <b>global</b> -- hard coded into the engine and domain independent, or <b>local</b> -- domain dependent and encoded as "meta rules", rules about how to fire rules.
			</p>
			<p>
				We fire several rules in order to reach a conclusion, this is an <b>inference chain</b>, like steps of a proof. You can think of all possible inference chains as a tree. 
			</p>
			<p>
				Building an inference chain can be done in two ways: <b>forward chaining</b> and <b>backwards chaining</b>.
			</p>
			<p>
				<b>Forward chaining</b> starts from data, and tries to fire rules until you reach a conclusion. Irrelevant facts will also be inferred. <b>Backwards chaining</b> goes from a goal and tries to reverse the rules to reach known facts. 
			</p>

			<button class="collapsible active">Forward Chaining Example... </button>
			<div class="ccontent" style="display: block;">
				<p>
					<b><i>Example.</i></b> Forward chaining is the <b>"bottom-up ground proof procedure"</b>. It is logically sound, deterministic, but may unnecessarily fire rules. Suppose we have the facts \(A, B, C, D, E\) and the rules 
					\begin{align}
						Y \land D &\longrightarrow Z \\
						X \land B \land E &\longrightarrow Y \\
						A &\longrightarrow X \\
						C &\longrightarrow L \\
						L \land M &\longrightarrow N
					\end{align}
					and wanted to get Z. (note that in the slides the \(\land\) is a & symbol)

				</p>
				<p>
					We're iterating through the list repeatedly, firing all the rules we can on every pass.
				</p>
				<ul>
					<li>
						Fire \(A \longrightarrow X\), known facts now \(A, B, C, D, E, X\)
					</li>
					<li>
						Fire \(C \longrightarrow L\), now have \(A, B, C, D, E, L, X\)
					</li>
					<li>
						Fire \(X \land B \land E \longrightarrow Y\), now have \(A, B, C, D, E, L, X, Y\)
					</li>
					<li>
						Fire \(Y \land D \longrightarrow Z \), now have \(A, B, C, D, E, L, X, Y, Z\)
					</li>
				</ul>
				<p>
					Note that inferring L was <b>wasteful</b>.
				</p>
			</div>
			<p></p>
			<button class="collapsible active">Backwards Chaining Example... </button>
			<div class="ccontent" style="display: block;">
				<p>
					<b><i>Example.</i></b> Backwards chaining is the <b>"top-down definite clause proof procedure"</b>. We start with a goal, and <i>replace</i> it with the LHS of a rule, until our clause entirely consists of KB facts. This is <b>nondeterministic</b>
				</p>
				<p>
					Suppose we have the same KB and ruleset as the previous example
				</p>
				<ul>
					<li>
						Start with \(Z\)
					</li>
					<li>
						Replace with \(Y \land D\) via rule. D is ok, but Y?
					</li>
					<li>
						Replace with \(X \land B \land E \land D\) via rule replacing Y. Now X?
					</li>
					<li>
						Replace with \(A \land B \land E \land D\) via rule replacing X. 
					</li>
					<li>
						Thus we fire the chain
						\begin{align}
							A &\longrightarrow X \\
							X \land B \land E &\longrightarrow Y \\
							Y \land D &\longrightarrow Z.
						\end{align}
					</li>
				</ul>

				
			</div>

			<p>
				In some cases, we want to ask the user at certain points. We can then introduce <b>askable clauses</b>. The idea is it is tedius for the user to input all possible knowledge, and also they don't know what will be relevant, and so askable clauses provide targeted asking at the right time. 
			</p>

			<h3 id="kbs-4">Debugging and Conflict Resolution</h3>

			<p>
				Sometimes, things go wrong (logically). There are 4 possibilities of wrong logical answers: (1) <b>incorrect</b>, when a clause that should be false is true, (2) <b>not produced</b>, a true clause could not be derived, (3) <b>infinite loop</b>, and (4) <b>irrelevant questions asked</b>, which requires reassesment of the KB.
			</p>
			<p>
				If the answer is incorrect, then there must be some fact that is wrong, and some rule in the KB that wrongly proved it. We can re-ask the user/expert about all the variables, but if that gets us nowhere, then a rule is wrong. 
			</p>
			<p>
				If the answer could not be derived, then either there's no rule for it, or the rule exists but isn't fired (another variable is wrong). Can solve this recursively by finding all variables that should be true, but are currently underivable. 
			</p>
			<p>
				If the rules are <b>cyclical</b>, a KB can loop. We can convert the KB into a directed graph, and check for cycles, and redo the KB so it does't exist. Forward chaining however <i>cannot</i> loop, since it doesn't fire a rule that doesn't add a new fact. 
			</p>
			
			<h4>Conflict Resolution</h4>

			<p>
				One rule firing can affect other rules, since it changes the KB. 
			</p>
			<p>
				<b>Conflict Resolution</b> is choosing which rule to fire when multiple possible can be. 
			</p>
			<p class="blue">
				The set of fireable rules is the <b>conflict set</b>. This is called the <b>agenda</b> in CLIPS.
			</p>

			<p>
				A basic (or fallback) approach would be to fire rules in order of appearance. However, then the order of rules has an impact, and there is <i>implicit</i> knowledge that higher rules are more important. 
			</p>
			<p>
				Generally, conflict resolution is based on <b>specificity</b>. The more conditions, the harder to satisfy, thus, match the most specific one.
			</p>
			<p>
				We can also use <b>recency</b>, the more recent the data, the higher its priority (useful for, say, weather). 
			</p>
			<p>
				We can also enforce <b>refactoriness</b>, a rather confusing way of saying each rule can only be used once.
			</p>

			<p>
				Conflict resolution can also employ <b>meta knowledge</b>, which are rules about the use of domain knowledge in a system. More often than not dependent on the domain at hand, and determine  Represented as <b>meta rules</b>. 
			</p>

			<button class="collapsible">Meta Rules Example... </button>
			<div class="ccontent">
				<p>
					<b><i>Example.</i></b> A domain independent meta rule can be 
				</p>
				<div class="codediv">IF there are rules which do not mention the current goal in premise
	AND there are rules which do mention the current goal in premise
THEN with probability (1.0) the former should be fired before the latter</div>
				<p>
					A domain dependent meta rule can be
				</p>
				<div class="codediv">IF the infection is a pelvic abcess
	AND there are rules which mention in premise enterobacteriacae
	AND there are rules which mention in premise gram-positive rods
THEN with probability (0.4) the former should be fired before the latter</div>
				<p>
					Look... Nathan does medical stuff, ok? Don't ask.
				</p>
			</div>

			<h4>RETE</h4>

			<p>
				We want to increase the efficiency of rule matching. Especially as every recognise-act cycle, little is changed, and many rules share conditions. 
			</p>
			<p>
				The idea is to use the <b>Rete</b> (latin for net) algorithm.
			</p>
			<img src="https://fico.my.salesforce.com/servlet/servlet.ImageServer?id=01580000002jqhuAAA&oid=00D0000000052aJ" alt="" title="(Sanjana Marce)" align="right" style="max-width: 240px;">
			<p>
				Every rule is represented as a directed acyclic graph, based on the characteristics of the  antecedent (left hand side), which must be met for the RHS to be fired. 
			</p>
			<p>
				There are two main types of nodes in the body, \(\alpha, \beta\). 
				<ul>
					<li>
						\(\alpha\) nodes are self-contained, that is, they have only one input, and can be thought of as a unary test. 
					</li>
					<li>
						\(\beta\) nodes are two input, they join together two conditions into one single constraint.
					</li>

				</ul>
			</p>
			<p>
				New or changed working memory elements (tokens) are passed through the tree, taking the path that matches best. If the token can make it from the root to the terminal rule node, then that rule can be fired. 
			</p>
			<p>
				Anyway, slides are obtuse, I got this from <a href="https://community.fico.com/s/blog-post/a5Q8000000082VcEAI/fico1097">this article here</a>, have a read if you want.
			</p>

			<hr>

			<p>
				Rule based systems are 
			</p>
			<ul>
				<li>(+) good because they make for an easy mapping between an expert's knowledge and rule format</li>
				<li>(+) they represent rules as individual bits of knowledge</li>
				<li>(-) however this makes relationships between rules can be unclear</li>
				<li>(-) and it is harder to manage with more rules.</li>
				<li>(+) they separate out processing from knowledge</li>
				<li>(+) and are able to represent and reason with uncertainty</li>
				<li>(-) but each cycle requires a full search through the rules, which can take a while</li>
				<li>(-) and the system cannot learn by itself, and is also easily broken.</li>
			</ul>

			<hr>

			<h3 id="kbs-5">Assumption Reasoning</h3>

			<p>
				Assumption based reasoning is where an agent makes some <b>assumptions</b> instead of deducing knowledge, such as
			</p>
			<p>
				<b>Abduction</b>, where an agent makes a hypothesis to explain observations, and 
			</p>
			<p>
				<b>Default reasoning</b>, where "normality" is assumed -- where something is assumed to exist as it normally would (no edge cases) to facilitate processing. 
			</p>
			<p>
				An <b>assumption based framework</b> has <i>two</i> sets of formulae, <b>facts</b> F and <b>possible hypotheses</b>/assumables H. Facts can also include integrity constraints, which musn't evaluate to true.
			</p>
			<p>
				Using this framework, 
				<ul>
					<li>
						Under <b>default reasoning</b> when the truth of a clause \(g\) is unknown, an explanation for \(g\) will be an argument / observation of \(g\)
					</li>
					<li>
						Compared to <b>abduction</b>, where given a \(g\), and we want to explain why it is. 
					</li>
				</ul>
				Given an observation we do abduction, then default reasoning is used to find the consequences.
			</p>
			<p>
				Repeat the assumption process until all our variables are comprised of assumables. However at the same time, we mustn't violate integrity constraints.
			</p>
		</div>

		<div class="colourband">
			<h2 id="pop">Partial Order Planning</h2>
		</div>
		
		<div class="cbox">

			<h3>Contents</h3>

			<ol>
				<li><a href="#pop-1">Reasoning with knowledge</a></li>
				<li><a href="#pop-2">Planning</a></li>
				<li><a href="#pop-3">Situation Calculus and STRIPS</a></li>
				<li><a href="#pop-4">Partial Order Planning</a></li>
				<li><a href="#pop-5">Heirarchical Decomposition</a></li>
				<li><a href="#pop-6">Conditional Planning and Replanning</a></li>
			</ol>

			<h3 id="pop-1">Reasoning with Knowledge</h3>
			<p>
				Partial Order planning, or <i>planning with certainty</i>, also uses knowledge bases. Agents who use knowledge bases are <b>knowledge based agents</b>.
			</p>

			<p>
				KB Agents can be characterised at three levels based on abstraction:
				<ul>
					<li>
						<b>Knowledge Level:</b> what is known, regardless of implementation. Most abstract level. We are given <code>ask()</code> to retrieve data and <code>tell()</code> to input data.
					</li>
					<li>
						<b>Logical level:</b> where the knowledge is encoded in formal sentences, like propositional logic.
					</li>
					<li>
						<b>Implementation level:</b> the actual data structures and whatever. 
					</li>
				</ul>
			</p>
			<p>
				A simple KB agent can be represented by this pseudocode:
			</p>
			<div class="codediv">static KB  # a knowledge base 
t = 0  # time counter

kbAgent(percept):
	tell(KB, makePerceptSentence(percept, t))
	action &lt;- ask(KB, makeActionQuery(t))
	tell(KB, makeActionSentence(action, t))
	t &lt;- t + 1
	return action</div>
			
			<p>
				A <b>percept</b> is an observation of the world state. The KB may contain initial background knowledge.
			</p>
			<p>
				Three functions here abstract away implementation. <code>makePercentSentence</code> encodes the observed percept into a formal sentence. <code>makeActionQuery</code> asks the knowledge base on what action to do at this time with this state. <code>makeActionSentence</code> then tells the query what action was done.
			</p>

			<p>
				In certain planning, we're interested in worlds where everything is certain - that is, there is no random chance. When there is no random chance, we can reason with absolute certainty.
			</p>

			<div style="display: block;">
				<p>
					Below is an example of a game where everything is certain.
				</p>
	
				<button class="collapsible">Wumpus World... </button>
				<div class="ccontent">
					<p>
						<b>Wumpus World</b> is a simple AI game, with an explorer exploring for treasure.
					</p>
					<p>
						The world is made up of a grid, in which can either be gold, a pit, or Wumpus. The player is an adventurer, who enters and exits from the bottom left square. The player has one arrow, which it can fire in the direction its facing. However, the player can only detect information about the square it is on. 
					</p>
					<p>
						The player can <b>perceive</b>: a breeze, when adjacent to a pit; a smell, when adjacent to the wumpus; glitter, when directly on top of the gold; a scream, if the player kills the wumpus; and a bump, when it walks into a wall. 
					</p>
					<p>
						The player can <b>do</b>: walk into an adjacent square; fire an arrow in a direction <i>(once only)</i>; grab or release the gold; and climb out of the room.
					</p>
					<p>
						The <b>goal</b> is to get the gold, and escape, without falling into a pit (and dying) or running into the wumpus (and dying)
					</p>

				</div>
			</div>

			<p>
				However, in some situations, even after all our logical predictions we cannot be sure of an optimal move. There are cases of <i>no good moves</i>. Imagine in minesweeper where you have two possible mine places, and one mine remaining, and there's no indication from the numbers which one it is. 
			</p>
			<p> 
				In this case, we can use a few things. <b>Background knowledge:</b> where we take what we know built into the agent, such as the distribution of mines, to make an educated guess. Or <b>Coersion:</b> where we force a move to reduce the world into a more certain state. In minesweeper, this risks loss as you make a random move. In other games, like <i>wumpus</i>, you can shoot and check if you killed anything. If not, that direction is safe. Thus, you sacrifice an arrow to coerce the world into where you have safe states. 
			</p>

			<h3 id="pop-2">Planning</h3>

			<p>
				We can encode all of these states and conditions formally in propositional logic, but, as found out in CS262 Logic, this blows up very quickly to lots and lots of sentences, thus searching through such possible state spaces is incredibly inefficient.
			</p>
			<p>
				Thus, we use <b>planning</b>. Planning is a different way of, well, planning actions, which does not require searching through a full state space. 
			</p>
			<p>
				Suppose we have an easier problem: We are at home with nothing. We can do all sorts of things, but our goal is to buy milk, bananas, and a drill. 
			</p>
			<p>
				Now, whilst this is easier to us, a search algorithm will be extremely lost. WHat's the first action? We can sit idly staring at a wall, take the dog out for a walk, go wander round the garden, make a sandwich, etc, etc ... since the branching factor may be thousands or more, and path length fairly long, exhaustive searching is infeasible. It may even be hard to use heuristics. 
			</p>
			<p>
				A <b>planning system</b> has a set of sentences for the <b>states</b>, and the <b>goals</b>. It also has <b>actions</b>, which have preconditions, and have effects. 
				<ul>
					<li>
						In search, states are stored in data structures, both the action and goal are hard-coded, and the plan is a sequence from \(s_0\) the start state.
					</li>
					<li>
						In planning, states are logic sentences, actions are just "boxes" with preconditions and outcomes, the goal is also a logic sentence, a <b>conjunction</b>, and the plan is just constraints on actions.
					</li>
				</ul>
			</p>
			<p>
				The planner connects states and actions. We can even use divide and conquer, where we have several subplans that have little interaction between them -- such as (a) get to the shop and (b) buy stuff in the shop, which are subplans which wouldn't affect each other. 
			</p>

			<button class="collapsible">Pseudocode Simple Planning Agent</button>
			<div class="ccontent">
				<div class="codediv">static KB
static t = 0  # time 
static p = None  # plan 

simplePlanningAgent(percept):
	G;  # a goal 
	current;  # description of current state
	tell(KB, makePerceptSentence(percept, t))
	current &lt;- stateDescription(KB, t)
	if p == noPlan:
		G &lt;- ask(KB, makeGoalQuery(t))
		p &lt;- idealPlanner(current, G, KB)
	if p == None or p is empty:
		action &lt;- noOp
	else:
		action %&lt;- first(p)
		p &lt;- rest(p)
	tell(KB, makeActionSentence(action, t))
	t &lt; t + 1 
	return action </div>
			</div>
			
			<p>
				The planning agent
				<ul>
					<li>Updates the knowledge base</li>
					<li>Create a new goal, and a plan, if there isn't already one. A NoOp must exist where the goal is infeasible or achieved</li>
					<li>Once agent has a plan, it will execute it to completion</li>
				</ul>
				There is minial interaction with the environment. 
			</p>

			<h3 id="pop-3">Situation Calculus and <span class="sc">STRIPS</span></h3> 

			<div class="blue">
				<p>
					<b>Situational Calcucus</b> is using <b>first order logic</b> to describe change. In SC, the world is viewed as <b>situations</b> - "snapshots" of state. Each situation is made from a previous via an action, and denoted with \(s\).
				</p>
			</div>
			<p>
				There are <b>fluents</b>: functions which change with time -- they have a situation in their argument: \(At(Agent, [1,1], s_0)\).
			</p>
			<p>
				And there are <b>Eternal/Atemporal</b> functions, which do not change and do not have a situation as argument: \(Wall(1,0)\).
			</p>
			<p>
				Change is represented by a <b>result function</b>: \(Result(action, situation)\).
			</p>
			<p>
				<b>Possibility axioms:</b> are expressions using function Poss(action, situation), which describes which actions are possible. \(At(Agent, a, s) \land Adjacent(a,b) \implies Poss(Go(a,b), s)\) [Go(a,b): go from a to b]
			</p>
			<p>
				<b>Effect axioms:</b> are expressions describing what an action can result in. \(Poss(Go(a,b), s) \implies At(Agent, b, Result(Go(a, b), s)\) [the result of going from a to b at state s -- varies depending on the situation]
			</p>
			<p>
				We can have logical statements about initial and goal states 
				<ul>
					<li>
						The <b>initial</b> state \(s_0\), we can have \(At(Home, s_0) \land \lnot Have(Milk, s_0) \land \lnot Have(Bananas, s_0) \dots\)
					</li>
					<li>
						The <b>final</b> condition: \(\exists s_f : At(Home, s_f) \land Have(Milk, s_f) \land Have(Bananas, s_f) \dots\)
					</li>
				</ul>
				From this, we can essentially reason logically to get from start to a state which matches the end conditions.
			</p>
			<p>
				All of the abovementioned, possibilities, effects, fluents, etc, all describe <i>change</i>. However, we also need to express when the world <i>stays the same</i> -- this is done via <b>frame axioms</b>.
			</p>
			<p>
				<b>Frame axioms</b> describe <b>non-changes</b> due to actions. i.e. walking into a wall does not change state. \(At(o,x,s) \land (o \neq Agent) \land \lnot Holding(o,s) \implies At(o, x, Result(Go(x, b), s))\). Note that as before, Result() returns a <b>state</b>, since we need the new state of the world, even if nothing else has changed.
			</p>
			<p>
				There's a problem with frames, the <a href="https://en.wikipedia.org/wiki/Frame_problem">frame problem</a>, which is... something. <b>Successor-state axioms</b> exist to solve the "Representational Frame Problem", and essentially the way it works is 
				<ul>
					<li>
						Each statement is about a predicate, and not necessarily an action. 
					</li>
					<li>
						P being true after \(\iff\) An action made P true, or P is already true and remains true. 
					</li>
					<li>
						e.g.	
					</li>
				</ul>
				\begin{align}
							Poss(a,s) \implies At(o,y,Result(a,s)) &\iff \\
							&a = Go(x,y) \land (o = Agent \lor Holding(o,s)) \lor  \\
							&(At(o,y,s) \land \exists z : y \neq z \land a = Go(y,z) \land (o=Agent \lor Holding(o,s)))
						\end{align}
				in plain english: the effect axiom that causes object to be at y, is only true if and only if (a) the agent is the object, or is holding the object, and goes to y; or (b) the object is at y, and the agent does not go to y.
			</p>

			<p>
				This gets clunky very quickly, and so most people use a restricted language -- <b><span class="sc">STRIPS</span></b>.
			</p>

			<h4>STRIPS</h4>
			<p>
				Strips has a planner, and not a generic theorem prover. It was written in Stanford to solve planning problems. 
			</p>
			<p>
				<b>States</b> are just conjunctions of free literals (which may be assigned meaning).
			</p>
			<p>
				State descriptions may be incomplete, but usually we work under <b>closed world assumption</b> -- if it's not stated to be true, it is false.
			</p>
			<p>
				The <b>goal</b> is also a state, and the <b>planner</b> is asked for a sequence of actions to get from the start to the goal state. 
			</p>
			
			<div class="blue">
				<p>
					<b>Operators</b> are the actual process, and are important. They have three parts:
				</p>
				<ol>
					<li>The <b>Action</b> / \(Buy(x)\)</li>
					<li>The <b>preconditions</b> / \(At(p), Sells(p,x)\)</li>
					<li>The <b>effect</b> / \(Have(x)\)</li>
				</ol>
				<p>
					Which are displayed like a box, with preconditions on top, effects on bottom, and actions inside.
				</p>
			</div>

			<p>
				An operator with variables is called an <b>operator schema</b> -- a "family" of related actions that need to be instantiated with an actual object.
			</p>
			<p>
				An operator is <b>applicable</b> if we can assign all variables to make the precondition match. 
			</p>
			
			<h3 id="pop-4">Partial Order Planning</h3>

			<p>
				In a standard search, each node is a state. In a <i>planning search</i> however, each node is a <b>partial plan</b> -- that is, an incomplete plan. 
			</p>
			<p>
				Operators act on partial plans by (1) <b>adding a</b> (causal) <b>link</b> from an existing action to an <i>open condition</i>, (2) <b>add a step</b> to fulfill the open condition, and (3) <b>order</b> one step w.r.t. another. 
			</p>
			<p>
				An <b>open condition</b> is any part of a precondition that is unfulfilled. 
			</p>
			<p>
				Some useful terminology are
				<ul>
					<li><b>Progression:</b> Search forward from initial state to goal (large branch factor, search space)</li>
					<li><b>Regression:</b> Search backwards from goal (lower branch factor, more complex due to conjunctions in goal)</li>
					<li><b>Partial Plan:</b> As name implies, is incomplete. Some steps not initialised.</li>
					<li><b>Partial Order:</b> Some steps are ordered w.r.t. others.</li>
					<li><b>Total Order:</b> A full list of ordered steps.</li>
				</ul>
			</p>
			<p>
				POPs work on the <b>principle of least commitment</b>, leaving choices as long as possible. When every precondition is satisfied, then the plan is referred to as <b>complete</b>.
			</p>
			<p>
				Note that for a precondition to be achieved, there must be <i>no possible intervening step</i> <i>(threats)</i> that undos it -- there are methods to ensure this. 
			</p>

			<h4>Overview</h4>
			<div class="blue">
				<p>
					A problem is defined by a <b>partial plan</b> (the <i>minimal plan</i>), consisting <i>only</i> of start, and finish.
				</p>
				<p>
					A <b>Partial-Order Planner</b> (POP) is a <b>regressive</b> (backtracking) planner, searching through the plan space. It adds causal links without breaking already existing ones.
				</p>
			</div>
			<ul>
				<li>
					The initial state is defined by the operator: <br>
					<div class="side">
						<span class="sc">Op</span>(Action: \(Start\), <br>
						Precond: --,<br>
						Effect: \(At(Home) \land Sells(HW, Drill) \land Sells(SM, Milk) \land Sells(SM, Bananas)\)) <br>
					</div>
					<i>HW: Hardware Store; SM: Supermarket</i>
				</li>
				<li>
					The final state is defined by: <br>
					<div class="side">
						<span class="sc">Op</span>(Action: \(Finish\) <br>
						Precond: \(At(Home) \land Have(Drill) \land Have(Milk) \land Have(Bananas)\) <br>
						Effect: --) 
					</div>
				</li>
				<li>
					We have the operators:
					<div class="side">
						<span class="sc">Op</span>(Action: \(Go(there)\) <br>
						Precond: \(At(here)\) <br>
						Effect: \(At(there) \land \lnot At(here)\))
					</div>
					and 
					<div class="side">
						<span class="sc">Op</span>(Action: \(Buy(x)\) <br>
						Precond: \(At(Store) \land Sells(Store, x)\) <br>
						Effect: \(Have(x)\))
					</div>
				</li>
			</ul>

			<h4>Algorithm</h4>
			<p>
				Overview:
				<ul>
					<li>Start with minimal plan</li>
					<li>Each iteration, find a step to achieve the precondition (\(c\)) of some step (\(s_{need}\))
						<ul>
							<li>By choosing an operator : it achieves \(c\)</li>
							<li>Add a causal link to the achieved precond</li>
							<li>Resolve any threats</li>
						</ul>
					</li>
					<li>If this fails entirely, backtrack</li>
				</ul>
			</p>
			
			<button class="collapsible">Pseudocode POP... </button>
			<div class="ccontent">
				<div class="codediv">POP(start, goal, operators) \(\longrightarrow\) plan:
	plan \(\longleftarrow\) makeMinimalPlan(start, goal)
	while True:
		if plan is solution:
			return plan 
		\(s_{need}, c \longleftarrow\) selectSubgoal(plan)
		chooseOperator(plan, operators, \(s_{need}, c\))
		resolveThreats(plan)

<div class="grey">-------</div>
selectSubgoal(plan) \(\longrightarrow\) (\(s_{need}, c\)):  
	pick step \(s_{need}\) from plan.steps with precondition \(c\) that is unachieved
	return \(s_{need}, c\)

chooseOperator(plan, operators, \(s_{need}, c\)):
	pick step \(s_{add}\) from operators or plan.steps with effect \(c\)
	if \(s_{add} \) does not exist:
		<i>fail</i>
	add causal link \(s_{add} \xrightarrow{c} s_{need}\) to plan.links 
	add ordering constraint \(s_{add} \prec s_{need}\) to plan.orderings 
	if \(s_{add} \) is a newly added step from operators:
		add \(s_{add} \) to plan.steps
		add \(Start \prec s_{add} \prec Finish\) to plan.orderings

resolveThreats(plan):
	for each \(s_{threat} \) that threatens a link \(s_i \xrightarrow{c} s_j\) in plan.links:
		choose
			either: demotion; Add \(s_{threat} \prec s_i\) to plan.orderings 
			or: promotion; Add \(s_j \prec s_{threat}\) to plan.orderings 
	if not consistent(plan):
		<i>fail</i></div>
			</div>

			<h4>Clobbering</h4>

			<p class="blue">
				A <b>threat</b>, or <b>clobber</b>, is a potentially intervening step that destroys a causal link. 
			</p>
			<p>
				Suppose we have Go(HWS) \(\xrightarrow{c} \) Buy(Drill). Since Go(HWS) makes you At(HWS), and Sells(HWS, Drill). The <b>clobberer</b> is Go(Home), since if that comes between the two actions, it makes you At(Home) and you no longer can buy a drill.
			</p>
			<p>
				To deal with clobbering, we need to either <b>demote</b> or <b>promote</b> the clobberer. By demote, we mean put it before, and by promote, we mean put it after. 
			</p>

			<button class="collapsible active">Blocks World... </button>
			<div class="ccontent" style="display: block;">
				<p>
					The <b>Blocks World</b> is a classic POP problem. You have some lettered blocks, on a table. 
				</p>
				<p>
					\(Clear(x)\) means that \(x\) does not have anything on it. \(On(x,y)\) means that \(x\) is on top of \(y\).
				</p>
				<figure>
					<img src="./blocksworld.png" alt="" style="max-width: 420px;">
				</figure>
			</div>

			<p>
				There's also something called the <a href="https://en.wikipedia.org/wiki/Sussman_anomaly">Sussman Anomaly</a>, which is mentioned as an aside. ඞ
			</p>

			<button class="collapsible active">Blocks World POP Example...</button>
			<div class="ccontent" style="display: block;">
				<p>
					<b><i>Example.</i></b> See image below. 
				</p>
				<figure>
					<img src="./clobbering.png" alt="">
				</figure>
				<p>
					Essentially the idea is to build the graph by finding operators whose effects satisfy the prerequisites of the ones after it without conflicts. 
				</p>
			</div>

			<h4>Problems</h4>

			<p>
				With <span class="sc">Strips</span>, we cannot express heirarchical plans, with different level of detail. 
			</p>
			<p>
				Often we also want to give the planner guidance as it's planning, which is difficult.
			</p>
			<p>
				Condition expressiveness is limited. 
			</p>
			<p>
				We don't have any quantification. We can't express more complex conditions, such as <i>Pickup(bag)</i> also picking up all items in the bag. 
			</p>
			<p>
				Operators are unconditional, thus we can't have one action do something different based on some conditions. 
			</p>

			<h3 id="pop-5">Heirarchical Decomposition</h3>
			
			<p>
				In STRIPS, Time is <b>discrete</b> and actions are <b>instant</b>. Thus, we cannot represent actions which take time. 
			</p>
			<p>
				In the real world, resources are limited and <b>actions have cost</b>, which is not representable in vanilla STRIPS. 
			</p>
			<p>
				Thus we need to extend STRIPS and POP with more fine-detailed control. Go(Shop) is not helpful if you don't know the exact route to the shop, for example. Course, representing everything at fine levels is unfeasible in computation time, 
			</p>
			<p>
				Thus, we extends STRIPS with <b>non-primitive/abstract</b> operators, which are <b>decomposed</b> into <b>primitive steps</b>. These decompositions are pre-determined and pre-stored. 
			</p>
			<p>
				A plan \(p\) <b>correctly implements</b> a nonprimitive \(o\) if it is a complete and consistent plan achiving its effects, given its preconds. 
			</p>

			<button class="collapsible">Modified POP Pseudocode... </button>
			<div class="ccontent">
				<div class="codediv">HD-Pop(plan, operators, methods) \(\longrightarrow\) plan:
	while True:
		if plan is solution:
			return plan 
		\(s_{need}, c \longleftarrow\) selectSubgoal(plan)
		chooseOperator(plan, operators, \(s_{need}, c\))
		\(s_{nonprim} \longleftarrow\) selectNonprimitive(plan)
		chooseDecomposition(plan, methods, \(s_{nonprim}\))
		resolveThreats(plan)</div>
			</div>

			<p>
				We also want to make operator descriptions more expressive. 
			</p>
			<p>
				Have <b>conditional effects</b> with the <code>when</code> clause: <span class="sc">Effect:</span> \(\dots \land Clear(y) \textrm{ when } y \neq Table\).
			</p>
			<p>
				Any step with effect \(\lnot c \textrm{ when } p\) is a threat to link \(s_i \xrightarrow{c} s_j\). We need to ensure \(p\) is false.
			</p>
			<p>
				Have <b>Negated goals</b>: to call <code>chooseOperator</code> with a ¬goal. We would need to be careful to avoid writing down all false representations, there should be something that explicitly matches a ¬goal.
			</p>
			<p>
				<b>Disjunctive Preconditions</b>, allowing <code>selectSubgoal</code> to select non-deterministically 
			</p>
			<p>
				<b>Disjunctive Effects</b>, allowing non-determinism/noncertainty in effects. Possibly addressable via coersion of state. 
			</p>
			<p>
				<b>Universally quantified preconditions and effects</b> - \(\forall x,\,Block(x) \implies \lnot On(x,b)\) vs \(Clear(b)\)
			</p>
			<p>
				This form of FOL is <i>restricted</i>, closed world assumption still followed. Initial states give all objects a type. 
			</p>
			<p>
				Have <b>Measures</b> (\(Litres(4),\; FuelLevel, \dots\)) -- numeric valued conditions. These are called <b>measure fluent</b>s. 
			</p>
			<p>
				Some measures, the planner cannot control. Others, the planner can by producing or consuming. It is generally a good idea to prioritise scarce resources, delaying choosing links where possible. We can pick the steps and do a rough check, before resolving threats, to check for failure without a finished plan. 
			</p>
			<p>
				Time in and of itself can be a resource. We can specify how long each operator takes. The main differences are
				<ul>
					<li>
						(1) parallel resources \(s_1, s_2\) cost \(\max(s_1, s_2)\) time, not \(s_1 + s_2\) as with everything else
					</li>
					<li>
						(2) time never goes backwards -- if deadline breached, might as well stop immediately. 
					</li>
				</ul>
			</p>

			<h3 id="pop-6">Conditional Planning and Replanning</h3>

			<p>
				The real world is not fully observable, static, and deterministic, as we have been assuming before. Usually we need to work with incomplete and incorrect information. 
			</p>

			<p>
				2 solutions:
				<ul>
					<li>
						<b>Conditional Planning</b>: plan to obtain information with sensing actions. Subplan for each contingency: [Check(thing), <i>if</i> A then do this]
					</li>
					<li>
						<b>Replanning/Monitoring</b>: <i>Assume</i> normal states and outcomes. Check the progress during execution, and replan on the fly. Note that unanticipated outcomes may lead to failure. 
					</li>
				</ul>
				Though generally, a combination of both is used. 
			</p>

			<h4>Conditional Planning</h4>

			<p>
				\[[\dots \textrm{if}(p, [\textrm{then }plan_1], [\textrm{else }plan_2]) \dots]\]
				During execution, check \(p\) against current KB, execute then or else accordingly. 
			</p>
			<p>
				If an open condition can be established by observation, we can add the action, and POP for <i>each possible outcome</i>, and insert a conditional step. 
				\[[CheckTyre(x)]\; KnowsIf(Intact(x))\]
			</p>
			<p>
				<b>Steps have context</b>, at exec time agent must know the current state. Sensing actions find state, but may also have side effects: putting something in water will make it wet. 
			</p>

			<figure>
				<img src="./conditional-planning.png" alt="">
				<figcaption>Conditional Planning Example with changing a car tyre</figcaption>
			</figure>

			<p>
				A sensing action may have several outcomes. We can use these actions in <b>parameterised plans</b>, which are plans where exact actions are only defined at runtime.
			</p>
			<p>
				E.g. we have <span class="sc">Goal:</span> <i>Colour(Chair, c) \(\land\) Colour(Table, c)</i>, and we start with <i>Colour(Chair, none)</i>. In our plan, we can sense the colour of the table, then use that colour to paint the chair. 
			</p>
			<p>
				Note that \(c\) is a <b>runtime variable</b>, unknown until sensed. 
			</p>
			<p>
				We can also have <b>maintenance goals</b>, to preserve certain facts: <span class="sc">Goal:</span> <i>Colour(Chair, c) \(\land\) Colour(Table, c) \(\land\) Maintain(Colour(Table, mahogany))</i>
			</p>
			<p>
				We can also branch into loops, etc, and it soon begins to look awfully like a programming language. 
			</p>

			<h4>Monitoring</h4>

			<p>
				The effect of an action might not be as predicted. 
			</p>
			<p>
				We must <b>monitor</b> the actions for failure, as that would ruin our plan. Either the entire plan fails because the action fails to deliver on some precondition, or the action itself cannot run. 
			</p>
			<p>
				Either way, we must <b>replan</b> on the fly. 
			</p>
			<p>
				Plans can fail in two main ways:
				<ul>
					<li>
						<b>Bounded indeterminacy</b>: unexpected effects can be enumerated :- use conditionaal planning
					</li>
					<li>
						<b>Unbounded indeterminacy</b>: too large to enumerate. Can plan for some contingencies, but not all. Must replan when contingency not planned for. 
					</li>
				</ul>
			</p>
			<p>
				The simplest way to replan is to do it from scratch, but this is inefficient, and may be stuck in a cycle of failue.
			</p>
			<p>
				Alternatively, we can <i>plan to get back on track</i>. If in the list \(h,i,j,k,l\) action \(i\) fails, then we want to find a way from \(i\) back to the planned list, in teh best possible place. 
			</p>
			<p>
				e.g <span class="sc">Start</span> with <i>Colour(Chair, blue), ¬Have(Red)</i> and want <i>Colour(Chair, Red)</i>
				<ul>
					<li>
						Actions go [Start, <i>Get(Red)</i>, <i>Paint(Red)</i>, Finish]
					</li>
					<li>
						Imagine Paint(Red) fails because you didn't get enough red paint. A replanner to get back on track would try run <i>Get(Red)</i> again to get more red paint.
					</li>
				</ul>
			</p>
			<p>
				In some other situations, we may be able to avoid replanning altogether, with some good enough methods such as 
				<ul>
					<li>Coersion</li>
					<li>Abstraction -- ignore unknown details</li>
					<li>Aggregation -- treat a large number of individually uncertain objects together as one predictable aggregate object</li>
				</ul>
			</p>
			<p>
				We can also use <b>Reactive Planning</b>, which abandons domain-independent planning entirely. The agent's knowledge is <i>procedural</i> -- it has a library of partial plans representing behaviours, from which it picks. 
			</p>
		</div>

		<div class="colourband">
			<h2 id="reinforcement">Reinforcement Learning</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#rei-1">Actions have Rewards (and consequences)</a></li>
				<li><a href="#rei-2">What Values do Policies Have Really?</a></li>
				<li><a href="#rei-3">Q Learning</a></li>
			</ol>
			
			<p>
				Imagine reinforcement learning is like training a pet, except that pet is an AI. You reward it for desired behaviour and punish it for undesirable behaviour. And instead of treats, it receives negative and positive numbers.
			</p>

			<h3 id="rei-1">Actions have Rewards (and consequences)</h3>

			<h4>Markov Decision Process</h4>

			<p>
				Reinforcement learning can be described as a type of <b>Markov Decision Process</b> (MDP). 
			</p>
			<p>
				Markov Decision Processes are based off <b>Markov Chains</b>, which are themselves Bayesian <b>belief networks</b> with variables in a sequence 
				\[s_0 \longrightarrow s_1 \longrightarrow s_2 \longrightarrow s_3 \longrightarrow \cdots\]
				Where every variable only relies on the one before it. I.e. \(P(s_{i+1} | s_0 \cdots s_i) = P(s_{i+1} | s_i)\).
			</p>
			<p>
				We augment markov chains to show the actions, and resulting rewards from one state to another.
			</p>
			<div class="blue">
				<p>
					A <b>MDP</b> consists of 
					<ul>
						<li>\(S\) the set of states</li>
						<li>\(A\) the set of actions</li>
						<li>\(P : S \times S \times A \longrightarrow [0,1] \) is a probability function called the <b>dynamics</b>. 
							<ul><li>Written \(P(s' | s, a)\), it describes the probability of transitioning to state \(s'\) given it performs action \(a\) in state \(s\).</li></ul>
						</li>
						<li>
							\(R : S \times A \times S \longrightarrow \mathfrak{R}\) is the <b>reward function</b>. 
							<ul>
								<li>
									\(R(s,a,s')\) is the reward for doing action \(a\) in state \(s\) and ending up in state \(s'\).
								</li>
								<li>
									\(R(s,a)\) is the overall expected reward of doing action \(a\) in state \(s\),
									<span class="nopad">\[R(s,a) = \sum_{\forall s'} R(s,a,s') \cdot P(s' | s, a).\]</span>
									
								</li>
							</ul>
						</li>
					</ul>
				</p>
			</div>

			<figure>
				<img src="https://artint.info/2e/html/x438.png" alt="" title="(Poole; Mackworth. Artifical Intelligence 2e)" style="max-width: 360px;">
				<figcaption>This is what a MDP chain may look like.</figcaption>
			</figure>

			<p>
				If the dynamics prob. dist. is the same across time, it is called <b>stationary</b>.
			</p>

			<p>
				In a <b>fully observable MDP</b>, the agent gets to see the entire current state. 
			</p>
			<p>
				In a <b>partially observable MDP</b>, the agent knows its history of rewards and previous actions, and can indirectly / partially observe the current state.
			</p>

			<h4>Valuation</h4>

			<p>
				A <i>state</i> leads to an <i>action</i>, which yields a <i>reward</i>. 
			</p>
			<p>
				An agent can <b>explore</b> to gain knowledge, or <b>exploit</b> existing knowledge. 
			</p>
			<p>
				For ease, we initially assume that only the current state is relevant to future states. 
			</p>
			
			<p>
				Agents can carry out actions forever or for some limit. During this, it gets rewards and punishments as it goes along. 
			</p>
			<p>
				Time is also an important factor. £1000 now is a lot better than £1000 in a year's time, since there's no benefit to waiting a year. The agent will also receive rewards \(r_1, r_2, \dots\) over time. 
			</p>
			<p>
				We want a way to quantify the rewards received and how valuable they are. In this, we use a value measure. And to represent rewards further down the line being possibly less valuable, we introduce a <b>discount factor</b>, \(\gamma \in [0,1]\), which represents how impatient we are. 
			</p>
			<p class="blue">
				The <b>discounted return value</b> is a measure of how good a series of rewards is. For a time \(t\):
				\begin{align}
					V &= r_1 + \gamma r_2 + \gamma^2 r_3 + \cdots \\
					&= r_1 + \gamma(r_2 + \gamma(\cdots)) \\
					\textrm{i.e. } V_t &= r_t + \gamma V_{t+1}.
				\end{align}
			</p>
			<p>
				Because \(\sum_0^n \gamma^n = \frac{1}{1-\gamma}\) the reward at time \(t\) must be 
				\[\frac{\textrm{min reward}}{1-\gamma} \leq V_t \leq \frac{\textrm{max reward}}{1-\gamma}\]
				Thus even if you go infinitely far, you only get a finite reward.
			</p>

			

			<h3 id="rei-2">What Values do Policies Have Really?</h3>

			<p class="blue">
				A <b>stationary policy</b> is a function \(\pi : S \longrightarrow A\), where given a state \(s,\; \pi(s)\) specifies the action the agent will take. 
			</p>

			<p>
				An <b>optmial policy</b> is the policy with maximum expected reward (discounted value).
			</p>
			<p>
				A fully observable MDP with stationary dynamics, rewards, and an infinite/indefinite horizon, will always have an optimal stationary policy. 
			</p>

			<h4>Introducing \(Q\)</h4>

			<div class="blue">
				<ul>
					<li>
						\(Q^\pi(s,a)\) is the expected value of doing \(a\) in state \(s\), following policy \(\pi\).
					</li>
					<li>
						\(V^\pi (s)\) is the expected value of the state \(s\) itself, following policy \(\pi\). 
					</li>
				</ul>
				<p>
					The "Q-value", and the state value, are usually mutually defined, as 
					\begin{align}
						Q^\pi (s,a) &= \sum_{\forall s'} P(s'| s, a) (R(s,a,s') + \gamma V^\pi(s')) \\
						V^\pi (s) &= Q(s, \pi(s))
					\end{align}
				</p>
			</div>
			<p>
				So uh, if they're mutually defined, how do we know where to start? Usually random initialisation.
			</p>
			
			<div class="blue">
				<p>
					The <b>optimal policy</b> value and q-value, denoted by \(Q^*(s,a)\) and \(V^*(s)\), is defined as:
					\begin{align}
						Q^*(s,a) &= \sum_{\forall s'} P(s'| s, a) (R(s,a,s') + \gamma V^*(s')) \\
						V^*(s) &= \max_a Q(s,a)
					\end{align}
					Which also leads to 
					<span class="nopad">
						\[\pi^*(s) = \textrm{arg}\max_a Q(s,a)\]
					</span>
					<i>argmax: take the action that generates the largest Q value</i>
				</p>
			</div>

			<p>
				Since these are defined recursively, the idea is to iteratively converge them to the optimal value.
			</p>

			<p>
				If we say \(V_k, Q_k\) are "\(k\)-step lookahead functions" - i.e. they work out the value after \(k\) steps, and we set \(V_0\) arbitrarily/randomly, we can compute \(Q_{i+1}, V_{i+1}\) from \(V_i\) and have it converge quickly to the optimum.
			</p>
			<p>
				This is <b>Value Iteration</b>.
			</p>
			<p>
				We can amend this to <b>asynchronous value iteration</b>, where each state can be individually updated instead of the whole state space. 
			</p>
			<p>
				These are stored together in an array, of either values \(V[s]\) or more commonly q-values \(Q[s,a] \).
				<div class="blue" style="display: block;"><span class="nopad">
					\begin{align}
					\textrm{Repeat: } Q[s,a] &\longleftarrow \sum_{\forall s'} P(s' | s, a) (R(s, a, s') + \gamma \max_{a'}(Q[s',a'])) \\
					\iff Q[s,a] &\longleftarrow R(s,a) + \gamma \sum_{\forall s'} P(s' | s, a) \max_{a'}(Q[s',a'])
					\end{align}
				</span></div>
				The above (look at the second one if it's easier) is the basis of the <b>Q-learning equation</b>. It reads: <br>
				<ul>
					<li>The sum, for all possible states \(s'\),</li>
					<li>of the probability of ending up in that new state after doing action \(a\) in state \(s\),</li>
					<li>times (the reward of doing \(a\) in \(s\) and ending in \(s'\)</li>
					<li>plus discounted the max Q-value of all possible actions \(a'\) in that new state \(s'\))</li>
				</ul>
				There's a similar equation for Value, with the last bit being \(\gamma V[s']\) instead.
			</p>
			<p>
				This'll converge for every state if every state is visited enough. 
			</p>

			<h3 id="rei-3">Q Learning</h3>

			<p>
				Reinforcement Learning, or Q-learning, is <i>flat, explicit state, indefinite/infinite stage, deterministic or stochastic,</i> with <i>complex preferences,</i> being <i>single agent, learned knowledge,</i> and <i>perfect rationality</i>.
			</p>

			<p>
				For <b>deterministic Q-Learning</b>, it is fairly simple, since no probability \(P(s' | s, a)\) is taken into account. We initialise our table of Q values \(Q[S,A] \) <b>randomly</b>. 
			</p>
			<p>
				Then, observe the current state \(s\). Repeat forever (or until stopped):
				<ul>
					<li>Select and perform action \(a\)</li>
					<li>Get reward \(r\) for state \(s'\)</li>
					<li>\(Q[s,a] \longleftarrow r + \gamma \max_{a'} Q[s', a']; \)</li>
					<li>\(s \longleftarrow s'\)</li>
				</ul>
			</p>
			<p>
				For nondeterministic, naturally we have to take into account the probabilties of ending in each state after doing an action as well.
			</p>

			<hr>

			<p>
				Suppose we have a history of values \(v_1, v_2, \dots\), and we want to predict the next, given what we know. The natural way to do this is with <b>running average:</b>
			</p>
			<p>
				Let \(A_k\) be estimate based on first \(k\) values of \(v\) -- the mean will do: \(A_k = \frac{v_1 + \cdots + v_k}{k}\)
				\begin{align}
					kA_k &= v_1 + \cdots + v_{k-1} + v_k \\ 
					&= (k-1) A_{k-1} + v_k \\
					A_k &= (1 - \frac{1}{k}) A_{k-1} + \frac{v_k}{k} \\
					&= (1-\alpha_k)A_{k-1} + \alpha_k v_k &(\alpha_k = \frac{1}{k}) \\
				\end{align}
				Thus we get, as the <b>temporal difference equation</b>:
				<div class="blue"><span class="nopad">
					\[A_k = A_{k-1} + \alpha_k(v_k - A_{k-1}).\]
				</span></div>
				
				\(v_k - A_{k-1} \) is called the <b>temporal difference error (TD Error)</b>. If \(\sum_{k=1}^\infty a_k = \infty\) and \(\sum_{k=1}^\infty a^2_k &lt; \infty\), then Q values <b>will converge</b>.
			</p>

			<hr>
			<p>
				An <b>experience</b> is a tuple \(\langle s,a,r,s' \rangle\), that describes an agent in a state, doing an action, getting a reward, and ending up a new state. 
			</p>
			<p>
				This is one piece of data for updating our q-value for \(Q[s,a]\), to give a new estimate of \(Q^* (s,a)\) via
				\[r + \gamma V[s'] = r + \gamma \max_{a'} Q[s',a'] \]
			</p>
			<p>
				Note, that what we're doing with \(Q[s,a]\) is just predicting the value based on our past experiences, thus we can use the <b>temporal difference equation</b> to Q-learn: 
			</p>
			<div class="blue"><span class="nopad">
				\[Q[s,a] \longleftarrow Q[s,a] + \alpha(r + \gamma \max_{a'} Q[s',a'] - Q[s,a])\]
			</span></div>
			<p>
				The \(\alpha\) value was \(\frac{1}{k}\), where \(k\) is a constant, and so we can simply make \(\alpha\) the constant -- which is between 0 and 1, showing how much the agent prizes new information. (No learning happens at 0). This constant will be called the <b>step size</b>.
			</p>
			
			<button class="collapsible">Pseudocode... </button>
			<div class="ccontent">
				<div class="codediv">AGENT qLearn(\(S, A, \gamma, \alpha\))
	<i>inputs</i>
		<i>\(S\) of states, \(A\) of actions, \(\gamma\) discount, \(\alpha\) step size</i>
	<i>local</i>
		<i>float array \(Q[S,A]\)</i>
		<i>state \(s, s'\)</i>
		<i>action \(a\)</i>
	
	\(Q[S,A] \longleftarrow\) random values 
	observe current state \(s\)
	while not terminated:
		select action \(a\)
		\(r, s' \longleftarrow\) execute(\(a\))
		\(Q[s,a] \longleftarrow Q[s,a] + \alpha(r + \gamma \max_{a'} Q[s', a'] - Q[s,a]\)
		\(s \longleftarrow s'\)</div>
			</div>
			
			<hr>

			<p>
				Q learning will always converge to an optimum ... provided each action and state is encountered enough. 
			</p>
			<p>
				<b>Exploiting</b> is selecting an action that maximises your \(Q[s,a]\), whereas <b>Exploring</b> is selecting literally anything else. 
			</p>
			<p>
				If we only exploit, then we will never learn anything new. If we only explore ... well what's the point of this whole "learning" thing then!? So we need some strategy to balacne exploration with exploitation. 
			</p>
			
			<p>
				The <b>epsilon-greedy strategy</b> uses a constant, \(\varepsilon\). Choose a random action (explore)* with probability \(\varepsilon\) and exploit with probability \(1-\varepsilon\).
			</p>
			<p class="small">
				*all random actions are equally selected between, and may include the maximised q-value action
			</p>
			<p>
				The <b>softmax action selection strategy</b> uses a temperature \(\tau > 0\), and will choose an action \(a\) in state \(s\) weighted with the probability 
				\[\frac{e^{\frac{Q[s,a] }{\tau}}}{\sum_a e^{\frac{Q[s,a] }{\tau}}}\]
			</p>
			<p>
				Amongst others.
			</p>

			<p>
				We know that q-values converge however. Thus, exploring has diminishing returns, and so we would want a mechanism that <b>decays</b> our constants over time to emphasise exploration. 
			</p>

			<h4>SARSA</h4>

			<p>
				Q-learning does what's called <b>off-policy learning</b>, meaning values converge regardless of policy. As long as there is enough exploration, there will be convergence. The value of the policy is never learned. 
			</p>
			<p>
				If exploration is dangerous (large negative rewards), this may be undesirable.
			</p>
			<p>
				<b>On-policy learning</b> learns the value of the policy, including exploration, in order to improve the policy. 
			</p>
			<p>
				Modify the experience tuple to be \(\langle s, a, r, s', a' \rangle\) (hence the name), the equation, and the algorithm accordingly. 
			</p>
			<p class="blue">
				The <b>SARSA equation</b> is 
				\[Q[s,a] \longleftarrow Q[s,a] + \alpha(r + \gamma Q[s', a'] - Q[s,a])\]
				i.e. <b>consider the action taken and not the best possible action</b>
			</p>
			<button class="collapsible">Pseudocode (SARSA)... </button>
			<div class="ccontent">
				<div class="codediv">AGENT qLearn(\(S, A, \gamma, \alpha\))
<i>inputs</i>
	<i>\(S\) of states, \(A\) of actions, \(\gamma\) discount, \(\alpha\) step size</i>
<i>local</i>
	<i>float array \(Q[S,A]\)</i>
	<i>state \(s, s'\)</i>
	<i>action \(a\)</i>

\(Q[S,A] \longleftarrow\) random values 
observe current state \(s\)
select action \(a\) using policy based on \(Q\)
while not terminated:
	\(r, s' \longleftarrow\) execute(\(a\))
	select action \(a'\) using policy based on \(Q\)
	\(Q[s,a] \longleftarrow Q[s,a] + \alpha(r + \gamma Q[s', a'] - Q[s,a]\)
	\(s \longleftarrow s'\)
	\(a \longleftarrow a'\)</div>
			</div>


			
		</div>
		

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="../../js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
</body>
</html>