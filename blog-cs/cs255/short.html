<!DOCTYPE html>
<html>
	<head>
		<title>CS255</title>
		<meta charset="utf-8">
		<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
		<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
		<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
		<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
		<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="./index.html" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="../../blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="../../about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS255 Abridged</h1>
                    <p class="subheading">Artificial Intelligence</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Introduction</h1>
			</div>
		</header>

		<!-- <div class="buttonwrapper beside" >
			<a href="./index-zh.html">简体中文</a>
		</div> -->
		<!-- REMEMBER TO DO! -->

		<div class="cbox">
			<p>
				AI is a massive module, filled with a lot of important things and redundant fluff. My aim here is to rewrite my notes for the module to remove all of the fluff, which not only makes the task easier for me, but also means there's less filler content. <b>This page assumes some inherent knowledge from AI and its keywords</b>, as well as knowledge of logic from CS130 or CS262 -- it will not do to read it from scratch.
			</p>
			<p>
				The old long one can be found <a href="./">here</a>.
			</p>

			<p>
				Looking through the past exam papers, there are 7 overall topics that are included, some more than others, being
			</p>

			<ul>
				<li><b>Conditional Probability and Bayes' Theorem</b>
				</li>
				<li>
					<b>CSPs</b>
				</li>
				<li>
					<b>Graph Searching and Heuristics</b>
				</li>
				<li>
					Reinforcement (Q-) Learning 
				</li>
				<li>
					Knowledge Bases 
				</li>
				<li>
					Partial Order Planning and Rule systems
				</li>
			</ul>

			<h2>Contents</h2>

			<ul>
				<li><a href="#search">Search</a></li>
				<li><a href="#probability">Conditional Probability and Bayes</a></li>
			</ul>
		</div>

		
		<div class="colourband">
			<h2 id="search">Search</h2>
		</div>

		<div class="cbox">
			<p>
				<b>Search</b> is one of the most essential forms of problem solving. It entails making moves along a problem space (usually nodes in a graph) in order to try get to a goal. 
			</p>
			
			<p>
				There are two main methods of searching: <i>Informed</i> and <i>Uninformed</i>.
			</p>

			<p>
				In all cases, assume we have reduced the problem down to a graph of nodes, where searching algorithms is at home. Specifically, often we want to talk about a tree graph. Trees have a root node and some goal nodes further down.
			</p>

			<p>
				Graph searching is pretty much no different from tree searching, except that we use \(\langle x, y, z \rangle\) to represent paths.
			</p>

			<h3>Uninformed Search</h3>

			<p>
				Uninformed does not use information from the question - it is a brute force method.
			</p>

			<h4>Generic Tree Search</h4>

			<p>
				In the general case, a search, starting from the root node of a tree (or from a given node in a graph) can be described as follows:
			</p>

			<ol class="side">
				<li>
					While there are still nodes yet to be expored (candidates for expansion):
					<ol type="a">
						<li>
							Expand a node according to your searching strategy
						</li>
						<li>
							Is it a goal? If so, return success. Else, carry on.
						</li>
					</ol>
				</li>
				<li>
					Return failure
				</li>
			</ol>

			<p>
				The unexpanded nodes that we can immediately expand to are called the <b>frontier</b>, and are stored in a <b>queue</b> structure -- our search strategy determines how this queue is ordered.
			</p>

			<h4>Breadth First Search</h4>

			<p>
				In short: expand the <b>shallowest</b> node first. 
			</p>
			<p>
				Frontier queue is ordered by distance from the origin/root. Successor nodes are added to the <b>end</b> of the queue.
			</p>

			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/5/5d/Breadth-First-Search-Algorithm.gif" alt="" style="max-width: 300px;">
				<figcaption>Process of BFS (Wikimedia Commons)</figcaption>
			</figure>

			<p>
				For a branching factor \(b\) and the <i>depth of least cost solution</i> \(d\) the <b>time</b> complexity is \(O(b^d)\)... i.e. \(O(n)\) in number of nodes. The <b>space</b> complexity is the <b>same</b> - which can be a big problem if there are lots of nodes.
			</p>
			<p>
				BFS will always find a solution if \(b\) is finite (complete).
			</p>

			<h4>Depth First Search</h4>

			<p>
				In short: expand the <b>deepest</b> node first.
			</p>
			<p>
				Frontier queue is a stack, put successors at the start. 
			</p>

			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/7/7f/Depth-First-Search.gif" alt="" style="max-width: 300px;">
				<figcaption>Process of DFS (Wikimedia Commons)</figcaption>
			</figure>

			<p>
				DFS has a time complexity of \(O(b^m)\), where \(m\) is the <i>maximum</i> depth, rather than that of the nearest solution. This can be bad if \(m\) is large. However, DFS's main advantage is its <b>space</b> complexity -- \(O(bm)\) -- only one path needs to be stored, making DFS better where memory is tighter.
			</p>

			<p>
				DFS is <b>incomplete</b> -- if \(d\) is infinite (or graph has loops) then DFS may never terminate. 
			</p>

			<h4>Lowest Cost First Search</h4>

			<p>
				As the name implies, select a node on the <b>path with the lowest cost</b> first. 
			</p>
			<p>
				The path cost is the sum of all the arcs from the origin to the newly expanded node: \(cost(\langle n_0 \dots n_k \rangle) = \sum_{i=1}^k cost(\langle n_{i-1}, n_i \rangle \).
			</p>
			<p>
				Frontier is priority queue ordered by cost. The first path to a goal found is the least cost goal. Note this reduces to breadth first when all arcs are of equal cost.
			</p>

			<h3>Informed Search</h3>

			<p>
				Informed search uses "problem specific knowledge", such as the location of the goal, an estimate of distance, etc., to help inform its search choices. They are usually much better than brute force uninformed search.
			</p>

			<h4>Best First Search</h4>

			<p>
				Best first search uses a <b>heuristic</b> - some estimate of the final distance for each path to determine the choice of exploration. Heuristics come up <b>a lot</b>, since AI is all about "good enough". There are two variants:
			</p>

			<p class="side">
				<b>Heuristic DFS</b> picks the <i>node</i> with the best possible heuristic estimate.
			</p>
			<p class="side">
				<b>Greedy Best First Search</b> picks the <i>path</i> with the best possible heuristic.
			</p>
			<p>
				And what is the heurisitc? Well, it depends on the situation, but say you're in a maze, and the nodes are intersections / turns. Perhaps the heuristic is the euclidean (i.e. straight diagonal distance) between that corner and the goal square. 
			</p>
			<p>
				However, you might quickly notice that for a maze, the closest "as the crow flies" might be a massive dead end. This is the problem with BFS, where the heuristic <i>may</i> just lead to the wrong path, and if the algorithm is not programmed well enough, forever looping.
			</p>
			<p class="blue">
				The crucial thing is that <b>heuristics are underestimates</b>.
			</p>

			<p>
				There seems to not be much difference between the two apart from naming, so just default to "Greedy BFS I guess".
			</p>

			<p>
				Greedy BFS has a time and space complexity of \(b^n\) for a <b>branching factor</b> \(b\) and a <b>path length</b> \(n\). It may not ever find a solution, and thus is <b>incomplete</b>.
			</p>

			<h4>A* Search</h4>

			<p>
				<b>The very important significant one</b>. A* takes into account both path cost and remaining heuristic when it does its searching. 
			</p>
			<p class="blue">
				Let \(g(p)\) or \(cost(p)\) be the cost of the current path \(p\), and the heuristic from the end of p to the goal as \(h(p)\).
				<br><br>
				Let \(f(p) = h(p) + g(p)\), the total estimate of a path's cost from start to finish, so to say. This is our final <b>evaluation function</b>
			</p>
			<p>
				A* orders the frontier by \(f(p)\), and picks like that. In this way, it is a mix of Lowest-cost first and Best-first, and is actually pretty damn good.
			</p>

			<p class="blue">
				An algorithm is <b>ADMISSIBLE</b> if a solution existing \(\implies\) the <b>optimal solution</b> is found.
			</p>
			<p>
				A* is an <b>admissible algorithm</b>.
			</p>

			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/9/98/AstarExampleEn.gif" alt="" style="max-width: 50%;">
				<figcaption>A* Search example (wikipedia)</figcaption>
			</figure>

			<p>
				A good heuristic is better, an overestimate can be really bad but too far of an underestimate is also bad - A* explores every path with an estimate less than the optimal cost, so if there are a lot of paths here, A* can still take a while.
			</p>
			<p>
				A* thus has a time complexity relative to \(\textrm{error of }h(p) \cdot \textrm{length of solution}\) - which is pretty good, the only problem is that A* is <b>exponential in space</b>, because it needs all nodes in memory. 
			</p>

			<h3>Cycle Checking and Path Pruning</h3>

			<p>
				This is how you stop algorithms which may not halt from not halting.
			</p>
			<p>
				<b>Pruning</b> a path means removing it from consideration entirely, which saves memory holding unnecessary paths.
			</p>
			<p>
				<b>Cycle Checking:</b> If your explored path reaches an already explored node in memory, for example if you went <code>a -> b -> c -> d -> b</code>, you can prune the <code>d -> c -> b</code> bit without losing an optimal path solution, since it forms a closed cycle. 
			</p>
			<p>
				<b>Path pruning:</b> If you have a path in memory that goes like <code>s -> b -> x -> y -> z -> u -> h -> m -> p</code>, and later on in searching you find a different path <code>s -> i -> m -> p</code> through a new node <code>i</code>, then you can prune away the first path, since you can use the second one to get to the same destination.
			</p>
			<p>
				<b>Note</b> though how I didn't say <i>longer</i> and <i>shorter</i> - that's because it's not, it's recency. <b>This is why you have underestimate heuristics</b> - otherwise you won't necessarily get the most optimal solution.
			</p>
			<p class="blue">
				A <b>Monotone Heuristic</b> is where the heuristic is an underestimate for all arcs across a graph. These heuristics are also called <b>consistent</b>, and will never overestimate. 
			</p>
			<p>
				A* Search with a consistent heuristic is needed to find an optimal path.
			</p>

			<h3>More Searching</h3>

			<p>
				Searching backwards from the goal is effectively the same as searching forward from the start. 
			</p>
			<p>
				Of course if the <i>backwards</i> branching factor is much different from the <i>forwards</i> one, the efficiency of one direction vs another may be vastly different. Sometimes, even, one direction is not available, if the graph is being constructed dynamically.
			</p>
			<p>
				Provided not, we can get <b>bidirectional search</b>: search from both start and end simultaneously. \(2b^{\frac{k}{2}} < b^k\) after all (b branch. factor, k depth of goal). Of course, the frontiers must somehow meet, so one side is usually a BFS.
			</p>
			<p>
				Extend to <b>island-driven search</b>, where we pick \(M\) "interesting locations/checkpoints" and search simultanously from all of those. \(Mb^{\frac{k}{M}}\) is faster still -- the problem is choosing said "interesting locations".
			</p>

			<h4>Iterative Deepening and Depth First Branch and Bound</h4>

			<p>
				We like DFS because low memory, but DFS might follow its own tail into the abyss, so how do we prevent that? We can limit how deep DFS will go - making <b>bounded DFS</b>.
			</p>
			<p>
				But what if the bound is too shallow? Then, we gradually increase the bound, and rerun DFS, until we get to the goal. This is now <b>Iterative Deepening</b> of the bound. 
			</p>
			<p>
				But this is still a dumb algorithm. What if we add h e u r i s t i c s to find an optimal solution? Suppose we already have a path to the goal. Let's set a bound \(p\) as its cost. If we have a path where the cost + heuristic \(&gt; p\), then clearly, it will never be shorter, and thus is immediately pruned. 
			</p>
			<p>
				Rinse and repeat until we have no more shorter paths than our \(p\). That is our optimal solution, and this is <b>Depth First Branch and Bound</b>.
			</p>

			<h3>Finding Heuristics</h3>

			<p>
				Heuristics are underestimates, but the closer the underestimate the better. A heuristic of 0 is no better than a dumb search. Finding heuristics however is difficult, good ones even more so, but there are a few approaches:
			</p>
			<p>
				<b>Relax the problem</b>: try a less restrictive version of the problem. If it's a maze, imagine there's no walls and you can fly. If it's a 15-tile game, imagine you can move tiles through others. 
			</p>
			<p>
				<b>Combine heuristics</b>: If you have several different admissible heuristics, combine them and use the best one out of them for each individual state as your final value.
			</p>
			<p>
				<b>Statistics</b>: Actually run simulations to try get data estimates. This is however <b>NOT ADMISSIBLE</b>, but can be good enough.
			</p>

		</div>

		<div class="colourband">
			<h2 id="probability">Conditional Probability and Bayes</h2>

			
		</div>

		<div class="cbox">
			<ul>
				<li><a href="#pro-1">Probability and Bayes</a></li>
				<li><a href="#pro-2">Decision Making</a></li>
			</ul>

			<h2 id="pro-1">Probability</h2>

			<p>
				First of all, see <a href="../cs130#probability">Probability</a> from CS130 since this needs probability spaces \(\Omega\) annd expected values of random vars \(E[X] = \sum_{s_i \in \Omega} P(s_i) X[s_i] \).
			</p>
			<p>
				Recall that for conditional probability: \(P(A|B) = \frac{P(A \land B)}{P(B)}\). If \(P(A|B) = P(A)\) and \(P(B|A) = P(B)\) then they're independent.
			</p>

			<p class="blue">
				<span class="nopad">
				From which we get <b>Bayes' Theorem</b> \(P(A|B) = \frac{P(B|A)P(A)}{P(B)}\) (provided neither \(P(A), P(B) = 0\)). <br>
				Where \(P(B) = P(B|A)P(A) + P(B|\lnot A)P(\lnot A)\) -- the possibility of B given all occurences of A.
				</span>
			</p>
		
			<p>
				If A is instead split into <i>mutually exclusive, exhaustive</i> states \(a_1 \cdots a_n\) rather than a true and false, then we can extend the equation for \(P(B)\) to be a sum \(\sum_{i=1}^{n} P(B|a_i) P(a_i)\)
			</p>
			<p class="blue">
				Note also the chain rule:
				\[P(a_1 \land a_2 \land \dots \land a_i) = P(a_1) \times P(a_2|a_1) \times P(a_3|a_1 \land a_2) \times \cdots \times P(a_i|a_1 \land a_2 \land \cdots \land a_{i-1} )\]
			</p>
			<p class="blue">
					The <b>joint probability distribution</b> \(P(A,B)\) is a probability dist over A, which has \(n\) states, and B, which has \(m\) states. Thus, for every A state \(a_i\), we have \(m\) entries in the prob. dist for every possible B state (i.e. it's \(A \land B\)). 
			</p>

			<p>
				To work out \(p(a_i)\) from \(p(A, B)\) do \(p(a_i) = \sum_{j=1}^{m} p(a_i, b_j)\). This is referred to as <b>marginalising</b> B out of \(P(A,B)\)
			</p>

			<p>
				Naturally this is difficult for continuous probabilities. In that case, we have to use integration (that I doubt will <i>actually</i> come up).
			</p>

			<p class="blue">
				Note that <b>conditional independence</b> for some \(X, Y\) given \(Z\) means that \(P(X, Y|Z) = P(X|Z) \cdot P(Y|Z)\).
			</p>

			<h3>Inference</h3>

			<p>
				<b>Probabilistic inference</b> is about computing <i>values</i> for queried <i>propositions</i> given observed <i>evidence</i>.
			</p>

			<ul class="side">
				<li>
					Let \(X\) be our query variable, and \(E\) be our evidence variables with observed values \(e\).
				</li>
				<li>
					Let \(Y\) be other, unobserved variables, with some values \(y\)
				</li>
				<li class="nopad">
					<!-- <span class="nopad"> -->
					We want \(p(X|e)\):
					\[p(X|e) =\frac{p(X, e)}{p(e)} = \alpha \sum_{y}p(X, e, y)\]
					Where the \(\frac{1}{p(e)}\) multiplier is called the <b>normalisation constant</b> (sometimes written \(\alpha\)), \(p(e)_= \sum_{x, y} p(x, e, y)\).
					<!-- </span> -->
				</li>
			</ul>
			<p>
				Now, this is probably meaningless when just presented in theory, so below is an example.	
			</p>

			<button class="collapsible active">Simple Example... </button>
			<div class="ccontent" style="display: block;"><span class="nopad">
				<p>
					<b><i>Example.</i></b> Given the following table, we want to calculate P(cavity|toothache).
				</p>
				<table>
					<tr>
						<th></th>
						<th colspan="2">Toothache</th>
						<th colspan="2">¬Toothache</th>
					</tr>
					<tr>
						<th></th>
						<th>catch</th>
						<th>¬catch</th>
						<th>catch</th>
						<th>¬catch</th>
					</tr>
					<tr>
						<th>cavity</th>
						<td>0.108</td>
						<td>0.012</td>
						<td>0.072</td>
						<td>0.008</td>
					</tr>
					<tr>
						<th>¬cavity</th>
						<td>0.016</td>
						<td>0.064</td>
						<td>0.144</td>
						<td>0.576</td>
					</tr>
				</table>

				<p>
					Note we have an "unobserved" (irrelevant variable) catch. Then, to work everything out:
				</p>
				<ul>
					<li>p(cavity \(\land\) toothache) = 0.108 + 0.012 = 0.12 (marginalising catch)</li>
					<li>p(cavity \(\land\) ¬toothache) = 0.008 + 0.072 = 0.08</li>
					<li>Then p(cavity) = 0.108 + 0.012 + 0.016 + 0.064 = 0.2</li>
					<li>p(toothache) = 0.108 + 0.012 + 0.016 + 0.064 = 0.2</li>
					<li>Thus p(cavity|toothache) = \(\frac{0.12}{0.2}\) = 0.6.</li>
				</ul>
			</span>
			</div>

			<p>
				For more complex examples, this type of enumeration may become incredibly difficult - thus we need some sort of <i>graphical method</i> to represent and take advantage of conditional independence.
			</p>
			<p>
				<img src="./infer-tree.png" alt="" align="right" style="size: 100px;">
				Since often events lead to other events which lead further in a chain, we can represent this graphically. 
			</p>
			<p>
				We want an acyclic graph, where each node corresponds to one variable in our probability distribution. 
			</p>
			<p>
				This graph should then satisfy the <b>markov condition</b>:
			</p>
			<p class="blue">
				<b>Markov Condition:</b> For all variables \(V\), \(V\) is <b>conditionally independent</b> of all its <b>non</b>descendants, given its parents: \(P(B,C|H) = P(B|H) \cdot P(C|H)\) because C does not descend from B.
			</p>
			<p>
				This reduces vastly the number of conditional probabilities we need to work out, and simplifies the ones we do -- only the conditional probabilities \(P(X|PA_X)\) (given the parents of X) need to be calculated.
			</p>
			<p>
				This type of graph is a <b>Bayesian Belief Network</b> -- directed, acyclic, showing influence, and in total representing a full joint prob. dist.
			</p>

			<p>
				When building a BBN, order of nodes matters (different order = different complexity) -- we really only want a node to depend on only those before it. Then, we can use chain rule principles and only the parents function \(PA_X\) to define the joint probability:
				\[P(X_1, X_2, \dots, X_n) = \prod_{i=1}^n P(X_i | PA_{X_i})\]
			</p>
			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/0e/SimpleBayesNet.svg/400px-SimpleBayesNet.svg.png" alt="">
				<FIGCaption>Bayesian Belief Network example (Wikiepdia)</FIGCaption>
			</figure>

			<h3>Noisy OR</h3>

			<p>
				Even still, local probability distributions tend to grow very quickly (\(O(2^n)\)) -- simplify using "canonical interaction" models like Noisy OR.
			</p>
			<div class="blue">
				<p>Noisy OR:</p>
				<ul>
					<li>Describes set of \(n\) clauses \((x_1 .. x_n\) and their common effect (descendant) \(y\)</li>
					<li><b>Assume</b> each \(x_i\) is sufficient to cause \(y\) regardless of state of other clauses: <b>independence</b>.</li>
					<li>Thus \(p_i\) (prob. of causing \(y\) rel. to \(x_i\)) \(= P(y|\lnot x_1, \dots, x_i, \lnot x_{i+1}, \dots, \lnot x_n)\)</li>
				</ul>
			</div>

			<p>
				<img src="./infer-noisy-or.png" alt="" align="right">

				Shown on the right is a BBN concerning <b>F</b>atigue, <b>B</b>ronchitis, <b>L</b>ung cancer, and <b>O</b>ther causes of fatigue. 
			</p>

			<p>
				<b>Causal inhibition</b> is where each of the three causes F, O, L, has an inhibitor (not shown): Bronchitis will cause fatigue <b>if and only if</b> the inhibior mechanism for that cause is absent.
			</p>

			<p>
				<b>Exception Independence</b> is where the inhibiting mechanism for each cause is independent.
			</p>

			<p>
				And of course, the effect F can only happen if at least one cause is present.
			</p>

			<p>
				A noisy-OR model adds new <b>deterministic nodes</b>, whose values are exactly specified by parents. We can extend the above BBN into a Noisy-OR tree, add in the inhibitors, and <i>voila</i>.
			</p>

			<figure>
				<img src="./noisy-or-tree.png" alt="" title="Noisy OR tree" style="max-width: 70%;">
			</figure>

			<p>
				Course a BBN won't actually show this, and condenses it down into a single probability / probability table. 
			</p>
			<p>
				When constructing a network, only draw causal links from the new node to already existing ones - depending on the order new nodes are added, this wildly changes the number of arcs and thus the complexity.
			</p>

			<h3>Reasoning</h3>

			<p>
				Different types of reasoning can be done on a BBN:
			</p>
			<ol>
				<li><b>Diagnostic Reasoning</b> from symptop to cause: backwards along the arcs. From our evidence reason <i>up</i> until we work out our query -- often involves Bayes' rule.</li>
				<li><b>Causal Reasoning</b> from cause to symptom: forwards along the arcs -- often involves <i>just reading from the graph</i>.</li>
				<li><b>Intercausal Reasoning</b>: If a cause and an immediate symptom are observed together, this concerns all other causes of a symptom</li>
				<li><b>Combined Reasoning</b> if we have to go backwards and forwards simultaneously</li>
			</ol>

			<p>
				A lot of this entials enumeration of possibilities.
			</p>

			<button class="collapsible">Reasoning Over 3 Nodes... </button>
			<div class="ccontent" style="display: none;">
				<p><b><i>Example.</i></b>; note that \(\alpha\) is the \(1/p\) normalisation constant.</p>
				<img src="./reasoning-3-node.png" alt="">
			</div>

			<h4>Variable Elimination</h4>

			<p>
				VE is more efficient than enumeration. VE works over factors, which:
			</p>
			<p class="blue">
				A <b>factor</b> is a <b>function</b> on a set of variables, called the <b>scope of the factor</b>.
				<br><br>
				Conditional Probability \(P(x|y,z)\) can be described as a factor with scope \(x,y,z\).
			</p>
			<p>
				If we can (1) order variables, (2) index them, then we can uniquely represent each factor as a <b>1d array</b>. Ex. the table 
				\begin{array} {|r|r|}\hline x & y & f_0 = P(z=T | x, y) \\ \hline T & T & 0.1 \\ \hline T & F & 0.2 \\ \hline F & T & 0.3 \\ \hline F & F & 0.4 \\ \hline  \end{array}
				Can be represented as the list \(f_0 = [0.1, 0.2, 0.3, 0.4] \). We can then <b>condition, sum,</b> and <b>multiply</b> these.
			</p>
			<p>
				<b>Conditioning:</b> If we have observed a var with a value, we can define a new factor with reduced scope: Starting with \(P(x|y,z\) then observe \(z=T\), define new factor \(P(x|y,z=t)\) with a scope \((x, y)\), since \(z\) is now known.
			</p>
			<p>
				<b>Multiplying:</b> If factor 1 has scope \(A, B\) and factor 2 has scope \(B, C\) then we can multiply these together, which is doing a <code>natural join</code> multiply. If \(f_1: A=T, B=F \implies 0.9\) and \(f_2: B=F, C=T \implies 0.6\) then \(f_{new}: A=T, B=F, C=T \implies 0.6 \times 0.9 = 0.54\).
			</p>
			<p>
				<b>Summing:</b> Eliminating a chosen variable by adding together possible outcomes. Given \(f(X,Y,Z)\) (factor over X, Y, Z) eliminate Y by doing \(f_{new}(X, Z) = f(X, Y=T, Z) + f(X, Y=F, Z)\).
			</p>
			<p>
				Thus, the VE algorithm goes as
			</p>
			<ol class="side">
				<li>Construct factor for each conditional probability</li>
				<li>Eliminate each non-query variable:
					<ol type="a">
						<li>If variable has observed value, set it to that and condition</li>
						<li>Otherwise sum it out</li>
					</ol>
				</li>
				<li>Multiply remaining factors and normalise</li>
			</ol>

			<h2 id="pro-2">Decision Making</h2>

			<p>
				BBNs model probabilities, and often these probabilities are used to make decisions. Decision making can also be formalised.
			</p>
			<p>
				First though, given a set of outcomes \(O=\{O_i ..\} \) of an action \(A\), a <b>utility function</b> \(U(O_i|A)\) assigns a utility to each outcome -- desirability. 
			</p>
			<p>
				The expected utility is a weighted sum of outcomes' probabilities and their utils,
				\[\sum_i P(O_i|A) \times U(O_i|A)\]
				And we assume that a decision maker always wants to <b>maximise utility</b>.
			</p>

			<p>
				The decision maker is an agent, and it must know how good outcomes are <i>relative</i> to each other. These relations should be complete and transitive.
			</p>

			<div class="blue">
				<p>Two outcomes \(\sigma_1, \sigma_2\) are</p>
				<ul>
					<li><b>Weakly preferred</b> \(\sigma_1 \succeq \sigma_2\), 1 is at least as good as 2</li>
					<li><b>Indifferent</b> \(\sigma_1 \sim \sigma_2\), 1 and 2 are the same</li>
					<li><b>Strictly preferred</b> \(\sigma_1 \succ \sigma_2\), 1 is better than 2, we do not weakly prefer 2</li>
				</ul>
			</div>

			<h3>Decision Trees</h3>

			<p>
				Decision trees have 2 types of nodes. <b>Chance Nodes</b> are circles, and are random variables. <b>Decision Nodes</b> are squares, and represent a set of mutually exclusive choices.
			</p>

			<button class="collapsible active">Buying stock...</button>
			<div class="ccontent" style="display: block;">
				<p>
					<img src="./decision-tree.png" alt="" align="right" style="max-width: 30%;">
					<b><i>Example.</i></b> Suppose you have £1000 to spend. You can put it in the bank, with a guaranteed 0.5% return in a month, or buy 100 shares of OCRCompnay @ $10 a share. The tree looks as follows:
				</p>
				<p>
					\(EU(X) = 0.25 \times 500 + 0.25 \times 1000 + 0.5 \times 2000 = £1375\).
				</p>
				<p>
					\(EU(D) = \max(EU(X), 1005) = £1375\)
				</p>
				<p>
					Thus buy stonks.
				</p>
				
			</div>
			<p>
				Naturally real people have different views on risk vs reward than an agent which goes off dumb numbers and a max function. We can also model this however. 
			</p>
			<p>
				We can change our utility function to instead of being just the straight number, account for the risk involved as well:
				\[U_R(x) = e^{\frac{-x}{R}}\]
				Thus with a higher R value for certain options over others, we can represent risk accordingly.
			</p>

			<p>
				If we have non-numeric outcomes in our decision tree, such as whether or not a suit gets ruined, we can still model this by <b>assigning</b> utility values, often 0 for the least desiresd, 1 for the most desired, and something in between for the others that reflects their undesirability vs their chance to mitigate disaster.
			</p>

			<h3>Influence Diagrams</h3>

			<p>
				
			</p>
		</div>
		

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="../../js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
</body>
</html>