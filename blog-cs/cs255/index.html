<!DOCTYPE html>
<html>
<head>
	<title>CS255</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="../../" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="../../blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="../../about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS255</h1>
                    <p class="subheading">Artificial Intelligence</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Introduction</h1>
			</div>
		</header>

		<div class="cbox">
			<h3>Contents</h3>

			<ol>
				<li><a href="#rational-agents">Rational Agents</a></li>
				<li><a href="#uninformedsearch">Uninformed Search</a></li>
				<li><a href="#informed">Informed Search</a></li>
			</ol>

			<h3>Introduction to Artificial Intelligence</h3>

			<p>
				A defintion of artificial intelligence can fall under 4 categories:
			</p>

			<table >
				<tr>
					<td>Thinking humanly</td>
					<td>Thinking rationally</td>
				</tr>
				<tr>
					<td>Thinking rationally&emsp13;</td>
					<td>Acting rationally</td>
				</tr>
			</table>

			<button class="collapsible">Categories...</button>
			<div class="ccontent">
				<div class="cornell">

					<div>
						<p>Acting Humanly</p>
					</div>
					<div>
						<p>
							The <i>Turing Test</i> is a well known test for how well an AI performs. It suggests that if it is acting intelligently, it is thinking intelligently. 
						</p>
						<p>
							It also suggests that an AI requires <b>knowledge</b>, <b>reasoning</b>, <b>learning</b> (and language).
						</p>
						<p>
							However the general consensus is that the Turing Test is not great, as many counterexamples and exploits can be found for it. 
						</p>
						<p>
							One such "counterexample" is <i><a href="https://plato.stanford.edu/entries/chinese-room/" class="text">Searle's Chinese Room</a></i>. 
						</p>
					</div>
	
					<div>
						<p>Thinking Humanly</p>
					</div>
					<div>
						<p>
							There is a field of study called cognitive modelling, which is modelling human thinking in programs. (We will however ignore this)
						</p>
					</div>
	
					<div>
						<p>Thinking rationally</p>
					</div>
					<div>
						<p>Given a premise/some premises, an agent (the AI model) must derive or deduce the solution via logic. Rational deduction however has problems dealing with uncertainty.</p>
					</div>
	
					<div>
						<p>Acting Rationally</p>
					</div>
					<div>
						<p>
							Acting rationally can be summed up as "doing the right thing", which is to say, the action that <i>maximises</i> some achievement metric. However, thought or inference in these actions is not necessarily involved.
						</p>
	
					</div>
				</div>	
			</div>

			
			<div class="cornell">
				<div>
					<p></p>
				</div>
				<div>
					<p>
						In actuality, the boundaries between these four categories are quite blurred.
					</p>
				</div>
				<div>
					<p>Specialised AI vs AGI</p>
				</div>
				<div>
					The AI type that is employed everywhere is called <i>specialised</i> AI, AI built for a specific task. There is also the concept of Artificial General Intelligence (AGI), which is what sci-fi AI usually describes. 
				</div>
				<div>
					<p>Rational Agents</p>
				</div>
				<div>
					<p>Specifically, we look at <b>Rational Agents</b>.</p>
					<p>
						These agents can be modelled as a function \(f:P \longrightarrow A\), where P denotes <i>percept histories</i> and A denotes <i>actions</i>.
					</p>
					<p>
						Agents are typically required to exhibit autonomy, and seek the best performance possible. (Perfect rationality may not always be the best as there is a trade off with computing resources)
					</p>
				</div>
			</div>

			
		</div>


		<div class="colourband">
			<h2 id="rational-agents">Rational Agents</h2>

		</div>

		<div class="cbox">
			<h3>Introduction</h3>
			
			<ol>
				<li><a href="#rat-1">Dimensions of Complexity</a></li>
				<li><a href="#rat-2">Representations and Solutions</a></li>
			</ol>

			<div class="cornell">
				<div>
					<p>Agent Input Model</p>
				</div>
				<div>
					<img src="./agent-input.svg" alt="An agent takes as inputs its Abilities, Goals, Prior Knowledge, Stimuli and Past Experiences from the environment, and acts on the environment.">
					<p>
						Above shows a diagram of what inputs an agent takes, and what it does.
					</p>

					<p style="display: none;">
						Think of a self-driving car. What attributes would fit for each label? For abilites, we could have steer, break, accelerate, ...; for goals getting to destination, being safe; prior knowledge could be signs, road speeds, laws, ...; stimuli vision, GPS; and for prior experience internal streetmaps, etc.
					</p>
				</div>

				<div>
					<p>Rational Agents</p>
				</div>
				<div>
					<p>The Goals of a rational agent can be specified by a <b>performance measure</b>, which is a numerical value.</p>
					<p>A <b>rational action</b> is an action which aims to <i>maximise</i> the performance measure, given the percept history to date. </p>

					<p>Note that an agent is <b><span class="sc">not</span></b> onmiscient, clairvoyant <i>(forseeing changes)</i>, nor always successful. The <i>expected</i> value, not the actual value, is considered - an action to maximise expected value which fails is <i>still</i> rational.</p> 
				</div>
			</div>

			<h3 id="rat-1">Dimensions of Complexity</h3>

			<div class="cornell">
				<div>
					<p>Dimensions of Complexity</p>
				</div>
				<div>
					<p>
						The design space / considerations of an AI can be defined by a set of "dimensions of complexity".
					</p>

					<table>
						<tr>
							<th>Dimension</th>
							<th>Values</th>
						</tr>
						<tr>
							<td>Modularity</td>
							<td>Flat, modular, heirarchical</td>
						</tr>
						<tr>
							<td>Planning Horizon</td>
							<td>Static/none, finite (stage), indefinite, infinite</td>
						</tr>
						<tr>
							<td>Representation</td>
							<td>Flat state, features, relations</td>
						</tr>
						<tr>
							<td>Computational Limits</td>
							<td>Perfect or bounded rationality</td>
						</tr>
						<tr>
							<td>Learning</td>
							<td>Knowledge given or learned</td>
						</tr>
						<tr>
							<td>Sensing Uncertainty</td>
							<td>Fully or partially obervable</td>
						</tr>
						<tr>
							<td>Effect Uncertainty</td>
							<td>Deterministic, stochastic</td>
						</tr>
						<tr>
							<td>Preference</td>
							<td>Goals, complex preferences</td>
						</tr>
						<tr>
							<td>Number of Agents</td>
							<td>Single, multiple</td>
						</tr>
						<tr>
							<td>Interaction</td>
							<td>Offline, online</td>
						</tr>
					</table>
					<hr>
				</div>
			</div>

			
			<div class="cornell">
				
				<div>
					<p>Modularity</p>
				</div>
				<div>
					<p>
						<b>Modularity</b> concerns the structure and abstraction.
						<ul>
							<li><b>Flat</b> where the agent only has 1 level of abstraction</li>
							<li><b>Modular</b> where the agent has interacting modules that can be separately understood</li>
							<li><b>Hierarchical</b> where the agent has modules that (recursively) decompose into others</li>
						</ul>
						Flat representations are good for simple systems, whereas complex systems will be heirarchical. The former can be continuous or discrete, whilst the latter is generally a hybrid.
					</p>
				</div>

				<div>
					<p>Planning Horizon</p>
				</div>
				<div>
					<p>
						<b>Planning Horizon</b> concerns how the world is, and how far forward does the agent plan to.
						<ul>
							<li><b>Static</b>, where the world does not change</li>
							<li><b>Finite Stage</b>, where the agent reasons for a fixed number of timesteps</li>
							<li><b>Indefinite State</b>, agent reasons about a finite, not predetermined number of steps</li>
							<li><b>Infinite Stage</b>, agent plans for going forever (process oriented)</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Representation</p>
				</div>
				<div>
					<p>
						Finding compact <b>Representations</b> and exploiting them for computational gains. An AI can reason in
						<ul>
							<li><b>Explicit States</b>, where a state describes everything about a world</li>
							<li><b>Features/Propositions</b>, which are descriptors of states. A few features can lead to a <i>lot</i> of states.</li>
							<li><b>Individuals</b> and their <b>relations</b>, a feature for each relationship on each tuple of individuals. Thus an agent can reason without knowing the individuals or if there are infinite individuals.</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Computational Limits</p>
				</div>
				<div>
					<p>
						<b>Computational Limits</b>:
						<ul>
							<li><b>Perfect rationality</b> is ideal, but does not take into account computing resources</li>
							<li><b>Bounded rationality</b> is what we hope to achieve, good descisions made accounting for perceptual and computing limitations</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Learning from Experience</p>
				</div>
				<div>
					<p>
						<b>Learning from Experience</b>:
						<ul>
							<li>Where knowledge is <b>given</b></li>
							<li>Or knowledge is <b>learned</b> from data or past experience</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Uncertainty</p>
				</div>
				<div>
					<p>
						<b>Uncertainty</b> has two dimensions: <i>sensing</i> and <i>effect</i>. In each, an agent can have:
						<ul>
							<li>
								<b>No</b> (uncertainty), agent knows what is true
							</li>
							<li>
								<b>Disjunctive</b>, a set of states are possible
							</li>
							<li>
								<b>Probabilistic</b>, there is a probability distribution between states
							</li>
						</ul>

						(On probability) Agents need to act even if they are uncertain, thus predictions are needed to decide what to do, which can be <i>definitive</i> (this <span class="sc">will</span> happen), <i>disjunctions</i> (do event A <span class="sc">else</span> B will happen), or <i>point probabilities</i> (Probability of the outcome is 0.01 if you do A or 0.8 if otherwise).
					</p>
				</div>

				<div>
					<p>Sensing Uncertainty</p>
				</div>
				<div>
					<p>
						<b>Sensing Uncertainty</b> is about what the agent can sense about state, the state can be
						<ul>
							<li><b>Fully observable</b></li>
							<li><b>Partially observable</b> (possible number of states from obs.)</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Effect Uncertainty</p>
				</div>
				<div>
					<p>
						<b>Effect Uncertainty</b> concerns whether an agent could predict an outcome from a state and an action.
						<ul>
							<li><b>Deterministic</b> environments say yes</li>
							<li><b>Stochastic</b> environments say no (have uncertainty).</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Preferences</p>
				</div>
				<div>
					<p>
						<b>Preferences</b> concerns what an agent tries to achieve.
						<ul>
							<li>
								<b>Achievement goal</b> is a single goal to achieve - can be a (complex) logical formula
							</li>
							<li>
								<b>Complex preferences</b> involve tradeoffs between various desires, with priorities. These are further split into <i>Ordinal</i> (order priority matters) and <i>Cardinal</i> (specific absolute values also matter).
							</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Number of Agents</p>
				</div>
				<div>
					<p>
						Crucially, <b>Number of <i>reasoning</i> agents</b>.
						<ul>
							<li>
								<b>Single Agent</b> reasoning relegates every other agent as environment and not really considered, whilst
							</li>
							<li>
								<b>Multiple Agent</b> reasoning involves an agent reasoning strategically about other agents.
							</li>
						</ul>
						Agents have their own goals, which can be competetive, cooperative, or independent.
					</p>
				</div>

				<div>
					<p>Interaction</p>
				</div>
				<div>
					<p>
						<b>Interaction</b> talks about when the agent reasons relating to its action
						<ul>
							<li><b>Reason offline</b>, before acting</li>
							<li><b>Reason online</b>, whilst interacting with environment</li>
						</ul>
					</p>
					<hr>
				</div>
				
			</div>

			<div class="cornell">
				<div>
					<p>Examples</p>
				</div>
				<div>
					<p>
						Some examples of the dimensions of different agents
					</p>
					<button class="collapsible"  id="sss">Example: State-space search</button>
					<div class="ccontent">
						<table>
							<tr>
								<th>Dimension</th>
								<th>Values</th>
							</tr>
							<tr>
								<td>Modularity</td>
								<td>Flat</td>
							</tr>
							<tr>
								<td>Planning Horizon</td>
								<td>indefinite</td>
							</tr>
							<tr>
								<td>Representation</td>
								<td>Flat state</td>
							</tr>
							<tr>
								<td>Computational Limits</td>
								<td>Perfect rationality</td>
							</tr>
							<tr>
								<td>Learning</td>
								<td>Knowledge given</td>
							</tr>
							<tr>
								<td>Sensing Uncertainty</td>
								<td>Fully obervable</td>
							</tr>
							<tr>
								<td>Effect Uncertainty</td>
								<td>Deterministic</td>
							</tr>
							<tr>
								<td>Preference</td>
								<td>Goals</td>
							</tr>
							<tr>
								<td>Number of Agents</td>
								<td>Single</td>
							</tr>
							<tr>
								<td>Interaction</td>
								<td>Offline</td>
							</tr>
						</table>
					</div>

					<button class="collapsible">Example: Reinforcement Learning</button>
					<div class="ccontent">
						<table>
							<tr>
								<th>Dimension</th>
								<th>Values</th>
							</tr>
							<tr>
								<td>Modularity</td>
								<td>Flat</td>
							</tr>
							<tr>
								<td>Planning Horizon</td>
								<td>indefinite, infinite</td>
							</tr>
							<tr>
								<td>Representation</td>
								<td>features</td>
							</tr>
							<tr>
								<td>Computational Limits</td>
								<td>Perfect rationality</td>
							</tr>
							<tr>
								<td>Learning</td>
								<td>Knowledge learned</td>
							</tr>
							<tr>
								<td>Sensing Uncertainty</td>
								<td>Fully obervable</td>
							</tr>
							<tr>
								<td>Effect Uncertainty</td>
								<td>Stochastic</td>
							</tr>
							<tr>
								<td>Preference</td>
								<td>complex preferences</td>
							</tr>
							<tr>
								<td>Number of Agents</td>
								<td>Single</td>
							</tr>
							<tr>
								<td>Interaction</td>
								<td>online</td>
							</tr>
						</table>
					</div>

					<button class="collapsible">Example: Humans</button>
					<div class="ccontent">
						<table>
							<tr>
								<th>Dimension</th>
								<th>Values</th>
							</tr>
							<tr>
								<td>Modularity</td>
								<td>heirarchical</td>
							</tr>
							<tr>
								<td>Planning Horizon</td>
								<td>indefinite, infinite</td>
							</tr>
							<tr>
								<td>Representation</td>
								<td>relations</td>
							</tr>
							<tr>
								<td>Computational Limits</td>
								<td>bounded rationality</td>
							</tr>
							<tr>
								<td>Learning</td>
								<td>Knowledge learned</td>
							</tr>
							<tr>
								<td>Sensing Uncertainty</td>
								<td>partially obervable</td>
							</tr>
							<tr>
								<td>Effect Uncertainty</td>
								<td>stochastic</td>
							</tr>
							<tr>
								<td>Preference</td>
								<td>complex preferences</td>
							</tr>
							<tr>
								<td>Number of Agents</td>
								<td>multiple</td>
							</tr>
							<tr>
								<td>Interaction</td>
								<td>online</td>
							</tr>
						</table>
					</div>
				</div>

				<div>
					<p>Complex interaction</p>
				</div>
				<div>
					<p>
						Dimensions intract and combine in complex ways. Partial observability makes multi-agent and indefinite stage more complex, and with modularity you can have fully or partially observcable modules. Heirarchical reasoning, Individuals and relations, and bounded rationality make reasoning simpler.
					</p>
				</div>
			</div>

			<h3 id="rat-2">Representations and Solutions</h3>

			<div class="cornell">
				<div>
					<p>Representations</p>
				</div>
				<div>
					<p>
						<img src="./representations.svg" alt="" style="max-width: 500px; width: 100%;">
					</p>
					<p>
						A task can be represented in a formal way, from which we can make formal deductions and interpret them to get our solution.
					</p>
					<p>
						We want a representation complex enough to solve the problem, to be as close to the problem as possible, but also compact, natural, and maintainable. 
					</p>
					<p>
						We want it to be amenable to computation, thus <br>
						(1) Can express features which can be exploited for computational gain <br>
						(1) Can tradeoff between accuracy and time/space complexity
						
					</p>
					<p>
						And able to acquire data from people, datasets, and past experiences.
					</p>

				</div>

				<div>
					<p>Defining a Solution</p>
				</div>
				<div>
					<p>
						Given informal description of problem, what is the solution? Much is usually unspecified, but they cannot be arbitrarily filled.
					</p>
					<p>
						Much work in AI is for <b>common-sense reasoning</b>, the agent needs to make common sense deductions about unstated assumptions.
					</p>
				</div>

				<div>
					<p>Quality of Solutions</p>
				</div>
				<div>
					<p>
						Solutions can be split into classes of quality
						<ul>
							<li><b>Optimal solution</b>: the best solution (for the quality measure)</li>
							<li><b>Satisficing solution</b>: a good-enough solution (that meets good-enough criteria)</li>
							<li><b>Approximately optimal solution</b>: close but not quite to best in theory</li>
							<li><b>Probable solution</b>: unproved, but likely to be good enough</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Descisions and Outcomes</p>
				</div>
				<div>
					<p>
						Good descisions can have bad outcomes (and vice versa). Information can lead to better descisions, known as <b>value of information</b>.
					</p>
					<p>
						We can often tradeoff computation time and complexity: an <b>anytime algorithm</b> can provide a solution at any time, but more time can lead to better answer.
					</p>
					<p>
						An agent is also concerned with acquiring appropriate information, timely computation, besides finding the right answer.
					</p>
					<p>
						We can often use a quality over time graph with a time discount to represent this.
					</p>
					<img src="./time-discount.svg" alt="An increasing time discount is applied to measure the solution quality for the time taken" style="max-width: 500px; width: 100%;">
				</div>

				<div>
					<p>Choosing a Representation</p>
				</div>
				<div>
					<p>
						Problem \(\longrightarrow\) specificiation of problem \(\longrightarrow\) appropriate computation.
					</p>
					<p>
						We need to choose a representation, this can be done as a high level formal/natural specificaiton, down to programming languages, assembly, etc, etc. 
					</p>
					
				</div>
				<div>
					<p>Physical Symbol Hypothesis</p>
				</div>
				<div>
					<p>
						We define a <b>Symbol</b> as a meaninigful physical pattern that can be manipulated (like logic symbols)
					</p>
					<p>
						A <b>Symbol system</b> creates, copies, modifies, and deletes symbols.
					</p>
					<p>
						The <b>Physical symbol hypothesis</b> states that a physical symb. system is necessary and sufficient for general intelligent action
					</p>
				</div>

				<div>
					<p>Knowledge and Symbol Levels</p>
				</div>
				<div>
					<p>
						Two levels of abstraction common amongst systems:
						<ul>
							<li>
								The <b>knowledge level</b>, what an agent knows, its goals.
							</li>
							<li>
								The <b>symbol level</b>, a description of agent in terms of its reasoning (internal symbolic reasoning).
							</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Mapping problem to representation</p>
				</div>
				<div>
					<p>
						We have a few questions to ask
						<ul>
							<li>What level of abstraction to use?</li>
							<li>What individuals and relations to represent?</li>
							<li>How can knowledge be represented s.t. natural, modular, and maintainable?</li>
							<li>How can agent acquire information from data, sensing, experience, etc?</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Choosing Level of Abstraction</p>
				</div>
				<div>
					<p>
						A high level description is easier for human understanding. A lower level abstraction however can be more accurate, predictive, however are harder to reason, specify, understand. One may not know all details for a low level description. Many cases lead to multipe levels of abstraction.
					</p>
				</div>

				<div>
					<p>Reasoning and Acting</p>
				</div>
				<div>
					<p>
						Reasoning is computation required to determine what action to do.
						<ul>
							<li>
								<b>Design time</b> reasoning is where computation is done by designer beforehand.
								<b>Offline</b> and <b>Online</b>, similar to the complex dimension, is either deciding before starting to act (using knowledge from the <i>knowledge base</i>) or deciding between receiving information and acting respectively. 
							</li>
						</ul>
					</p>
				</div>
			</div>
		</div>

		<div class="colourband">
			<h2 id="uninformedsearch">Uninformed Search</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#usea-1">Problem Solving</a></li>
				<li><a href="#usea-2">Tree Searching</a></li>
				<li><a href="#usea-3">Graph Searching</a></li>
			</ol>

			<h3 id="usea-1">Problem Solving</h3>	


			<div class="cornell">
				<div>
					<p>Problem Solving</p>
				</div>
				<div>
					<p>
						Often we are only given the specification for a solution, and have to search for it. A typical situation is where we have an agent in one state, and have a set of deterministic actions with which it will go towards the goal state. 
					</p>
					<p>
						Often this can be abstrated into finding the goal state in a directed graph of linked states. 
					</p>
				</div>

				<div>
					<p>Example</p>
				</div>
				<div>
					<p>
						Suppose you're in Coventry and want to get to London. How do you achieve this? <br>Using what transport do you get there? What routes do you go via? More importantly, which routes are <i>reasonable</i>?
					</p>
					<p>
						Finding the actions that would get you from Coventry to London is problem solving.
					</p>
					<p>
						A <b>problem-solving agent</b> is a <i>goal-based</i> agent that will determine sequences of actions leading to desirable states. It must in this process determine which states are <i>reasonable</i> to go through.
					</p>
				</div>

				<div>
					<p>Problem solving steps</p>
				</div>
				<div>
					<p>
						There are 4 steps：
						<ul>
							<li>
								<b>Goal formulation</b>: identify goal given current situation.
							</li>
							<li>
								<b>Problem formulation</b>: Identify permissible actions <i>(operators)</i> and states to consider
							</li>
							<li><b>Search</b>: Find the sequence of actions to achieve a goal</li>
							<li><b>Execution</b>: Perform the actions in the solution</li>
						</ul>
						The two types of problem solving mentioned before are <b>offline</b> and <b>online</b>, the former acting with <i>complete knowledge</i> and the latter with <i>incomplete knowledge</i>. 
					</p>

					<p>
						For our example from earlier:
						<ul class="not">
							<li>Formulate goal: be in London</li>
							<li>Formulate problem:
								<ul class="not">
									<li><i>states</i>: (in which) towns and cities</li>
									<li><i>operators</i>: drive between towns and cities</li>
								</ul>
							</li>
							<li>Search: sequence of towns and cities</li>
							<li>Execution: drive</li>
						</ul>
					</p>
				</div>

				

				<div>
					<p>Pseudocode agent</p>
					
				</div>
				<div>
					<p>
						We can make a mockup of an offline reasoning agent in pseudocode. 
					</p>
					<div class="codediv">global Action[] s;  	 # action sequence, initially empty
						
global State state;  	 # current world state
global Goal g;           # goal, initially null
global Problem problem;  # problem to solve

function simpleProblemSolvingAgent(p: Percept) -&gt; Action:
	state &lt;- updateState(state, p)
	if s.empty():
		g &lt;- formulateGoal(state)
		problem &lt;- formulateProblem(state, p)
		s &lt;- search(problem)
	action &lt;- recommendation(state)
	s &lt;- result(state)
	return action</div>

					<p>
						Naturally all the functions mentioned are left as an excersise to the reader (i.e. assumed implemented). 
						<code>formulateGoal</code> internally will use its current perceptions.
					</p>
				</div>

				<div>
					<p>Problem types</p>
				</div>
				<div>
					<p>
						Problems of course can come in many types.
						<ul>
							<li>
								<b>Single state problem</b>: deterministic and fully observable. Sensors tell agent current state, it knows exactly what to do, can work out what state it will be in after.
							</li>
							<li>
								<b>Multiple state problem</b>: deterministic and <i>partially</i> observable. Agent knows its actions, but has limited access to state. There may be a set of states the agent would end up in, agent must manipulate sets.
							</li>
							<li>
								<b>Contingency problem</b>: <i>stochastic</i> and partially observable. Agent does not know current state so is unsure of result of action. Must use sensors during execution. Solution is a tree with contingency branches. Search and execution is often <i>interleaved</i>. 
							</li>
							<li>
								<b>Exploration problem</b>: unknown state space. Agent does not know what it's actions will do, and has to explore and learn. This is an <b>online</b> problem.
							</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Vacuum Cleaner World</p>
				</div>
				<div>
					<p>
						Let's use an example of a simple environment, a vacuum robot <code>V</code> and a world with two locations, <code>[  ][  ]</code>. Each location may also contain dirt (<code>[ d]</code> or <code>[Vd]</code> for vacuum and dirt). The vacuum can move left or right, and suck dirt.
					</p>

					<p>
						<b><i>Single state.</i></b> Suppose we know we are in the state <code>[V][d]</code>. We then know our sequence of actions is just [right, suck]. 
					</p>
					<p>
						<b><i>Multiple State.</i></b> (sensorless / conformant) Suppose we don't know which state we start in. We could have the states:
						<table>
							<tr>
								<td>1. <code>[Vd][ d]</code></td>
								<td>2. <code>[ d][Vd]</code></td>
								<td>3. <code>[Vd][  ]</code></td>
								<td>4. <code>[ d][V ]</code></td>
							</tr>
							<tr>
								<td>5. <code>[V ][ d]</code></td>
								<td>6. <code>[  ][Vd]</code></td>
								<td>7. <code>[V ][  ]</code></td>
								<td>8. <code>[  ][V ]</code></td>
							</tr>
						</table> 

						If we start in any one of those states, and we move to the right, we will always end up in states {2, 3, 6, 8}. If we move left afterwards, we will guarantee vacuum ends up on the left. Thus we can do [right, suck, left, suck]. 
					</p>

					<p>
						<b><i>Contingency.</i></b> Suppose we start in state <code>[V ][ d]</code>, but our vacuum has received a downgrade: if we suck a clean square, there is a chance that it may make that square <i>less</i> clean. So what do we do? 
					</p>
					<p>
						What we can is sense whilst acting, thus: [right, <b>if</b> dirt <b>then</b> suck].
					</p>
				</div>

				<div>
					<p>State space problems</p>
				</div>
				<div>
					<p>
						A <b>State space problem</b> consists of a set of states; subsets of <b>start states, goal states</b>; a set of actions; an <b>action function</b> (f(state, action) -> new state); and a criterion that specifies the <i>quality</i> of an acceptable solution (depends on problem).
					</p>
					<p>
						Since the real world is complex, state space may require <i>abstraction</i> for problem solving. States, operators, and solutions approximate the real world equivalent, though one must take care that their operations are still valid for the real world counterpart.
					</p>
				</div>

				<div>
					<p>8-puzzle</p>
				</div>
				<div>
					<p>
						A 3\(\times\)3 grid of tiles numbered 1 to 8, with a blank space for sliding. We want to get from some initial state to some goal state. 

						<pre><code>283   123
164   8_4
7_5   765</code></pre>

						On the left is an initial state, and on the right is the goal state.
					</p>
					<ul>
						<li><i>States</i> can be coordinate locations of tiles</li>
						<li><i>Operators</i> - we want the fewest possible - are moving the <i>blank square</i> up, down, left, or right</li>
						<li>Our <i>goal test</i> would be the given goal state</li>
						<li>The <i>path cost</i> we can set as 1 per operation</li>
					</ul>
				</div>

				<div>
					<p>State Space Graphs</p>
				</div>
				<div>
					<p>
						A general formultion for a problem is a <b>state space graph</b>. It is:
						<ul>
							<li>A <i>directed</i> graph consisting of a set N of <i>nodes</i>, set A of <i>arcs/edges</i>, which are ordered pairs of nodes.</li>
							<li>Node \(n_2\) <i>neighbours</i> node \(n_1\) if there is an arc \(\langle n_1, n_2 \rangle\) (denoted with triangle brackets)</li>
							<li>A <i>path</i> is a sequence of nodes \(\langle n_0, n_1, ..., n_k\rangle\) s.t. \(\langle n_{i-1}, n_i \rangle \in A\) (arcs between all nodes)</li>
							<li>The <i>length</i> of a path from \(n_0\) to \(n_k\) is \(k\).</li>
							<li>Given a set of <i>start nodes</i> and <i>goal nodes</i>, a <b>solution</b> is a path from a start node to a goal node.</li>
						</ul>
					</p>
				</div>
			</div>

			<h3 id="usea-2">Tree Searching</h3>

			<div class="cornell">
				<div>
					<p>Tree searching algorithms</p>
				</div>
				<div>
					<p>
						A tree search is a basic approach to problem solving. It is an <i>offline</i>, simulated exploration of the state space. Starting with a start state, we <i>expand</i> an explored state by generating / exploring its successor branches, to build a search tree. 
					</p>
					<p>
						We have a single start state, and repeatedly expand until we find a solution, or run out of valid candidates. 
					</p>
					<button class="collapsible">Example tree of 8 puzzle</button>
					<div class="ccontent">
						<img src="./8puzzle-search.png" alt="Some branches are omitted for simplicity." style="max-width: 400px;">
					</div>
				</div>

				<div>
					<p>Implementation details</p>
				</div>
				<div>
					<p>
						A <b>state</b> represents a physical configuration of a puzzle, and a <b>node</b> will be the data structure for one state of the tree.
					</p>
					<p>
						A node stores the <b>state, parent, children, depth, path cost</b> - the last of these is usually denoted \(g\).
					</p>
					<img src="./tree-node.svg" alt="A typical tree node" style="max-width: 400px;">
					<p>
						Nodes (explored) but not yet expanded are called the <b>frontier</b>, and is generally represented as a queue.
					</p>
				</div>

				<div>
					<p>Search strategies</p>
				</div>
				<div>
					<p>
						The strategy of searching is defined by which order we expand nodes in. These strategies are evaluated over several criteria:
						<ul>
							<li><b>completeness</b>: will the strategy always find a solution, if it exists?</li>
							<li><b>optimality</b>: will the best (least cost) solution be found?</li>
							<li><b>time complexity</b>: The number of nodes expanded - this depends on the maximum <i>branching factor</i> \(b\), depth of best solution \(d\), and the maximum depth of the state space \(m\), which may be \(\infty\).</li>
							<li><b>space complexity</b>, the maximum number of nodes stored in memory (also in terms of \(b, d, m\)) </li>
						</ul>
					</p>
				</div>

				<div>
					<p>Simple, uninformed strategies</p>
				</div>
				<div>
					<p>
						<b>Uninformed</b> search only uses information given in problem definiton - we have no measure of which nodes would be better to expand to, nor do we really check the nodes once we've explored them (i.e. not checking cycles).
					</p>
					<p>
						Two of these algorithms are Breadth-first search BFS and Depth-first search DFS.
					</p>
				</div>

				<div>
					<p>Breadth first</p>
				</div>
				<div>
					<p>
						BFS expands the shallowest node: put recently expanded successors at the end of queue: FIFO.
					</p>
					<ul>
						<li>
							<b>Completeness</b>. BFS is complete because it will always find a solution, provided \(b\) finite.
						</li>
						<li>
							<b>Time</b>. Since at every node we expand \(b\) branches, in total for a depth of \(d\) we will be expanding \(1 + b + b^2 + b^3 + ... + b^d = O(b^d)\) times. This becomes \(O(b^{d+1})\) if the check for goal state is done when that node is about to be expanded, instead of as part of expansion.
						</li>
						<li>
							<b>Space</b>. Each leaf node is kept in memory, thus there are \(O(b^{d-1})\) explored nodes and \(O(b^{d})\) in the frontier, and overall space is dominated by frontier, \(O(b^d)\).
						</li>
						<li>
							<b>Optimality</b>. BFS is optimal <i>only if</i> the cost function does not decrease. 
						</li>
					</ul>
					<p>
						The main problem for BFS is the space usage, which can balloon very very quickly (with \(b = 10\) and 1 KB per node, at depth 8 one needs around 1 TB of memory).
					</p>
				</div>

				<div>
					<p>Depth first</p>
				</div>
				<div>
					<p>
						DFS expands the deepest node: put most recently expanded successors at the start of the "queue" (stack): LIFO. 
					</p>
					<ul>
						<li>
							<b>Completeness.</b> DFS is <i>not</i> complete because it will fail in infinite depth and loop situations. If we avoid repeated paths and have or limit to a finite depth, then DFS <i>is</i> complete.
						</li>
						<li>
							<b>Time.</b> \(O(b^m)\), which is O(horrible) if \(m > d\) significantly. Otherwise may be significantly faster than BFS.
						</li>
						<li>
							<b>Space.</b> \(O(bm)\), i.e. linear space: store path from root to leaf and unexpanded siblings of nodes on path.
						</li>
						<li>
							<b>Optimality.</b> Not guaranteed.
						</li>
					</ul>
				</div>
			</div>

			<h3 id="usea-3">Graph Searching</h3>

			<div class="cornell">
				<div>
					<p>Notes</p>
				</div>
				<div>
					<p>
						This section is <i>uninformed</i> search, and graph searches are similarly uninformed, so much so that the algorithms presented do not even check for cycles, despite common intuition.
					</p>

					<p>
						The main difference is that in graph search we look at <i>paths</i>, rather than nodes. Expanding into a node would be adding it to the stored <i>path</i>.
					</p>
				</div>
	
				<div>
					<p>Graphs</p>
				</div>
				<div>
					<p>
						With trees, if we have looped states these will just branch out to infinity. Graphs are a good way to take loops into account and are a practical representation of such spaces.
					</p>
					<p>
						Similar to a tree, we incrementally explore, maintaining a frontier of explored paths. The frontier should expand into unexplored nodes until a goal node is encountered. 
					</p>
					<p>
						The way nodes are selected is the <i>search strategy</i>, the <i>neighbours</i> of (all) nodes define a graph, and the <i>goal</i> defines the solution.
					</p>
					<p>
						We can continue searching from the return call even if a solution has been found, if we want more or all possible solutions.
					</p>
				</div>

				<div>
					<p>BFS on Graphs</p>
				</div>
				<div>
					<p>
						All paths on the frontier \([p_1 .. p_r]\) are expanded one by one, and the neighbours of an explored path are added to the <i>end</i> of the frontier. 
					</p>
					<p>
						The time complexity is instead written \(O(b^n)\), where \(n\) is the <i>path length</i> (instead of tree depth).
					</p>
				</div>

				<div>
					<p>DFS on Graphs</p>
				</div>
				<div>
					<p>
						DFS treats the frontier as a stack instead. The time complexity is \(O(b^k)\) where \(k\) is the maximum number of arcs a path can have (i.e. max depth). Space complexity is linear: \(O(k + (b-1))\).
					</p>
					<p>
						Now, DFS is <i>said</i> to fail on both infinitely long graphs and graphs with cycles, although I'm not sure why you're adding already explored nodes to the frontier (cycle case). 
					</p>
				</div>

				<div>
					<p>Lowest cost first</p>
				</div>
				<div>
					<p>
						Sometimes, there are <i>costs</i>, or weights, associated with edges. The cost of a path is thus the sum of the cost of edges, and an optimal solution has the path with the smallest total cost. 
					</p>
					<p>
						Lowest cost first is a fairly naive algorithm that simply selects the path on the frontier with the lowest cost. I.e., frontier paths are stored in a <i>priority queue</i> according to path length.
					</p>
					<p>
						Lowest cost is <b>complete</b>.
					</p>
				</div>
			</div>

			<p>
				Supposedly <i>none</i> of these algorithms will halt on a finite graph with cycles, but one has to be a particular type of person to program a graph search without cycle checking <i>/ not adding explored nodes to the unexplored frontier</i>.
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="informed">Informed Search</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol></ol>

			<div class="cornell">
				<div>
					<p>Overview</p>
				</div>
				<div>
					<p>
						Uninformed search is inefficient, especially when we have extra information we can exploit. By using <i>problem-specific</i> knowledge, we can improve our searching - this is <b>informed search</b>.
					</p>
					<p>
						Still offline.
					</p>
				</div>
			</div>

			<h3 id="inf-1">Heuristic Search</h3>

			<div class="cornell">
				<div>
					<p>Overview</p>
				</div>
				<div>
					<p>
						If you know where the goal is, when selecting paths do not ignore it. 
					</p>
					<p>
						Employ the use of an extra <b>heuristic</b> measurement \(h(n)\) - an <i>estimate</i> of the remaining cost to the goal. 
					</p>
					<p>
						This heuristic should be <b>efficient</b> to compute, and always be an <b>underestimate</b> (no path to goal strictly less than \(h(n)\)). If a heuristic meets these criteria, then it is <b>admissible</b>.
					</p>
				</div>
				<div>
					<p>Examples</p>
				</div>
				<div>
					<p>
						If one is on a euclidean plane finding the shortest path, \(h(n)\) can be the straight-line distance to the goal. If the cost is time, \(h(n)\) can be straight-line distance divided by max speed. 
					</p>
					<p>
						We can think of this as reducing the constraints in the problem to an easier representation that we can quickly calculate an answer for. The straight-line distance does not count obstacles, for example. 
					</p>	
				</div>

				<div>
					<p>Best First Search</p>
				</div>
				<div>
					<p>
						When we choose nodes to expand in a frontier, the heuristic can determine the <i>order</i> / rank of the frontier structure. 
					</p>
					<p>
						<b>Heuristic DFS</b> is where we select a neighbour such that the <i>best neighbour</i> is selected first. 
					</p>
					<p>
						If by best we mean closest (by some metric) distance to the goal, then we have <b>Greedy Best First Search</b> (BFS).
					</p>
					<p>
						The frontier is a priority queue ordered by \(h(n)\).
					</p>
				</div>

				<div>
					<p>Problems</p>
				</div>
				<div>
					<p>
						Some graphs are very much not good for BFS. Especially if we do not track cycles.

						<figure>
							<img src="./bad-best-first.svg" alt="A bad graph for best-first search" >
							<figcaption><i>A bad graph for best first search - it will loop forever because the bottom path is closer as the crow flies.</i></figcaption>
						</figure>

						
					</p>
				</div>
			</div>
		</div>

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="../../js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
</body>
</html>