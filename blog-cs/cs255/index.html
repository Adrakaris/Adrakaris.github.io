<!DOCTYPE html>
<html>
<head>
	<title>CS255</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="../../" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="../../blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="../../about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS255</h1>
                    <p class="subheading">Artificial Intelligence</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Introduction</h1>
			</div>
		</header>

		<div class="cbox">
			<h3>Contents</h3>

			<ol>
				<li><a href="#rational-agents">Rational Agents</a></li>
				<li><a href="#uninformedsearch">Uninformed Search</a></li>
				<li><a href="#informed">Informed Search</a></li>
				<li><a href="#csp">Constraint Satisfaction Problems</a></li>
				<li><a href="#localsearch">Local Search</a></li>
				<li><a href="#adversarialsearch">Adversarial Search</a></li>
				<li><a href="#uncertainty">Planning with Uncertainty</a></li>
			</ol>

			<h3>Introduction to Artificial Intelligence</h3>

			<p>
				A defintion of artificial intelligence can fall under 4 categories:
			</p>

			<table >
				<tr>
					<td>Thinking humanly</td>
					<td>Thinking rationally</td>
				</tr>
				<tr>
					<td>Thinking rationally&emsp13;</td>
					<td>Acting rationally</td>
				</tr>
			</table>

			<button class="collapsible">Categories...</button>
			<div class="ccontent">
				<div class="cornell">

					<div>
						<p>Acting Humanly</p>
					</div>
					<div>
						<p>
							The <i>Turing Test</i> is a well known test for how well an AI performs. It suggests that if it is acting intelligently, it is thinking intelligently. 
						</p>
						<p>
							It also suggests that an AI requires <b>knowledge</b>, <b>reasoning</b>, <b>learning</b> (and language).
						</p>
						<p>
							However the general consensus is that the Turing Test is not great, as many counterexamples and exploits can be found for it. 
						</p>
						<p>
							One such "counterexample" is <i><a href="https://plato.stanford.edu/entries/chinese-room/" class="text">Searle's Chinese Room</a></i>. 
						</p>
					</div>
	
					<div>
						<p>Thinking Humanly</p>
					</div>
					<div>
						<p>
							There is a field of study called cognitive modelling, which is modelling human thinking in programs. (We will however ignore this)
						</p>
					</div>
	
					<div>
						<p>Thinking rationally</p>
					</div>
					<div>
						<p>Given a premise/some premises, an agent (the AI model) must derive or deduce the solution via logic. Rational deduction however has problems dealing with uncertainty.</p>
					</div>
	
					<div>
						<p>Acting Rationally</p>
					</div>
					<div>
						<p>
							Acting rationally can be summed up as "doing the right thing", which is to say, the action that <i>maximises</i> some achievement metric. However, thought or inference in these actions is not necessarily involved.
						</p>
	
					</div>
				</div>	
			</div>

			
			<div class="cornell">
				<div>
					<p></p>
				</div>
				<div>
					<p>
						In actuality, the boundaries between these four categories are quite blurred.
					</p>
				</div>
				<div>
					<p>Specialised AI vs AGI</p>
				</div>
				<div>
					The AI type that is employed everywhere is called <i>specialised</i> AI, AI built for a specific task. There is also the concept of Artificial General Intelligence (AGI), which is what sci-fi AI usually describes. 
				</div>
				<div>
					<p>Rational Agents</p>
				</div>
				<div>
					<p>Specifically, we look at <b>Rational Agents</b>.</p>
					<p>
						These agents can be modelled as a function \(f:P \longrightarrow A\), where P denotes <i>percept histories</i> and A denotes <i>actions</i>.
					</p>
					<p>
						Agents are typically required to exhibit autonomy, and seek the best performance possible. (Perfect rationality may not always be the best as there is a trade off with computing resources)
					</p>
				</div>
			</div>

			
		</div>


		<div class="colourband">
			<h2 id="rational-agents">Rational Agents</h2>

		</div>

		<div class="cbox">
			<h3>Introduction</h3>
			
			<ol>
				<li><a href="#rat-1">Dimensions of Complexity</a></li>
				<li><a href="#rat-2">Representations and Solutions</a></li>
			</ol>

			<div class="cornell">
				<div>
					<p>Agent Input Model</p>
				</div>
				<div>
					<img src="./agent-input.svg" alt="An agent takes as inputs its Abilities, Goals, Prior Knowledge, Stimuli and Past Experiences from the environment, and acts on the environment.">
					<p>
						Above shows a diagram of what inputs an agent takes, and what it does.
					</p>

					<p style="display: none;">
						Think of a self-driving car. What attributes would fit for each label? For abilites, we could have steer, break, accelerate, ...; for goals getting to destination, being safe; prior knowledge could be signs, road speeds, laws, ...; stimuli vision, GPS; and for prior experience internal streetmaps, etc.
					</p>
				</div>

				<div>
					<p>Rational Agents</p>
				</div>
				<div>
					<p>The Goals of a rational agent can be specified by a <b>performance measure</b>, which is a numerical value.</p>
					<p>A <b>rational action</b> is an action which aims to <i>maximise</i> the performance measure, given the percept history to date. </p>

					<p>Note that an agent is <b><span class="sc">not</span></b> onmiscient, clairvoyant <i>(forseeing changes)</i>, nor always successful. The <i>expected</i> value, not the actual value, is considered - an action to maximise expected value which fails is <i>still</i> rational.</p> 
				</div>
			</div>

			<h3 id="rat-1">Dimensions of Complexity</h3>

			<div class="cornell">
				<div>
					<p>Dimensions of Complexity</p>
				</div>
				<div>
					<p>
						The design space / considerations of an AI can be defined by a set of "dimensions of complexity".
					</p>

					<table>
						<tr>
							<th>Dimension</th>
							<th>Values</th>
						</tr>
						<tr>
							<td>Modularity</td>
							<td>Flat, modular, heirarchical</td>
						</tr>
						<tr>
							<td>Planning Horizon</td>
							<td>Static/none, finite (stage), indefinite, infinite</td>
						</tr>
						<tr>
							<td>Representation</td>
							<td>Flat state, features, relations</td>
						</tr>
						<tr>
							<td>Computational Limits</td>
							<td>Perfect or bounded rationality</td>
						</tr>
						<tr>
							<td>Learning</td>
							<td>Knowledge given or learned</td>
						</tr>
						<tr>
							<td>Sensing Uncertainty</td>
							<td>Fully or partially obervable</td>
						</tr>
						<tr>
							<td>Effect Uncertainty</td>
							<td>Deterministic, stochastic</td>
						</tr>
						<tr>
							<td>Preference</td>
							<td>Goals, complex preferences</td>
						</tr>
						<tr>
							<td>Number of Agents</td>
							<td>Single, multiple</td>
						</tr>
						<tr>
							<td>Interaction</td>
							<td>Offline, online</td>
						</tr>
					</table>
					<hr>
				</div>
			</div>

			
			<div class="cornell">
				
				<div>
					<p>Modularity</p>
				</div>
				<div>
					<p>
						<b>Modularity</b> concerns the structure and abstraction.
						<ul>
							<li><b>Flat</b> where the agent only has 1 level of abstraction</li>
							<li><b>Modular</b> where the agent has interacting modules that can be separately understood</li>
							<li><b>Hierarchical</b> where the agent has modules that (recursively) decompose into others</li>
						</ul>
						Flat representations are good for simple systems, whereas complex systems will be heirarchical. The former can be continuous or discrete, whilst the latter is generally a hybrid.
					</p>
				</div>

				<div>
					<p>Planning Horizon</p>
				</div>
				<div>
					<p>
						<b>Planning Horizon</b> concerns how the world is, and how far forward does the agent plan to.
						<ul>
							<li><b>Static</b>, where the world does not change</li>
							<li><b>Finite Stage</b>, where the agent reasons for a fixed number of timesteps</li>
							<li><b>Indefinite State</b>, agent reasons about a finite, not predetermined number of steps</li>
							<li><b>Infinite Stage</b>, agent plans for going forever (process oriented)</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Representation</p>
				</div>
				<div>
					<p>
						Finding compact <b>Representations</b> and exploiting them for computational gains. An AI can reason in
						<ul>
							<li><b>Explicit States</b>, where a state describes everything about a world</li>
							<li><b>Features/Propositions</b>, which are descriptors of states. A few features can lead to a <i>lot</i> of states.</li>
							<li><b>Individuals</b> and their <b>relations</b>, a feature for each relationship on each tuple of individuals. Thus an agent can reason without knowing the individuals or if there are infinite individuals.</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Computational Limits</p>
				</div>
				<div>
					<p>
						<b>Computational Limits</b>:
						<ul>
							<li><b>Perfect rationality</b> is ideal, but does not take into account computing resources</li>
							<li><b>Bounded rationality</b> is what we hope to achieve, good descisions made accounting for perceptual and computing limitations</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Learning from Experience</p>
				</div>
				<div>
					<p>
						<b>Learning from Experience</b>:
						<ul>
							<li>Where knowledge is <b>given</b></li>
							<li>Or knowledge is <b>learned</b> from data or past experience</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Uncertainty</p>
				</div>
				<div>
					<p>
						<b>Uncertainty</b> has two dimensions: <i>sensing</i> and <i>effect</i>. In each, an agent can have:
						<ul>
							<li>
								<b>No</b> (uncertainty), agent knows what is true
							</li>
							<li>
								<b>Disjunctive</b>, a set of states are possible
							</li>
							<li>
								<b>Probabilistic</b>, there is a probability distribution between states
							</li>
						</ul>

						(On probability) Agents need to act even if they are uncertain, thus predictions are needed to decide what to do, which can be <i>definitive</i> (this <span class="sc">will</span> happen), <i>disjunctions</i> (do event A <span class="sc">else</span> B will happen), or <i>point probabilities</i> (Probability of the outcome is 0.01 if you do A or 0.8 if otherwise).
					</p>
				</div>

				<div>
					<p>Sensing Uncertainty</p>
				</div>
				<div>
					<p>
						<b>Sensing Uncertainty</b> is about what the agent can sense about state, the state can be
						<ul>
							<li><b>Fully observable</b></li>
							<li><b>Partially observable</b> (possible number of states from obs.)</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Effect Uncertainty</p>
				</div>
				<div>
					<p>
						<b>Effect Uncertainty</b> concerns whether an agent could predict an outcome from a state and an action.
						<ul>
							<li><b>Deterministic</b> environments say yes</li>
							<li><b>Stochastic</b> environments say no (have uncertainty).</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Preferences</p>
				</div>
				<div>
					<p>
						<b>Preferences</b> concerns what an agent tries to achieve.
						<ul>
							<li>
								<b>Achievement goal</b> is a single goal to achieve - can be a (complex) logical formula
							</li>
							<li>
								<b>Complex preferences</b> involve tradeoffs between various desires, with priorities. These are further split into <i>Ordinal</i> (order priority matters) and <i>Cardinal</i> (specific absolute values also matter).
							</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Number of Agents</p>
				</div>
				<div>
					<p>
						Crucially, <b>Number of <i>reasoning</i> agents</b>.
						<ul>
							<li>
								<b>Single Agent</b> reasoning relegates every other agent as environment and not really considered, whilst
							</li>
							<li>
								<b>Multiple Agent</b> reasoning involves an agent reasoning strategically about other agents.
							</li>
						</ul>
						Agents have their own goals, which can be competetive, cooperative, or independent.
					</p>
				</div>

				<div>
					<p>Interaction</p>
				</div>
				<div>
					<p>
						<b>Interaction</b> talks about when the agent reasons relating to its action
						<ul>
							<li><b>Reason offline</b>, before acting</li>
							<li><b>Reason online</b>, whilst interacting with environment</li>
						</ul>
					</p>
					<hr>
				</div>
				
			</div>

			<div class="cornell">
				<div>
					<p>Examples</p>
				</div>
				<div>
					<p>
						Some examples of the dimensions of different agents
					</p>
					<button class="collapsible"  id="sss">Example: State-space search</button>
					<div class="ccontent">
						<table>
							<tr>
								<th>Dimension</th>
								<th>Values</th>
							</tr>
							<tr>
								<td>Modularity</td>
								<td>Flat</td>
							</tr>
							<tr>
								<td>Planning Horizon</td>
								<td>indefinite</td>
							</tr>
							<tr>
								<td>Representation</td>
								<td>Flat state</td>
							</tr>
							<tr>
								<td>Computational Limits</td>
								<td>Perfect rationality</td>
							</tr>
							<tr>
								<td>Learning</td>
								<td>Knowledge given</td>
							</tr>
							<tr>
								<td>Sensing Uncertainty</td>
								<td>Fully obervable</td>
							</tr>
							<tr>
								<td>Effect Uncertainty</td>
								<td>Deterministic</td>
							</tr>
							<tr>
								<td>Preference</td>
								<td>Goals</td>
							</tr>
							<tr>
								<td>Number of Agents</td>
								<td>Single</td>
							</tr>
							<tr>
								<td>Interaction</td>
								<td>Offline</td>
							</tr>
						</table>
					</div>

					<button class="collapsible">Example: Reinforcement Learning</button>
					<div class="ccontent">
						<table>
							<tr>
								<th>Dimension</th>
								<th>Values</th>
							</tr>
							<tr>
								<td>Modularity</td>
								<td>Flat</td>
							</tr>
							<tr>
								<td>Planning Horizon</td>
								<td>indefinite, infinite</td>
							</tr>
							<tr>
								<td>Representation</td>
								<td>features</td>
							</tr>
							<tr>
								<td>Computational Limits</td>
								<td>Perfect rationality</td>
							</tr>
							<tr>
								<td>Learning</td>
								<td>Knowledge learned</td>
							</tr>
							<tr>
								<td>Sensing Uncertainty</td>
								<td>Fully obervable</td>
							</tr>
							<tr>
								<td>Effect Uncertainty</td>
								<td>Stochastic</td>
							</tr>
							<tr>
								<td>Preference</td>
								<td>complex preferences</td>
							</tr>
							<tr>
								<td>Number of Agents</td>
								<td>Single</td>
							</tr>
							<tr>
								<td>Interaction</td>
								<td>online</td>
							</tr>
						</table>
					</div>

					<button class="collapsible">Example: Humans</button>
					<div class="ccontent">
						<table>
							<tr>
								<th>Dimension</th>
								<th>Values</th>
							</tr>
							<tr>
								<td>Modularity</td>
								<td>heirarchical</td>
							</tr>
							<tr>
								<td>Planning Horizon</td>
								<td>indefinite, infinite</td>
							</tr>
							<tr>
								<td>Representation</td>
								<td>relations</td>
							</tr>
							<tr>
								<td>Computational Limits</td>
								<td>bounded rationality</td>
							</tr>
							<tr>
								<td>Learning</td>
								<td>Knowledge learned</td>
							</tr>
							<tr>
								<td>Sensing Uncertainty</td>
								<td>partially obervable</td>
							</tr>
							<tr>
								<td>Effect Uncertainty</td>
								<td>stochastic</td>
							</tr>
							<tr>
								<td>Preference</td>
								<td>complex preferences</td>
							</tr>
							<tr>
								<td>Number of Agents</td>
								<td>multiple</td>
							</tr>
							<tr>
								<td>Interaction</td>
								<td>online</td>
							</tr>
						</table>
					</div>
				</div>

				<div>
					<p>Complex interaction</p>
				</div>
				<div>
					<p>
						Dimensions intract and combine in complex ways. Partial observability makes multi-agent and indefinite stage more complex, and with modularity you can have fully or partially observcable modules. Heirarchical reasoning, Individuals and relations, and bounded rationality make reasoning simpler.
					</p>
				</div>
			</div>

			<h3 id="rat-2">Representations and Solutions</h3>

			<div class="cornell">
				<div>
					<p>Representations</p>
				</div>
				<div>
					<p>
						<img src="./representations.svg" alt="" style="max-width: 500px; width: 100%;">
					</p>
					<p>
						A task can be represented in a formal way, from which we can make formal deductions and interpret them to get our solution.
					</p>
					<p>
						We want a representation complex enough to solve the problem, to be as close to the problem as possible, but also compact, natural, and maintainable. 
					</p>
					<p>
						We want it to be amenable to computation, thus <br>
						(1) Can express features which can be exploited for computational gain <br>
						(1) Can tradeoff between accuracy and time/space complexity
						
					</p>
					<p>
						And able to acquire data from people, datasets, and past experiences.
					</p>

				</div>

				<div>
					<p>Defining a Solution</p>
				</div>
				<div>
					<p>
						Given informal description of problem, what is the solution? Much is usually unspecified, but they cannot be arbitrarily filled.
					</p>
					<p>
						Much work in AI is for <b>common-sense reasoning</b>, the agent needs to make common sense deductions about unstated assumptions.
					</p>
				</div>

				<div>
					<p>Quality of Solutions</p>
				</div>
				<div>
					<p>
						Solutions can be split into classes of quality
						<ul>
							<li><b>Optimal solution</b>: the best solution (for the quality measure)</li>
							<li><b>Satisficing solution</b>: a good-enough solution (that meets good-enough criteria)</li>
							<li><b>Approximately optimal solution</b>: close but not quite to best in theory</li>
							<li><b>Probable solution</b>: unproved, but likely to be good enough</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Descisions and Outcomes</p>
				</div>
				<div>
					<p>
						Good descisions can have bad outcomes (and vice versa). Information can lead to better descisions, known as <b>value of information</b>.
					</p>
					<p>
						We can often tradeoff computation time and complexity: an <b>anytime algorithm</b> can provide a solution at any time, but more time can lead to better answer.
					</p>
					<p>
						An agent is also concerned with acquiring appropriate information, timely computation, besides finding the right answer.
					</p>
					<p>
						We can often use a quality over time graph with a time discount to represent this.
					</p>
					<img src="./time-discount.svg" alt="An increasing time discount is applied to measure the solution quality for the time taken" style="max-width: 500px; width: 100%;">
				</div>

				<div>
					<p>Choosing a Representation</p>
				</div>
				<div>
					<p>
						Problem \(\longrightarrow\) specificiation of problem \(\longrightarrow\) appropriate computation.
					</p>
					<p>
						We need to choose a representation, this can be done as a high level formal/natural specificaiton, down to programming languages, assembly, etc, etc. 
					</p>
					
				</div>
				<div>
					<p>Physical Symbol Hypothesis</p>
				</div>
				<div>
					<p>
						We define a <b>Symbol</b> as a meaninigful physical pattern that can be manipulated (like logic symbols)
					</p>
					<p>
						A <b>Symbol system</b> creates, copies, modifies, and deletes symbols.
					</p>
					<p>
						The <b>Physical symbol hypothesis</b> states that a physical symb. system is necessary and sufficient for general intelligent action
					</p>
				</div>

				<div>
					<p>Knowledge and Symbol Levels</p>
				</div>
				<div>
					<p>
						Two levels of abstraction common amongst systems:
						<ul>
							<li>
								The <b>knowledge level</b>, what an agent knows, its goals.
							</li>
							<li>
								The <b>symbol level</b>, a description of agent in terms of its reasoning (internal symbolic reasoning).
							</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Mapping problem to representation</p>
				</div>
				<div>
					<p>
						We have a few questions to ask
						<ul>
							<li>What level of abstraction to use?</li>
							<li>What individuals and relations to represent?</li>
							<li>How can knowledge be represented s.t. natural, modular, and maintainable?</li>
							<li>How can agent acquire information from data, sensing, experience, etc?</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Choosing Level of Abstraction</p>
				</div>
				<div>
					<p>
						A high level description is easier for human understanding. A lower level abstraction however can be more accurate, predictive, however are harder to reason, specify, understand. One may not know all details for a low level description. Many cases lead to multipe levels of abstraction.
					</p>
				</div>

				<div>
					<p>Reasoning and Acting</p>
				</div>
				<div>
					<p>
						Reasoning is computation required to determine what action to do.
						<ul>
							<li>
								<b>Design time</b> reasoning is where computation is done by designer beforehand.
								<b>Offline</b> and <b>Online</b>, similar to the complex dimension, is either deciding before starting to act (using knowledge from the <i>knowledge base</i>) or deciding between receiving information and acting respectively. 
							</li>
						</ul>
					</p>
				</div>
			</div>
		</div>

		<div class="colourband">
			<h2 id="uninformedsearch">Uninformed Search</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#usea-1">Problem Solving</a></li>
				<li><a href="#usea-2">Tree Searching</a></li>
				<li><a href="#usea-3">Graph Searching</a></li>
			</ol>

			<h3 id="usea-1">Problem Solving</h3>	


			<div class="cornell">
				<div>
					<p>Problem Solving</p>
				</div>
				<div>
					<p>
						Often we are only given the specification for a solution, and have to search for it. A typical situation is where we have an agent in one state, and have a set of deterministic actions with which it will go towards the goal state. 
					</p>
					<p>
						Often this can be abstrated into finding the goal state in a directed graph of linked states. 
					</p>
				</div>

				<div>
					<p>Example</p>
				</div>
				<div class="side">
					<p>
						Suppose you're in Coventry and want to get to London. How do you achieve this? <br>Using what transport do you get there? What routes do you go via? More importantly, which routes are <i>reasonable</i>?
					</p>
					<p>
						Finding the actions that would get you from Coventry to London is problem solving.
					</p>
					<p>
						A <b>problem-solving agent</b> is a <i>goal-based</i> agent that will determine sequences of actions leading to desirable states. It must in this process determine which states are <i>reasonable</i> to go through.
					</p>
				</div>

				<div>
					<p>Problem solving steps</p>
				</div>
				<div>
					<p>
						There are 4 steps：
						<ul>
							<li>
								<b>Goal formulation</b>: identify goal given current situation.
							</li>
							<li>
								<b>Problem formulation</b>: Identify permissible actions <i>(operators)</i> and states to consider
							</li>
							<li><b>Search</b>: Find the sequence of actions to achieve a goal</li>
							<li><b>Execution</b>: Perform the actions in the solution</li>
						</ul>
						The two types of problem solving mentioned before are <b>offline</b> and <b>online</b>, the former acting with <i>complete knowledge</i> and the latter with <i>incomplete knowledge</i>. 
					</p>

					<p>
						For our example from earlier:
						<ul class="not">
							<li>Formulate goal: be in London</li>
							<li>Formulate problem:
								<ul class="not">
									<li><i>states</i>: (in which) towns and cities</li>
									<li><i>operators</i>: drive between towns and cities</li>
								</ul>
							</li>
							<li>Search: sequence of towns and cities</li>
							<li>Execution: drive</li>
						</ul>
					</p>
				</div>

				

				<div>
					<p>Pseudocode agent</p>
					
				</div>
				<div>
					<p>
						We can make a mockup of an offline reasoning agent in pseudocode. 
					</p>
					<div class="codediv">global Action[] s;  	 # action sequence, initially empty
						
global State state;  	 # current world state
global Goal g;           # goal, initially null
global Problem problem;  # problem to solve

function simpleProblemSolvingAgent(p: Percept) -&gt; Action:
	state &lt;- updateState(state, p)
	if s.empty():
		g &lt;- formulateGoal(state)
		problem &lt;- formulateProblem(state, p)
		s &lt;- search(problem)
	action &lt;- recommendation(state)
	s &lt;- result(state)
	return action</div>

					<p>
						Naturally all the functions mentioned are left as an excersise to the reader (i.e. assumed implemented). 
						<code>formulateGoal</code> internally will use its current perceptions.
					</p>
				</div>

				<div>
					<p>Problem types</p>
				</div>
				<div>
					<p>
						Problems of course can come in many types.
						<ul>
							<li>
								<b>Single state problem</b>: deterministic and fully observable. Sensors tell agent current state, it knows exactly what to do, can work out what state it will be in after.
							</li>
							<li>
								<b>Multiple state problem</b>: deterministic and <i>partially</i> observable. Agent knows its actions, but has limited access to state. There may be a set of states the agent would end up in, agent must manipulate sets.
							</li>
							<li>
								<b>Contingency problem</b>: <i>stochastic</i> and partially observable. Agent does not know current state so is unsure of result of action. Must use sensors during execution. Solution is a tree with contingency branches. Search and execution is often <i>interleaved</i>. 
							</li>
							<li>
								<b>Exploration problem</b>: unknown state space. Agent does not know what it's actions will do, and has to explore and learn. This is an <b>online</b> problem.
							</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Vacuum Cleaner World</p>
				</div>
				<div class="side">
					<p>
						Let's use an example of a simple environment, a vacuum robot <code>V</code> and a world with two locations, <code>[  ][  ]</code>. Each location may also contain dirt (<code>[ d]</code> or <code>[Vd]</code> for vacuum and dirt). The vacuum can move left or right, and suck dirt.
					</p>

					<p>
						<b><i>Single state.</i></b> Suppose we know we are in the state <code>[V][d]</code>. We then know our sequence of actions is just [right, suck]. 
					</p>
					<p>
						<b><i>Multiple State.</i></b> (sensorless / conformant) Suppose we don't know which state we start in. We could have the states:
						<table>
							<tr>
								<td>1. <code>[Vd][ d]</code></td>
								<td>2. <code>[ d][Vd]</code></td>
								<td>3. <code>[Vd][  ]</code></td>
								<td>4. <code>[ d][V ]</code></td>
							</tr>
							<tr>
								<td>5. <code>[V ][ d]</code></td>
								<td>6. <code>[  ][Vd]</code></td>
								<td>7. <code>[V ][  ]</code></td>
								<td>8. <code>[  ][V ]</code></td>
							</tr>
						</table> 

						If we start in any one of those states, and we move to the right, we will always end up in states {2, 3, 6, 8}. If we move left afterwards, we will guarantee vacuum ends up on the left. Thus we can do [right, suck, left, suck]. 
					</p>

					<p>
						<b><i>Contingency.</i></b> Suppose we start in state <code>[V ][ d]</code>, but our vacuum has received a downgrade: if we suck a clean square, there is a chance that it may make that square <i>less</i> clean. So what do we do? 
					</p>
					<p>
						What we can is sense whilst acting, thus: [right, <b>if</b> dirt <b>then</b> suck].
					</p>
				</div>

				<div>
					<p>State space problems</p>
				</div>
				<div>
					<p>
						A <b>State space problem</b> consists of a set of states; subsets of <b>start states, goal states</b>; a set of actions; an <b>action function</b> (f(state, action) -> new state); and a criterion that specifies the <i>quality</i> of an acceptable solution (depends on problem).
					</p>
					<p>
						Since the real world is complex, state space may require <i>abstraction</i> for problem solving. States, operators, and solutions approximate the real world equivalent, though one must take care that their operations are still valid for the real world counterpart.
					</p>
				</div>

				<div>
					<p>8-puzzle</p>
				</div>
				<div class="side">
					<p>
						A 3\(\times\)3 grid of tiles numbered 1 to 8, with a blank space for sliding. We want to get from some initial state to some goal state. 

						<pre><code>283   123
164   8_4
7_5   765</code></pre>

						On the left is an initial state, and on the right is the goal state.
					</p>
					<ul>
						<li><i>States</i> can be coordinate locations of tiles</li>
						<li><i>Operators</i> - we want the fewest possible - are moving the <i>blank square</i> up, down, left, or right</li>
						<li>Our <i>goal test</i> would be the given goal state</li>
						<li>The <i>path cost</i> we can set as 1 per operation</li>
					</ul>
				</div>

				<div>
					<p>State Space Graphs</p>
				</div>
				<div>
					<p>
						A general formultion for a problem is a <b>state space graph</b>. It is:
						<ul>
							<li>A <i>directed</i> graph consisting of a set N of <i>nodes</i>, set A of <i>arcs/edges</i>, which are ordered pairs of nodes.</li>
							<li>Node \(n_2\) <i>neighbours</i> node \(n_1\) if there is an arc \(\langle n_1, n_2 \rangle\) (denoted with triangle brackets)</li>
							<li>A <i>path</i> is a sequence of nodes \(\langle n_0, n_1, \cdots, n_k\rangle\) s.t. \(\langle n_{i-1}, n_i \rangle \in A\) (arcs between all nodes)</li>
							<li>The <i>length</i> of a path from \(n_0\) to \(n_k\) is \(k\).</li>
							<li>Given a set of <i>start nodes</i> and <i>goal nodes</i>, a <b>solution</b> is a path from a start node to a goal node.</li>
						</ul>
					</p>
				</div>
			</div>

			<h3 id="usea-2">Tree Searching</h3>

			<div class="cornell">
				<div>
					<p>Tree searching algorithms</p>
				</div>
				<div>
					<p>
						A tree search is a basic approach to problem solving. It is an <i>offline</i>, simulated exploration of the state space. Starting with a start state, we <i>expand</i> an explored state by generating / exploring its successor branches, to build a search tree. 
					</p>
					<p>
						We have a single start state, and repeatedly expand until we find a solution, or run out of valid candidates. 
					</p>
					<button class="collapsible">Example tree of 8 puzzle</button>
					<div class="ccontent">
						<img src="./8puzzle-search.png" alt="Some branches are omitted for simplicity." style="max-width: 400px;">
					</div>
				</div>

				<div>
					<p>Implementation details</p>
				</div>
				<div>
					<p>
						A <b>state</b> represents a physical configuration of a puzzle, and a <b>node</b> will be the data structure for one state of the tree.
					</p>
					<p>
						A node stores the <b>state, parent, children, depth, path cost</b> - the last of these is usually denoted \(g\).
					</p>
					<img src="./tree-node.svg" alt="A typical tree node" style="max-width: 400px;">
					<p>
						Nodes (explored) but not yet expanded are called the <b>frontier</b>, and is generally represented as a queue.
					</p>
				</div>

				<div>
					<p>Search strategies</p>
				</div>
				<div>
					<p>
						The strategy of searching is defined by which order we expand nodes in. These strategies are evaluated over several criteria:
						<ul>
							<li><b>completeness</b>: will the strategy always find a solution, if it exists?</li>
							<li><b>optimality</b>: will the best (least cost) solution be found?</li>
							<li><b>time complexity</b>: The number of nodes expanded - this depends on the maximum <i>branching factor</i> \(b\), depth of best solution \(d\), and the maximum depth of the state space \(m\), which may be \(\infty\).</li>
							<li><b>space complexity</b>, the maximum number of nodes stored in memory (also in terms of \(b, d, m\)) </li>
						</ul>
					</p>
				</div>

				<div>
					<p>Simple, uninformed strategies</p>
				</div>
				<div>
					<p>
						<b>Uninformed</b> search only uses information given in problem definiton - we have no measure of which nodes would be better to expand to, nor do we really check the nodes once we've explored them (i.e. not checking cycles).
					</p>
					<p>
						Two of these algorithms are Breadth-first search BFS and Depth-first search DFS.
					</p>
				</div>

				<div>
					<p>Breadth first</p>
				</div>
				<div>
					<p>
						BFS expands the shallowest node: put recently expanded successors at the end of queue: FIFO.
					</p>
					<ul>
						<li>
							<b>Completeness</b>. BFS is complete because it will always find a solution, provided \(b\) finite.
						</li>
						<li>
							<b>Time</b>. Since at every node we expand \(b\) branches, in total for a depth of \(d\) we will be expanding \(1 + b + b^2 + b^3 + \cdots + b^d = O(b^d)\) times. This becomes \(O(b^{d+1})\) if the check for goal state is done when that node is about to be expanded, instead of as part of expansion.
						</li>
						<li>
							<b>Space</b>. Each leaf node is kept in memory, thus there are \(O(b^{d-1})\) explored nodes and \(O(b^{d})\) in the frontier, and overall space is dominated by frontier, \(O(b^d)\).
						</li>
						<li>
							<b>Optimality</b>. BFS is optimal <i>only if</i> the cost function does not decrease. 
						</li>
					</ul>
					<p>
						The main problem for BFS is the space usage, which can balloon very very quickly (with \(b = 10\) and 1 KB per node, at depth 8 one needs around 1 TB of memory).
					</p>
				</div>

				<div>
					<p>Depth first</p>
				</div>
				<div>
					<p>
						DFS expands the deepest node: put most recently expanded successors at the start of the "queue" (stack): LIFO. 
					</p>
					<ul>
						<li>
							<b>Completeness.</b> DFS is <i>not</i> complete because it will fail in infinite depth and loop situations. If we avoid repeated paths and have or limit to a finite depth, then DFS <i>is</i> complete.
						</li>
						<li>
							<b>Time.</b> \(O(b^m)\), which is O(horrible) if \(m > d\) significantly. Otherwise may be significantly faster than BFS.
						</li>
						<li>
							<b>Space.</b> \(O(bm)\), i.e. linear space: store path from root to leaf and unexpanded siblings of nodes on path.
						</li>
						<li>
							<b>Optimality.</b> Not guaranteed.
						</li>
					</ul>
				</div>
			</div>

			<h3 id="usea-3">Graph Searching</h3>

			<div class="cornell">
				<div>
					<p>Notes</p>
				</div>
				<div>
					<p>
						This section is <i>uninformed</i> search, and graph searches are similarly uninformed, so much so that the algorithms presented do not even check for cycles, despite common intuition.
					</p>

					<p>
						The main difference is that in graph search we look at <i>paths</i>, rather than nodes. Expanding into a node would be adding it to the stored <i>path</i>.
					</p>
				</div>
	
				<div>
					<p>Graphs</p>
				</div>
				<div>
					<p>
						With trees, if we have looped states these will just branch out to infinity. Graphs are a good way to take loops into account and are a practical representation of such spaces.
					</p>
					<p>
						Similar to a tree, we incrementally explore, maintaining a frontier of explored paths. The frontier should expand into unexplored nodes until a goal node is encountered. 
					</p>
					<p>
						The way nodes are selected is the <i>search strategy</i>, the <i>neighbours</i> of (all) nodes define a graph, and the <i>goal</i> defines the solution.
					</p>
					<p>
						We can continue searching from the return call even if a solution has been found, if we want more or all possible solutions.
					</p>
				</div>

				<div>
					<p>BFS on Graphs</p>
				</div>
				<div>
					<p>
						All paths on the frontier \([p_1 .. p_r]\) are expanded one by one, and the neighbours of an explored path are added to the <i>end</i> of the frontier. 
					</p>
					<p>
						The time complexity is instead written \(O(b^n)\), where \(n\) is the <i>path length</i> (instead of tree depth).
					</p>
				</div>

				<div>
					<p>DFS on Graphs</p>
				</div>
				<div>
					<p>
						DFS treats the frontier as a stack instead. The time complexity is \(O(b^k)\) where \(k\) is the maximum number of arcs a path can have (i.e. max depth). Space complexity is linear: \(O(k + (b-1))\).
					</p>
					<p>
						Now, DFS is <i>said</i> to fail on both infinitely long graphs and graphs with cycles, although I'm not sure why you're adding already explored nodes to the frontier (cycle case). 
					</p>
				</div>

				<div>
					<p>Lowest cost first</p>
				</div>
				<div>
					<p>
						Sometimes, there are <i>costs</i>, or weights, associated with edges. The cost of a path is thus the sum of the cost of edges, and an optimal solution has the path with the smallest total cost. 
					</p>
					<p>
						Lowest cost first is a fairly naive algorithm that simply selects the path on the frontier with the lowest cost. I.e., frontier paths are stored in a <i>priority queue</i> according to path length.
					</p>
					<p>
						Lowest cost is <b>complete</b>.
					</p>
				</div>
			</div>

			<p>
				Supposedly <i>none</i> of these algorithms will halt on a finite graph with cycles, but one has to be a particular type of person to program a graph search without cycle checking <i>/ not adding explored nodes to the unexplored frontier</i>.
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="informed">Informed Search</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<ol>
				<li><a href="#inf-1">Heuristic Search</a></li>
				<li><a href="#inf-2">A* Search</a></li>
				<li><a href="#inf-3">Pruning, Cycle Checking, and Restrictions</a></li>
				<li><a href="#inf-4">Changing Directions</a></li>
			</ol>

			<div class="cornell">
				<div>
					<p>Overview</p>
				</div>
				<div>
					<p>
						Uninformed search is inefficient, especially when we have extra information we can exploit. By using <i>problem-specific</i> knowledge, we can improve our searching - this is <b>informed search</b>.
					</p>
					<p>
						Still offline.
					</p>
				</div>
			</div>

			<h3 id="inf-1">Heuristic Search</h3>

			<div class="cornell">
				<div>
					<p>Overview</p>
				</div>
				<div>
					<p>
						If you know where the goal is, when selecting paths do not ignore it. 
					</p>
					<p>
						Employ the use of an extra <b>heuristic</b> measurement \(h(n)\) - an <i>estimate</i> of the remaining cost to the goal. 
					</p>
					<p>
						This heuristic should be <b>efficient</b> to compute, and always be an <b>underestimate</b> (no path to goal strictly less than \(h(n)\)). If a heuristic meets these criteria, then it is <b>admissible</b>.
					</p>
				</div>
				<div>
					<p>Examples</p>
				</div>
				<div class="side">
					<p>
						If one is on a euclidean plane finding the shortest path, \(h(n)\) can be the straight-line distance to the goal. If the cost is time, \(h(n)\) can be straight-line distance divided by max speed. 
					</p>
					<p>
						We can think of this as reducing the constraints in the problem to an easier representation that we can quickly calculate an answer for. The straight-line distance does not count obstacles, for example. 
					</p>	
				</div>

				<div>
					<p>Best First Search</p>
				</div>
				<div>
					<p>
						When we choose nodes to expand in a frontier, the heuristic can determine the <i>order</i> / rank of the frontier structure. 
					</p>
					<p>
						<b>Heuristic DFS</b> is where we select a neighbour such that the <i>best neighbour</i> is selected first. 
					</p>
					<p>
						If by best we mean closest (by some metric) distance to the goal, then we have <b>Greedy Best First Search</b> (BFS).
					</p>
					<p>
						The frontier is a priority queue ordered by \(h(n)\).
					</p>
				</div>

				<div>
					<p>Problems</p>
				</div>
				<div>
					<p>
						Some graphs are very much not good for BFS. Especially if we do not track cycles.

						<figure>
							<img src="./bad-best-first.svg" alt="A bad graph for best-first search" >
							<figcaption><i>A bad graph for best first search - it will loop forever because the bottom path is closer as the crow flies.</i></figcaption>
						</figure>

						
					</p>
				</div>

				<div>
					<p>Analysis</p>
				</div>
				<div>
					<p>
						BFS has an exponential space complexity, of \(O(b^n\) where \(b\) is the branching factor and \(n\) is the number of nodes. TIme complexity is also \(O(b^n)\).
					</p>
					<p>
						It is not guaranteed to find the solution, even if one exists, and may not necessarily find the shortest path (but will go in its general direction).
					</p>
				</div>
			</div>

			<h3 id="inf-2">A* Search</h3>

			<div class="cornell">
				<div>
					<p>A* Search</p>
				</div>
				<div>
					<p>
						A* considers both path cost and heuristic values. We denote the cost of a path \(p\) by \(g(p)\) or \(cost(p)\), and the <i>estimate</i> of remaining path to goal is \(h(p)\).
					</p>

					<p>
						Let \(f(p) = cost(p) + h(p)\), at some node on the path \(p\). \(f(p)\) is thus the overall estimate cost of \(p\). A* searches using this estimate value.
					</p>

					<p>
						Thus A* can be considered as a mix of lowest-cost and best-first, and its frontier is a priority queue ordered by the metric \(f\).
					</p>
				</div>

				<div>
					<p>Admissibility</p>
				</div>
				<div>
					<p>
						A* <b>is admissible</b> if
						<ul>
							<li>The branching factor is <i>finite</i></li>
							<li>Arc costs are bounded above some positive number (cannot just be \(>0\) because we'd get  <a href="https://artint.info/2e/html/ArtInt2e.Ch3.S5.SS4.html">Zeno's Paradox</a> )</li>
							<li>\(h(n)\) is non-negative and an <b>underestimate</b></li>
						</ul>
					</p>

					<p>
						<b><i>Justification.</i></b> A path \(p\) is selected by A*. Suppose another path \(p'\) ends on the frontier. Since \(p\) is chosen before \(p'\), by definition 
						\[cost(p) \leq cost(p') + h(p')\]
						And since \(h\) is an underestimate \(cost(p') + h(p') \leq cost(p'_C)\), where \(p'_C\) is a complete path extending \(p'\) and reaching the goal. By definition of the algorithm, we must have for any other unpicked path \(p'_C\)
						\[cost(p) \leq cost(p'_C).\]
					</p>

					<p>
						A* can always find a solution if there is one.
					</p>
					<p>
						The frontier always contains part of a path to the goal, and A* will halt as the costs of the paths increase beyond any finite number. 
					</p>
					<p>
						However, this doesn't mean A* won't pick unoptimal nodes, just that its final result is always the optiumal path (even if we have cycles in our graph).
					</p>
				</div>

				<div>
					<p>Heuristics are not equal</p>
				</div>
				<div>
					<p>
						Some heuristics are better than others. The closer a heuristic is to the true value <i>(without overestimating)</i>, the more efficient A* is. 
					</p>
					<p>
						For some optimal cost \(c\), A* will explore all paths \(p:cost(p) + h(p) < c\). A* will explore some paths \(q:cost(q) + h(q) = c\) (until it finds the solution), and will not explore overestimates. 
					</p>
					<p>
						Thus the closer the underestimate, the less paths there are where \(cost(p) + h(p) < c\). Additionally, if the number of paths where \(cost(p) + h(p) = c\) is large, then there may be much variability in space and time of A*.
					</p>
				</div>

				<div>
					<p>Complexity</p>
				</div>
				<div>
					<p>
						A* is <b>exponential</b> time in \(h\)-error \(\times\) path length to solution. A* is also <b>exponential</b> space, since it stores all nodes in memory. 
					</p>
				</div>
			</div>

			<h3 id="inf-3">Pruning, Cycle Checking, and Restrictions</h3>

			<div class="cornell">
				<div>
					<p>Cycle Checking</p>
				</div>

				<div>
					<p>
						Before, we have not dealt with the cases where a search algorithm hits an already visited node (i.e. go in a cycle). If this is so, we can "prune" a path that leads into a cycle, removing all unnecessary nodes from consideration.
					</p>
					<p>
						In depth-first methods, cycle checking can be done in \(O(1)\) with things like hash functions. In most other algoirthms though this is often \(O(n)\).
					</p>
				</div>

				<div>
					<p>Multiple Path Pruning</p>
				</div>
				<div>
					<p>
						Similarly, this pruning can apply when we have two paths to the same node. We can prune the path to said node and keep just the existing path.
					</p>
					<p>
						THis can be implemented via a set of explored nodes at the <i>end</i> of explored paths, the <b>closed list</b>. 
					</p>
					<p>
						When a path \(\langle n_0 \cdots n_k \rangle\) is selected, if \(n_k \in\) closed list, then discard path, otherwise add \(n_k\) to list and proceed on.
					</p>
				</div>

				<div>
					<p>On optimal solutions</p>
				</div>
				<div>
					<p>
						How do we make sure we do not prune a path whcih is better than the already found one?
					</p>
					<p>
						The most straightforward answer is to make sure that you always select the best path first, though this may be hard. We could remove all paths which use the longer route, but those paths may extend further (past the node which the better route reaches), and that's a lot of work wasted.
					</p>
					<p>
						Otherwise we can simply replace the longer path: given a path \(p = \langle s \cdots n \cdots m \rangle\) if \(p' = \langle s \cdots n \rangle\) is found which is shorter than the corresponding part in \(p\), replace that specific segment.  
					</p>
				</div>

				<div>
					<p>Path Pruning and A*</p>
				</div>
				<div>
					<p>
						A* selects a path \(p'\) to node \(n'\). Suppose there's a shorter path to \(n'\), via a path \(p\), which currently ends at the fronter node \(n\) (so \(len(p') > len(p) + len(\langle n \cdots n' \rangle )\)).
					</p>
					<p>
						Since \(p'\) was selected before \(p\), \(f(p') \leq f(p)\), thus \(cost(p') + h(p') \leq cost(p) + h(p)\). Let \(cost(n, n')\) be the real length of a path \(\langle n \cdots n' \rangle\). Since \(p\) is lower cost than \(p'\), \(cost(p) + cost(n, n') &lt; cost(p')\).

						\begin{align}
							\implies cost(p') - cost(p) &\leq h(p) - h(p') \\
							\textrm{and } cost(n, n') & &lt; cost(p') - cost(p) \\
							h(p) - h(p') &= h(n) - h(n') \\
							\implies cost(n, n') & &lt; h(n) - h(n')

						\end{align}

						If we make sure \(|h(n) - h(n')| \leq cost(n, n')\) then this situation will never happen (always select the best path first option).
					</p>
				</div>

				<div>
					<p>Monotone Restriction</p>
				</div>
				<div>
					<p>
						A heuristic \(h\) satisfies <b>monotone restriction</b> if \(|h(m) - h(n)| \leq cost(m, n) \; \forall \textrm{arcs } \langle m, n \rangle\). It is then termed <b>consistent</b>. 
					</p>
					<p>
						A* with a consistent heuristic and path pruning always finds the shortest path.
					</p>
				</div>
			</div>

			<h3 id="inf-4">Changing Directions</h3>

			<div class="cornell">
				<div>
					<p>Direction of Search</p>
				</div>
				<div>
					<p>
						In graph/tree searches, the direction we search doesn't really matter. We can go from goal to start nodes.

					</p>
					<p>
						Whilst the forward branching factor is the number of arcs out of a node, the <b>backwards branching factor</b> is the number of arcs going into a node. 
					</p>
					<p>
						If the forward factor is larger than backwards factor, our \(O(b^n)\) algorithm would do much better searching backwards.
					</p>
					<p>
						Of course, a backwards graph isn't always available. 
					</p>
				</div>

				<div>
					<p>Bidirectional Search</p>
				</div>
				<div>
					<p>
						We have a \(O(b^k)\) (\(k\) depth of goal) algorithm. We can get it to be less horrible by searching from <i>both sides simultaneously</i>. \(2b^{\frac{k}{2}} < b^k\) after all.
					</p>
					<p>
						The main problem with this is making sure both sides meet. 
					</p>
					<p>
						Oftentimes, BFS is used from the goal for a full set of locations that leads to goal, whilst another method is used from the start to link up the path.
					</p>
				</div>

				<div>
					<p>Island Hopping</p>
				</div>
				<div>
					<p>
						An <b>island driven search</b> is like an extension of the bidirectional search - we have a series of "islands": useful nodes that can link a start and a finish, 
						\[s \longrightarrow i_1 \longrightarrow i_2 \longrightarrow \cdots \longrightarrow i_m \longrightarrow t\]
						Now, we have \(m\) subproblems to solve, which can improve efficiency to \(mb^{\frac{k}{m}} \).
					</p>
					<p>
						The difficult is then in identifying the correct islands, and it's hard to guarantee optimality with such an approach. 
					</p>
				</div>

				<div>
					<p>Dynamic Programming</p>
				</div>
				<div>
					<p>
						<b>Dynamic programming</b> is a fancy way of saying have an array of values you update and retrieve from as you go along. This is explained more in depth in CS260. <!--TODO: Link to CS260 dynam programming when available-->
					</p>
					<p>
						We build a table \(D: D_n = \) the distance of shortest path from \(n\) to the goal. Then, apply the following strategy:
						\[D_n = \begin{cases} &0 &\textrm{if isGoal}(n) \\ &\min_{\forall \langle m, n \rangle} (|\langle m, n \rangle| + D_m) &\textrm{otherwise}\end{cases}\]
					</p>
				</div>

				<div>
					<p>Bounded DFS</p>
				</div>
				<div>
					<p>
						A bounded DFS does normal DFS, but has a cost/depth bound at which it will stop exploring. Only part of the search graph is explored. 
					</p>
					<p>
						We do this because DFS uses linear space and if space is tight it's appropriate. 
					</p>
				</div>

				<div>
					<p>Iterative Deepening</p>
				</div>
				<div>
					<p>
						If we bounded-DFS over an ever increasing bound, we have <b>iterative deepening</b>. Starting with a bound \(b = 0\), do DFS, and either return the solution or increase the bound. 
					</p>
					<p>
						This finds the first solution BFS does, but uses linear space. There is an overhead of \(\frac{b}{b-1} \times\) the cost of expanding a node at depth \(k\) with BFS; i.e. bounded DFS will do \(\frac{b}{b-1} \times\) more work than DFS at the same \(b\) depth. As \(b\) increases, overhead decreases.
					</p>
				</div>

				<div>
					<p>Depth-First Branch & Bound</p>
				</div>
				<div>
					<p>
						<b>Depth first branch and bound</b> Combines DFS with heuristic info, and so will find an optimal solution. More useful if multiple solutions, and still has the space complexity of DFS. 
					</p>

					<p>
						We want to find a single optimal solution. Suppose \(b\) is the cost of the shortest path to goal so far. 
					</p>
					<p>
						What if we then encounter a path \(p : cost(p) + h(p) \geq b\)? \(p\) can be pruned. If we instead communicate a path \(q : q\) is better, then \(q\)'s cost can be \(b\) and the path \(q\) remembered.
					</p>
					<p>
						This is linear space (since DFS) and we can guarantee an optimal solution when it completes. 
					</p>
				</div>

				<div>
					<p>Initalising Bound</p>
				</div>
				<div>
					<p>
						We can initialise the bound \(b\) to \(\infty\). We can also set it to an estimate of the minimum path cost. 
					</p>
					<p>
						After DFS terminates, either (1) a solution found, (2) no solution found, no path pruned, (3) no solution found, path pruned.
					</p>
					<p>
						Can be combined with iterative deepening to increase the bound until a solution is found. 
					</p>
					<p>
						Cycle pruning works well, but multiple path pruning does not, since storing extra paths defeats the point of saving space with DFS. 
					</p>
				</div>
			</div>

			<h3 id="inf-5">Admissible Heuristics</h3>

			<div class="cornell">
				<div>
					<p>The 8-puzzle example</p>
				</div>
				<div class="side">
					<p>
						The branching factor is roughly 3, and a typical solution is 20 steps. Thus, an exhaustive search is \(3^20\) states, and there are \(9!\) non-repeating states. It would be nice to have a heuristic to search with. 
					</p>

					<p>
						We could:
						<ul>
							<li>
								\(h_1(n)\) the number of misplaced tiles - admissible since each misplaced tile must be moved at least once
							</li>
							<li>
								\(h_2(n)\) the Manhattan (city block) distance of each tile - admissible since a single move can only move one step orthogonally. 
							</li>
						</ul>
					</p>
				</div>

				<div>
					<p>Characterising Heuristics</p>
				</div>
				<div>
					<p class="blue">
						<b><i>Definition.</i></b> If A* search expands \(N\) nodes and solution is depth \(d\), then the <b>effective branching factor</b> \(b* \) is the factor a uniform tree of depth \(d\) must have to have \(N\) nodes. 
						\[N = 1 + b* + (b*)^2 + \dots + (b*)^d\]
						Ex. depth 5 and 52 nodes: \(b* = 1.91\).
					</p>
					<p>
						As \(b* \longrightarrow 1\), the larger the problem that can be solved (efficiently).
					</p>

					<p>
						Whilst solving for \(b* \) may be difficult, we can estimate it experimentally, over small instances. 
					</p>
					<p>
						On the prior example, an experiment determined that \(h_2\) is better than \(h_1\) (lower EBF). 
					</p>
					<p class="blue">
						<b><i>Definition.</i></b> If some \(h_2(n) \geq h_1(n) \; \forall n\) then \(h_2\) <b>dominates</b> \(h_1\).
					</p>

					<p>
						Note that whilst an (admissible) heuristic which is closer to the true cost is better, we must make sure the heuristic still remains cheap to calculate. 
					</p>
				</div>
				<div>
					<p>Deriving Heuristics - Relaxed problems</p>
				</div>
				<div>
					<p>
						Admissible heuristics can be the exact solutions of problems with certain conditions relaxed. 
					</p>
					<p>
						If the problem is defined in formal language this is easier. 
					</p>
					<p>
						We may generate several different relaxed problems, and thus may have several admissible heuristics. If one dominates, use that. Else, if you have \(h_1, h_2, \dots, h_n\) admissible heuristics which do not dominate each other, just use \(h = \max(h_1, h_2, \dots, h_n)\). 
					</p>					
					<p>
						Because all options are admissible, the aggregate \(h\) is admissible - and dominates each constituent individually. 
					</p>
				</div>
				<div>
					<p>Example</p>
				</div>
				<div class="side">
					<p>
						If we describe the 8-puzzle as "A tile can move from square A to B if A is adjacent to B, and B is blank", then we can generate some relaxed problems:
						<ul>
							<li>A tile can move from A to B if A adjacent to B</li>
							<li>A tile can move from A to B if B is blank</li>
							<li>A tile can move from A to B</li>
						</ul>
						All of which give a heuristic. 
					</p>
				</div>
				<div>
					<p>Subproblems</p>
				</div>
				<div>
					<p>
						We can derive a heuristic from a subproblem.
					</p>
					<p>
						For the 8 puzzle, we can only solve the problem for 4 squares (ignoring 5-8), and use that as a heuristic. Of course, the subproblem should be pretty easy to solve. 
					</p>
				</div>

				<div>
					<p>Pattern Databases</p>
				</div>
				<div>
					<p>
						We can then store the possible solution costs for each instance of a subproblem in a data structure for easy lookup. Then the heuristic is the looked-up cost of the corresponding subproblem. 
					</p>
					<p>
						Different pattern databases can also be combined to get a maximum of them all. 
					</p>
				</div>

				<div>
					<p>Disjoint Pattern Databases</p>
				</div>
				<div>
					<p>
						If we have multiple <b>independent</b> subproblems, we can store them in different databases and <b>add</b> them together.
					</p>
					<p>
						These databases form <b>disjoint pattern databases</b>. 
					</p>
				</div>

				<div>
					<p>Other approaches</p>
				</div>
				<div>
					<p>
						We can run searches over smaller training problems, and use the statistical results as a heuristic. 
					</p>
					<p>
						Say we have a base heuristic \(\eta\). If in 95% our training sets, \(\eta = 14\) when the real cost is 18, we can just use \(h = 18\) as our statistical heuristic.
					</p>
					<p>
						Now this is <b>not admissible</b>, but can often be quite efficient (and good enough).
					</p>
				</div>
			</div>
		</div>

		<div class="colourband">
			<h2 id="csp">Constraint Satisfaction Problems</h2>
		</div>

		<div class="cbox">
			<h3>Contents</h3>

			<ol>
				<li><a href="#csp-1">Varialbes, Constraints, and Worlds</a></li>
				<li><a href="#csp-2">Constraint Satisfaction</a></li>
				<li><a href="#csp-3">Consistency Algorithms</a></li>
			</ol>

			<h3 id="csp-1">Variables, Constraints, and Worlds</h3>

			<div class="cornell">
				<div>
					<p>Varibles</p>
				</div>
				<div>
					<p>
						Constraint Satisfaction Problems (CSPs) are described with regards to <i>variables</i> and configurations of variables <i>(worlds)</i>. 
					</p>
					<p >
						<b>Algebraic Variables</b> are <i>symbols</i> used to describe features. Often denoted with capital letters, like \(X\), which have a corresponding domain of possible values, \(D_X\) or \(dom(X)\). 
					</p>
					<p style="color: gray;"><i>
						Symbols internally are just labels - they only have meaning to the user. The meaning associated to a variable should follow the <b>clarity principle</b>, that an omniscient agent can determine values knowing meaning of the variables.
					</i></p>
					<p>
						<b>Discrete Variables</b> has a finite or countably infinite domain - a special type of discrete variable with domain size 2 is a <i>binary</i> variable, most often seen as boolean variables. 
					</p>
					<p>
						<b>Continuous Variables</b> have uncountably infinite domain, e.g. \(\mathbb{R}\)
					</p>
				</div>
				<div>
					<p>Assignments and Worlds</p>
				</div>
				<div>
					<p>
						Given a set of variables, an <b>assignment</b> is a function \(: \textrm{vars} \rightarrow \textrm{dom(vars)}\). Variables \(\{X_1, X_2, \dots, X_n\} \) would have an assignment \(\{X_1 = v_1, X_2 = v_2, \dots, X_n = v_n\} : v_i \in D_{X_i} \).
					</p>
					<p>
						A <b>total assignment</b> assigns all variables, since assignments can be partial.
					</p>
					<p>
						A <b>possible world</b> is a total assignment. If \(X_i\) has value \(v_i\) in the assignment of a world \(w\), then we say <i>"variable \(X_i\) has value \(v_i\) in world \(w\)</i>.
					</p>
					<p>
						The number of worlds is product of all domains: \(|W| = \prod^n_{i=1} |D_{X_i}|\)
					</p>
					<p>
						The advantage of using variables over pure discrete states is that a few variables can describe many states. 
					</p>
				</div>
				
				<div>
					<p>Constraints</p>
				</div>
				<div>
					<p>
						A <b>(hard) constraint</b> specificies legal combinations of some variables. 
					</p>
					<p>
						A <b>scope</b> is a set of (relevant) variables, a constraint \(C\) is just a relation on that scopt \(C : S \longrightarrow \{T, F\}\), which involves each variable in scope. 
					</p>
					<p>
						An assignment \(A\) on the full set of variables \(S'\) (i.e. the superset of the scope / \(S \subseteq S'\)) <b>satisfies</b> a constraint on \(S\) if everything maps to <b>true</b>. Else, the constraint is <b>violated</b>. 
					</p>
					<p>
						Constraints can be defined by <b>inten<i>s</i>ion</b> and <b>extension</b>. Intension is "define by rule", where formulae are used. Extension is "define by enumeration", where the valid combinations are listed out. 
					</p>
					<p>
						A <b>unary</b> constraint is on 1 variable, a <b>binary</b> on two, etc.
					</p>
				</div>
			</div>

			<h2 id="csp-2">Constraint Satisfaction</h2>

			<div class="cornell">
				<div>
					<p>CSPs</p>
				</div>
				<div>
					<p>
						A CSP consists of a set of variables, a domain per variable, and a set of constraints. The goal is to make true (satisfy) all constraints.
					</p>
					<p>
						A <b>finite CSP</b> has a finite set of variables, with finite domains. We will usually be dealing with these. <i>Note that some algorithms which work for one type of CSP may not work for others.</i>
					</p>
					
					<button class="collapsible">Example 1...</button>
					<div class="ccontent">
						<p>
							<b><i>Example.</i></b> A delivery robot must carry out activities \(a,b,c,d,e\). Those activities happen at any time 1, 2, 3, 4. Let \(A\) be variable of activity \(a\). The domains of all variables are thus \(\{1,2,3,4\}\). The following must be satisfied:

							\[\{ B \neq 3, C \neq 2, A \neq B, B \neq C, C &lt; D, A = D, E &lt; A, E &lt; B, E &lt; C, E &lt; D, B \neq D\} \]
						</p>
					</div>

					<p>
						When we have a CSP, a number of tasks are helpful for it:
						<ul>
							<li>Determine if there is a solution</li>
							<li>Find the solution model</li>
							<li>Count the number of models / enumerate them</li>
							<li>Find the best model (given some metric)</li>
							<li>Determine if some assertion holds</li>
						</ul>
						One can think of each variable in a CSP as a "dimension" - since CSPs can have lots of dimensions, they are often difficult to solve, we can exploit a few structural properties. 
					</p>
				</div>

				<div>
					<p>Generate and Test</p>
				</div>
				<div>
					<p>
						Obviously the simplest way to solve a CSP is via a <b>generate-and-test</b> (G&T) algorithm - <i>to solve via exhaustion</i>. 
					</p>
					<p>
						Let \(D\) be the <i>"assignment space"</i> (set of total assignments), G&T will generate a random assignment, and save it.
					</p>
				</div>

				<div>
					<p>Complexity</p>
				</div>
				<div>
					<p>
						G&T is an \(O(\textrm{horrible})\) algorithm. 
					</p>
					<p>
						For \(n\) variables each with domain size \(d\), and \(e\) constraints, the total number of constraints tested is \(O(ed^n)\). It's fine for small problems but can quickly spiral out of control. 
					</p>
					<p>
						G&T assigns values to all variables regardless, but some constraints are unary, so some values can be ruled out even before generating. 
					</p>
				</div>

				<div>
					<p>Backtracking Search</p>
				</div>
				<div>
					<p>
						<b>Backtracking</b> is about tree-searching a tree constructed from incremental assignments of variables in a CSP. 
					</p>
					<p>
						We define the search space as thus:
						<ul>
							<li>Nodes are assignments of values</li>
							<li>Neighbours of a node \(n\) are gotten by selecting some var \(Y \neq n\) and assigning a value, then checking constraints</li>
						</ul>

					</p>

					<button class="collapsible">Example 2...</button>
					<div class="ccontent">
						<p>
							<b><i>Example.</i></b> Let us have a CSP with variables \(A, B, C\) each with domain \(\{1,2,3,4\} \), with constraints \((A < B), (B < C)\). A search tree can be drawn:

							<figure>
								<img src="./csp-backtrack.png" alt="Backtracking tree for the CSP">
								<caption><i>Backtracking tree for the CSP</i></caption>
							</figure>
							
							The node here represents it and all assignments towards it from the root. Anything with a cross means that assignment violates the constraint, and that branch is pruned. Anything with a tick means that is a valid total assingment. 
						</p>
					</div>

					<p>
						Since all goals are at depth \(n\) (\(n\) vars), we use DFS to save space. The branching factor \(b = (n-l)d\) for a depth \(l\) and domain size \(d\). 
					</p>
				</div>

				<div>
					<p>Implementation</p>
				</div>
				<div>
					<p>
						We can implement backtracking as the following:
					</p>
					<div class="codediv">backtrackCSP(CSP):
	return backtrack_r(\(\{\}\), CSP)

backtrack_r(assignment, CSP):
	if assignment complete: 
		return assignment
	\(X \longrightarrow\) unassigned variable in CSP.vars
	for each \(v \in\) CSP.domain(\(X\)):
		if \(v\) consistent with assignment given CSP.constraints:
			assignment.add(\(X = v\))
			result \(\longrightarrow\) backtrack_r(assignment, CSP)
			if result \(\neq\) fail: 
				return result
			assignment.remove(\(X = v\))
		return fail</div>
				</div>

				<div>
					<p>Selection heuristic</p>
				</div>
				<div>
					<p>
						How do we select a variable? How do we assign a value? We use heuristics (which can be combined) to determine which to select. 
						<ul>
							<li><b>MRV &middot; Minimum Remaining Values</b>: select the <i>variable</i> with fewest possible values.</li>
							<li><b>Most Degrees</b>: <i>variable</i> with the most constraints - often used as a tiebreaker to MRV.</li>
							<li><b>LCV &middot; Least Constraining Value</b>: assign a <i>value</i> which rules out the least other values.</li>
						</ul>
					</p>
				</div>
			</div>

			<h3 id="csp-3">Consistency Algorithms</h3>

			<div class="cornell">
				<div>
					<p>
						Introduction
					</p>
				</div>
				<div>
					<p>
						Though DFS is an improvement on G&T, it still isn't that <i>good</i>. 
					</p>
					<p>
						In example 2, the DFS searched through all possible alignments in a domain, even if they were clearly contradictory with one or more other variables. 
					</p>
					<p>
						We could interleave an algorithm that removes all impossible values given the current assigned domains and the constraints. 
					</p>
				</div>
				<div>
					<p>Consistency Algorithms</p>
				</div>
				<div>
					<p>
						Consistency algorithms operate on a <b>network</b> of constraints: 
					</p>
					<p style="padding-left: 2em;">
						Each node is a <b>variable</b>, which is annotated with its <b>domain</b>. 
						Each arc between nodes is a <b>constraint</b> over those variables, and annotated with the rule of said constraint. 
					</p>
					<p>
						We say that for a variable \(X\), \(D_X\) will now be the set of possible values (which initially is its domain). For every constraint \(c,\; \forall X\) in scope of \( c ,\; \exists \textrm{ arc } \langle X, c \rangle\).
					</p>
					<p style="color: gray;">
						Another way is to represent variables as <b>round</b> nodes and constraints as <b>rectangular</b> ones, with an arc between a varaible and a constraint meaning that variable is in scope of that constraint. 
					</p>
				</div>
				<div>
					<p>Arc Consistency</p>
				</div>
				<div>
					<p>
						In the simplest case, the constraint is unary. The variable is then <b>domain consistent</b> if all values satisfy the constraint. 
					</p>
					<p>
						Then, let constraint \(c\) have the scope \(\{X_1, Y_1, \dots, Y_k\} \). Arc \(\langle X, c \rangle\) is <b>arc consistent</b> if \(\forall x \in D_x\), \(\exists y_1 \dots y_k : y_i \in D_{Y_i} : c(X=x, Y_1=y_1, \dots, Y_k = y_k)\) is satisfied. I.e. every value for X has some compatible assignments for all other variables. 
					</p>
					<p>
						A <i>network</i> is arc consistent if <i>all</i> arcs are consistent. 
					</p>
					<p>
						If an arc is not consistent, then \(\exists x \in D_X\) which is incompatible with some other variables in the constraint. We can <b>delete</b> values to make an arc consistent. 
					</p>
					<p>
						Though deleting a value from one \(D_X\), it may make other constraint arcs with \(X\) inconsistent again, so these need to be redone. We compile all of this into an algorithm like so:
					</p>
					
				</div>
				<div></div>
				<div>
					<div class="codediv">generic_arc_consistency(vars \(V\), domains \(dom\), constraints \(C\)):
	return gac2(\(V\), \(dom\), \(\{\langle X, c \rangle : c \in C, x \in scope(C)\} \))

gac2(vars \(V\), domains \(dom\), constraints \(C\), paths \(toDo\)):
	while \(toDo \neq \varnothing\):
		remove and select \(\langle X, c \rangle\) from \(toDo\)
		\(\{Y_1, \dots, Y_k\} \longrightarrow scope(C) \setminus \{X\}\)
		\(ND \longleftarrow \begin{align} \{X &: 
			x \in dom[x] \land \exists y_1, \dots, y_k \in dom[Y_1], \dots, dom[Y_k] 
			\\ &: c(X=x, Y_1=y_1, \dots, Y_k=y_k) \textrm{ true}\} \end{align}\)
		if \(ND \neq dom[x]\):
			\(toDo \longleftarrow toDo \cup \{\langle Z, c' \rangle 
				: \{X, Z\} \subseteq scope(c'), c' \neq c, Z \neq X\}\)
			\(dom[x] \longleftarrow ND\)
		return \(dom\)</div>
				</div>
				<div>
					<p>Algorithm Description</p>
				</div>
				<div>
					<ul>
						<li>GAC keeps track of a (toDo) list of unconsidered arcs, which are initially all arcs in the graph. </li>
						<li>Until the to do list is expended, we pick and remove an arc every time, and making it consistent with regards to the constraint (\(c\)). If we make any changes, this may adversely affect all other constraints including (X).</li>
						<li>Thus, for all other constraints (\(c'\)), we need to add the arcs for all <i>other</i> variables in those constraints (\(Z \neq X\)) to the to do list to reconsider again.</li>
					</ul>

					<p>
						<b>Regardless</b> of arc selection order, the algorithm will terminate with the same arc-consistent graph and set of domains. Three cases will happen:
						<ol>
							<li>A domain becomes \(\varnothing \implies\) no solution to CSP</li>
							<li>Each domain has 1 value \(\implies\) unique solution</li>
							<li>All or some domains have multiple values \(\implies\) we do not know the solution and will have to use another method to find it</li>
						</ol>
					</p>
					<p>
						<span class="sc">Note Well:</span> <b>Arc consistent algorithms can still have no solutions</b>. Example: Take a CSP with variables \(A, B, C\) all with domain \(\{1,2,3\} \), and constraints \(A = B, B = C, C \neq A\). The graph is arc consistent - but as we can clearly see there are no possible solutions. 
					</p>
				</div>

				<div>
					<p>Complexity</p>
				</div>
				<div>
					<p>
						Assume we have only binary constraints.
					</p>
					<p>
						Suppose there are \(c\) many constraints, and the domain of each variable is size \(d\). There are thus \(2c\) arcs. Checking the arc \(\langle X, c = r(X, Y) \rangle\)* is at the <i>worst case</i> iterating through each domain value in \(Y\) for each in \(X\), thus making \(O(d^2)\).
					</p>
					
					<p class="small">
						*As mentioned previously \(c\) is a relation, \(c = r(X,Y)\) is just denoting that it's over the variables X and Y.
					</p>
					<p>
						The arc may need to be checked once for each element in \(dom(Y)\), which is \(O(cd)\).
					</p>
					<p>
						Thus <b>time complexity</b> of GAC on binary constraints is order \(O(cd^3)\), which is linear on \(c\). 
					</p>
					<p>
						The <b>space complexity</b> of GAC is \(O(nd)\) for \(n\) variables. 
					</p>
				</div>
				<div>
					<p>Extensions</p>
				</div>
				<div>
					<p>
						The domain need not be finite - and many be specified through intensions.
					</p>
					<p>
						If constraints defined extensionally, when we prune a value from X, we can prune that value from all other constraints including X.
					</p>
					<p>
						Higher order techniques like path consistency can consider \(k\)-tuples of vars at once - this may deal with the "arc consistent but no solution" problem but is often less efficient. 
					</p>
				</div>
			</div>
		</div>

		<div class="colourband">
			<h2 id="localsearch">Local Search</h2>
		</div>

		<div class="colourband">
			<h2 id="adversarialsearch">Adversarial Search</h2>
		</div>

		<div class="colourband">
			<h2 id="uncertainty">Planning with Uncertainty</h2>
		</div>

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="../../js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
</body>
</html>