<!DOCTYPE html>
<html>
<head>
	<title>CS349</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="../../" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="../../blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="../../about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS349</h1>
					<p class="subheading">Princples of Programming Languages</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Contents </h1>
			</div>
		</header>


		<div class="cbox">
			<div class="md-conv">
				&nbsp;
				1. [Lambda Calculus](#lcalc)
				2. [Simply Typed Lambda Calculus](#stlc)
				3. [PCF Model Language](#pcf)
				4. [MiniML](#miniml)
				5. [Polymorphic Lambda Calculus](#plc)
				6. [Simple Language with Subtypes](#sls)
				7. [Imperative Programming](#imp)

				\( 
					\renewcommand{\vec}{\mathbf}
					\renewcommand{\epsilon}{\varepsilon}
					\DeclareMathOperator*{\argmax}{argmax}
					\DeclareMathOperator*{\argmin}{argmin}
					\def\lra{{\longrightarrow}}
					\def\ra{{\rightarrow}}
					\def\lla{{\longleftarrow}}
					\def\la{{\leftarrow}}
					\newcommand{\fr}{\frac}
					\newcommand{\vecu}{\underline}
					\newcommand{\rm}{\textrm}
					\newcommand{\bb}{\mathbb}
					\newcommand{\ol}{\overline}
					\newcommand{\t}{^\top}
					\newcommand{\bs}{\boldsymbol}
					\newcommand{\gang}{\;|\;}
					\newcommand{\types}{\;\;\vdash\;}
					\newcommand{\type}{\;:\;}
					\newcommand{\cal}{\mathcal}
				\)
				Just a warm reminder that the further down we get the less sense everything makes :)
			</div>
		</div>

		
		<div class="colourband">
			<h2 id="lcalc">Lambda Calculus</h2>
		</div>

		<div class="cbox">	
			<div class="md-conv">
				### Contents 

				1. [Introduction](#lc-intro)
				2. [Evaluation](#lc-eval)
				3. [Normal Forms](#lc-nf)
				4. [More LC Representations](#lc-more)
				5. [Church Encodings](#lc-church)
			</div>

			<div class="md-conv" id="lc-intro">
				### Introduction

				Lambda calculus (LC) is a formal, mathematical way to represent programming languages. It can be thought of as a purely functional "language", which consist of 
				
				<div class="blue">
					* **Variables** $x$, which are denoted by single lower case letters;
					* **Terms** $M \lra x | (\lambda x \cdot M) | (M\;M)$, in which 
					* $\lambda x \cdot M$ is **abstraction** (and can be thought of as a function)
					* $M \; M$ is **application** (and can be thought of as applying functions)
				</div>
				
				
				In LC, 
				* Application associates **left**: $a\;b\;a = ((a \; b) \; a)$
				* Abstraction associates **right**: $\lambda x \; y \cdot M = (\lambda x \cdot (\lambda y \cdot M))$. (Multiple abstraction can be collapsed)
				* Application **binds stronger** than abstraction: $\lambda a \cdot a \; b \; a = (\lambda a \cdot ((a \; b) \; a))$
				* Brackets are not included if not necessary. 
				* There *is no empty word*. 
				
				An abstraction (lambda) **binds** the variable that comes after it. All non-bound variables are **free**. In 
				$$\lambda a \cdot a \; b \; a$$
				$a$ is a bound variable, $b$ is a free variable, and $a \; b \; a$ is the **scope** of variable $a$.
				
				We can define properties about LC using structural induction (which looks like recursion). For example, let us define the set $FV(X)$ over a term $X$, of the free variables in that term. This can be defined as 
				$$
				\begin{align*}
				FV(x) &= \{x\} & \pod{x \in \textrm{Variables}}\\
				FV(M_1\;M_2) &= FV(M_1) \cup FV(M_2) & \pod{M_1, M_2 \in \textrm{Terms}}\\
				FV(\lambda x \cdot M) &= FV(M) \setminus \{x\} & \pod{M \in \rm{Terms}}
				\end{align*}
				$$
			</div>

			<div class="md-conv" id="lc-eval">
				### Evaluation 

				<div class="blue">

					Define **substitution** of a free variable $x$ by some term $N$, within another term $M$ as $[N/x]M$. 

				</div>
				
				
				You can read $[N/x]$ as "N replaces x".
				
				Some examples are given: 
				
				$$
				\begin{align*}
					[(\lambda x \cdot y) / a] (\lambda m \cdot a\;m) &\implies  (\lambda m \cdot ((\lambda x \cdot y) \; m)). \\
				[b / a] (\lambda x \cdot x \; y) &\implies  (\lambda x \cdot x \; y).\\
				[(\lambda x \cdot m) / a] (\lambda m \cdot a \; m) &\implies (\lambda m \cdot (\lambda x \cdot m) \; m) \pod{*}\\
				&\equiv (\lambda m_1 \cdot (\lambda x \cdot m) \; m_1).
				\end{align*}
				$$

				($*$) Note that we can substitute variables with the same label in, thus creating ambiguous labelling. This can and should be relabelled. 
				
				Formally, the rules for substitute are given 
				* $[N / x] x = N$
				* $[N / x] (M_1 \; M_2) = ([N / x] M_1 \; [N / x] M_2)$
				* $[N / x] (\lambda x \cdot M) = \lambda x \cdot M$ (bound variables **cannot** be substituted)
				* $[N / x] (\lambda y \cdot M) = \lambda y \cdot ([N / x]M)$ if $x \neq y$ and $y \not \in FV(N)$
				
				Substitution is a purely syntactic operation and should be thought of as such. 
				
				Substitution also has **capture avoidance** -- prevent accidental binding of free variables (hint: rename), and **freshness** -- if a variable is not free in $N$ it cannot be reused. 
				
				<div class="blue">

					Define **$\alpha$-equivalence** as the principle that variable *name* doesn't matter, only the **order** and **binding status**.
				</div>
				
				Thus we can rename variables. 

				<div class="blue">

					Define a **contraction**, also called a **$\beta$-reduction** as the following operation:
					$$
					(\lambda x \cdot M)\; N \underset{\beta \textrm{-red}}{\lra} [N / x]M
					$$
					The whole left hand side is called the **$\beta$-redex**.

				</div>
				
				
				
				You can think of this like a function $\lambda x \cdot M$ being passed in $N$ as its parameter $x$, so replace all instances of bound variable $x$ with $N$ in the function body.
				
				* A reduction series is thus a series of contractions 
				* And a **normal form** is when all beta-redexes have been resolved. 
				* An **evaluation** $A \implies B$ or $A \lra^* B$ is a **full reduction** of $A$ to the corresponding **normal form** $B$.
				* A **divergence** is when a formula cannot reach normal form.

				<div class="blue">

					Define eta **$\eta$-reduction** as the following simplification:
					$$
					\lambda x \cdot M \; x \underset{\eta}{\lra} M
					$$
				</div>

				This can be thought of as unwrapping a redundant function call. 
				
				<hr>

				For multiple redexes, we have multiple reduction orders. In some cases, some reduction orders converge whilst others do not. In other cases, some reduction orders are quicker than others. 

				> Try $(\lambda a \; b \cdot b) ((\lambda x \cdot x\;x) (\lambda x \cdot x\; x)) (\lambda x \cdot x)$ yourself. 
				
				Assuming that $M \underset{\beta}{\lra} M'$ and $N \underset{\beta}{\lra} N'$ our three orders are 
				* $(\lambda x \cdot M) N \lra (\lambda x \cdot M) N'$ : **eager evaluation** (reduce argument)
				* $(\lambda x \cdot M) N \lra [N/x]M$ : **lazy evaluation** (reduce function)
				* $(\lambda x \cdot M) N \lra (\lambda x \cdot M') N$ : **optimising evaluation** (reduce function body)
				
				For multiple candidate redexes, we can also reduce them based on their **physical** order. 
				* **Normal order** is reducing from the left 
				* **Applicative order** is reducing from the right 
				
				Normal will *always* converge if possible, whereas applicative may not.
				
				> Try $(\lambda x \cdot (\lambda y \cdot (\lambda z \cdot (x \; z) (y \; z))))(\lambda u \cdot u)(\lambda v \cdot v)$ with both orders. 
			</div>

			<div class="md-conv" id="lc-nf">
				### Normal Forms 

				A normal form (NF) is an expression with **no redexes**. 
				
				...or at least, one may naively think, because it's not so simple. 
				
				Take $a ((\lambda b \cdot b\;b) (\lambda c \cdot c\;c))$. This has a redex, so it's not normal form, right? Said the average eager evaluation fan. The lazy evaluation enjoyer *would* say this is normal form.
				
				Thus we make **head normal form** HNF -- where the head (the first term) is normal form and we don't care about arguments. 
				
				Take $\lambda a \cdot (\lambda b \cdot b) \; a$. This is not in normal form, but the function body wouldn't be optimised in lazy execution. However this is also not HNF, as the head term is reducible. 
				
				Thus we make **weak normal form** WNF -- where function bodies are ignored. 
				
				Recall the definition of terms $M \lra x \;|\; (\lambda x \cdot M) \;|\; (M\;M)$
				
				* **Normal form** is defined as $$M \lra x \gang \lambda x \cdot M \gang M_1 \; M_2 \cdots M_n$$.
				* **HNF** is defined as $$M \lra x \gang \lambda x \cdot  M \gang x\;m_1\;m_2\cdots m_n$$ for any term $m_i$, which doesn't have to be NF.
				* **WNF** is defined as $$M \lra x \gang \lambda x \cdot m \gang x \; M_1 \; M_2 \cdots M_n$$ for any term $m$, and (weak) **normal form** terms $M_i$.
				* **Weak head normal form** (WHNF) is a combination of WNF and HNF, and is the most relaxed. It is defined as $$M \lra x \gang \lambda x \cdot m \gang x \; m_1 \; m_2 \cdots m_n$$ for any term $m, m_i$. i.e. as long as the top level has no redexes, we're good.
				
				<p class="side">
					Try: what normal forms are $a \; ((\lambda b \cdot b\; b)(\lambda c \cdot c \; c))$ in?
				</p>
				<p class="side">
					Try: what normal forms are $\lambda a \cdot a \; ((\lambda b \cdot b\; b)(\lambda c \cdot c \; c))$ in?
				</p>
			
			</div>

			<div class="md-conv" id="lc-more">
				### More Representations of LC 

				Note: none of these are extensions, as in they don't add anything new to the LC that we know. However, with the LC we have we can actually do rather a lot...
				
				#### De Bruijn Representation 
				
				Variables can replace other variables through $\alpha$-equivalence conversion. We need this to avoid name collisions. Rather than using variable names, we can use place values instead of variable names. 
				
				Take $\lambda x \cdot x (\lambda y \cdot x \; y \; x \; (\lambda z \cdot y \; z \; x)) \; x$. We can label which occurrence binds to which abstraction:
				
				<figure>
					<img src="./assets/debruijn.png" alt="" style="max-width: 280px;">
					<figcaption>(Sara Kalvala)</figcaption>
				</figure>
				
				Rather than writing variable names, we just write how many levels up does the variable bind to. Thus this becomes 
				$$
				\lambda\; 1 \;(\lambda \; 2 \; 1 \; 2 (\lambda \; 2 \; 1 \; 3)) \; 1
				$$
				So 1 means binding to the immediate lambda, 2 means one layer up, etc, etc. The lambdas are actually redundant too; we can just use brackets:
				$$
				(1\; (2\;1\;2\;(2\;1\;3))\;1)
				$$
				This is **de Bruijn's representation**.
				
				This has the *minor* problem that using round brackets conflicts with using round brackets to specify order, in the case of $\lambda x\; y \cdot x \; (y \; x)$, this decomposes to $\lambda x \cdot \lambda y \cdot x \; (y\; x) =  (1 ( 2 \; (1 \; 2)))$... except it doesn't because the start of a bracket implies a lambda. 
				
				So we have to mix brackets; $[1\; [2\; (1\; 2)]]$ which is fine since it's just notation, but switching notation can get confusing. 
				
				We also have the second *minor* problem of free variables -- just use an arbitrary string for them (i.e. keep the letters). 
				
				However, free variables are generally **unhelpful** and **bad** and need to be sent off to be **disciplined** and so we avoid having them if we can. 
				
				#### Combinatory Logic 
				
				Certain functions have rather interesting properties. These are the **combinators**, and each is given a letter. 
				
				As far as I remember you don't need to remember the equations but you do need to know what they do.
				
				$$
				\begin{align*}
				I &= \lambda x \cdot x & S &=  \lambda x \cdot \lambda y \cdot \lambda z \cdot x\; z\;(y\;z) \\
				K &= \lambda x \cdot \lambda y \cdot x & B &= \lambda x \cdot \lambda y \cdot \lambda x \cdot x\;(y\;z)\\
				Y &= \lambda f \cdot (\lambda x \cdot f\;(x\;x))(\lambda x \cdot f\;(x\;x)) & T &= \lambda x \cdot \lambda y \cdot y \; x
				\end{align*}
				$$
				
				Note there are no free variables. $I$ is generally called "identity", $K$ is generally called "constant", $T$ is generally called "transpose", and $Y$ is the $Y$ combinator, because it's special. (this comes up really far on) 
				
				It is provable that any combinator can be made using $S$ and $K$. 
				$$
				\begin{align*}
				I &= SKK\\
				B &= S(KS)K\\
				T &= B(SI)K
				\end{align*}
				$$
				($Y$ is complicated but possible.)
				
				
			</div>

			<div class="md-conv" id="lc-church">
				### Church Encodings 
				
				An "extension" (but not really) of lambda calculus, by strategically making functions we can get some... interesting things. Developed by someone surnamed Church.
				
				Take:
				$$
				\begin{align*}
				T &= \lambda x\; y \cdot x\\
				F &= \lambda x \; y \cdot y\\
				IF &= \lambda p \; x \; y \cdot p \; x \; y
				\end{align*}
				$$
				Note how $IF\; T \; a \; b \implies a$, whereas $IF \; F \; a \; b \implies b$. Suspiciously... boolean, no?
				
				Take:
				$$
				\begin{align*}
				A &= \lambda p\; q \cdot p \; q \; p\\
				O &= \lambda p \; q \cdot p \; p \; q\\
				N &= \lambda p \; a \; b \cdot p \; b \; a
				\end{align*}
				$$
				Note how $N \; T \implies F$, and $A \; T \; x \implies x$. Make the derivations yourself :). 
				
				$T, F, IF, A, O, N$ are encodings found by Church for booleans and boolean operations. 
				
				Now take:
				$$
				\begin{align*}
				Add &= \lambda i \; j \; f \; x \cdot (i\; f) \;(j\;f\;x)\\
				0 &= \lambda f \; x \cdot x\\
				1 &= \lambda f \; x \cdot f \; x
				\end{align*}
				$$
				These are Church encodings for numbers. We can try $Add \; 0 \; 1 \implies 1$. We can also add ones to get $ 2 = \lambda f \; x \cdot f \; f \; x$. In fact, this is a consistent pattern. 
				
				Note how $0 \equiv F$, zero is false, but $1 \not \equiv T$, and is not true. We could easily get confused where we are, and so we may want to introduce...
				
				Types.
			</div>
		</div>

		<div class="colourband">
			<h2 id="stlc">Simply Typed Lambda Calculus</h2>
			<p class="subheading">STLC</p>
		</div>
		
		<div class="cbox">
			<div class="md-conv">

				<div class="blue">

					Define a **type environment** as a set $T = x_1:t_1 \cdots x_n:t_n$ (the curly brackets are implied), where $x_i$ is a variable and $t_i$ is its type.
				</div>
				 
				<div class="blue">

					Define a **typing judgment** as a triple of a type environment $T$, a term $M$, and a resulting type $t$, denoted as:
					$$
					T \vdash M : t
					$$
	
				</div>

				<div class="blue">

					Define a **typing derivation** as a tree where nodes are labelled by typing judgments. This is generally done as a form of natural deduction proof tree. 
				</div>
				
				<div class="blue">

					**Simply Typed Lambda Calculus (STLC)** is a language built on LC that has 
				* Variables $x \in \textrm{Variables}$
				* Terms $M := x \gang \lambda x : t \cdot M \gang M \; M$, where $x : t$ means parameter $x$ has type $t$
				* Types $t := o \gang t \ra t$, where $o$ is a generic "object" type, and $\ra$ denotes function types (recall haskell). 
				</div>
				
				
				
				An example typing judgment would thus be 
				$$
				x:o, f:o\ra o\ra o \types (f \; x)\; x \type o
				$$
				
				Now we will be looking at typing rules, to infer the type of a term. 
				
				For a variable $x$ we cannot say much about its type. Thus our typing rule is given 
				$$
				\frac{}{
				T,x:t \types x \type t
				}
				$$
				The **identity** rule.
				
				For a lambda, the typing rule is given 
				$$
				\frac{
				x:t,T  \types M \type t_1
				}{
				T \types \lambda x:t \cdot M \type t \ra t_1
				}
				$$
				The **abstraction** rule. I.e. $x$ is bound to $t$ and so would appear as type $t$ in $M$.
				
				For application, the typing rule is given 
				$$
				\frac{
				\begin{matrix}T \types M_1 \type t_2 \ra t & & T \types M_2 \type t_2\end{matrix}
				}{
				T \types M_1 \; M_2 \type t
				}
				$$
				The **application** rule. $M_1$ must be a function for the application to make sense. 
				
				To construct a type proof/inference tree, we start at the bottom with the statement we want to prove, and build upwards, creating branches as we go. 
				
				We first decompose, without writing in the type environments or the typing, and then fill those in top to bottom afterwards. 
				
				<p class="side">
					
				</p>
				<div class="side">

					***Example:.*** prove $x:o, f:o\ra o\ra o \types (f \; x)\; x \type o$.

					***Answer.*** The typing tree is built from the bottom up, and goes as follows:
				
				$$
				\frac{
				\begin{matrix}
				\dfrac{\begin{matrix}
				\dfrac{}{x:o,f:o\ra o\ra o \types f \type o \ra o \ra o}
				&&
				\dfrac{}{x:o,f:o\ra o\ra o \types x \type o}
				\end{matrix}
				}{x:o,f:o\ra o\ra o \types f \; x : o \ra o 
				}
				&& 
				\dfrac{}{x:o,f:o\ra o\ra o \types x \type o}
				\end{matrix}
				}{
				x:o, f:o\ra o\ra o \types (f \; x)\; x \type o
				}
				$$
				
				This is ... not pleasant to type up.

				If it expands over the page I'm not going to fix it.
				</div>
			</div>

			<div class="side md-conv">
				***Example.*** find the types of $\lambda (f:o\ra o) \cdot \lambda (y:o) \cdot f \; y$ and $\lambda (f:o ) \cdot \lambda y:o \cdot f \; y$.

				***Answer.*** We start by inspection. By inspection, the first one looks alright, and the second one is clearly a type error.
				
				The proof tree for the first one is as follows. Note that for the type environment in the proof tree, if we are not given it it is usually safe to leave it blank if there are no relevant type bindings. 
				
				$$
				\frac{
					\dfrac{
						\dfrac{
							\begin{matrix}
							\dfrac{}{f:o\ra o, y:o \types f \type o \ra o} 
							&&
							\dfrac{}{f:o \ra o, y:o \types y \type o}
							\end{matrix}
						}{
						f:o \ra o, y:o \types f \; y \type o
						}
					}{
					f:o \ra o \types \lambda y:o \cdot f \; y \type o \ra o
					}
				}{
				\types \lambda (f:o\ra o) \cdot \lambda (y:o) \cdot f \; y \type (o \ra o) \ra (o \ra o)
				}
				$$
				
				It is also handy perhaps to use multiple colours.
			</div>

			<div class="md-conv">
				
				With proof trees we can do type analysis (**static semantics**) as well as evaluation (**dynamic semantics**).
				
				We can also have evaluation judgements, $E \types M \implies v$ where $E$ would be our variable scope environment, and $\implies$ is the full reduction series.
				
				Note that the syntax for all of these changes around a bit since there is no one standard.
			</div>
		</div>

		<div class="colourband">
			<h2 id="pcf">PCF Model Language</h2>
			<p class="subheading">"Programmable computable functions"</p>
		</div>

		<div class="cbox">
			\(
\DeclareMathOperator*{\num}{num}
\DeclareMathOperator*{\bool}{bool}
\DeclareMathOperator*{\true}{true}	
\DeclareMathOperator*{\false}{false}	
\let\inst\succ
\DeclareMathOperator*{\succ}{succ}	
\DeclareMathOperator*{\pred}{pred}	
\DeclareMathOperator*{\zeroq}{zero?}	
\DeclareMathOperator*{\if}{if}	
\DeclareMathOperator*{\then}{then}	
\DeclareMathOperator*{\else}{else}	
\DeclareMathOperator*{\fst}{fst}
\DeclareMathOperator*{\snd}{snd}
\DeclareMathOperator*{\let}{let}
\DeclareMathOperator*{\inn}{in}
\DeclareMathOperator*{\and}{and}
\DeclareMathOperator*{\rec}{rec}
			\)

			<div class="md-conv">
				### Contents 

				1. [PCF](#pcf-intro)
				2. [PCF Recursion](#pcf-rec)
				3. [Structural Operational Semantics](#pcf-sos)
			</div>

			<div class="md-conv" id="pcf-intro">
				### PCF

				PCF incorporates church encodings.
				
				<div class="blue">

					The **PCF Model Language** is defined as having:
				
					Types:
					$$
					t := \num \gang \bool \gang t \ra t
					$$
					Expressions:
					$$
					\begin{align*}
					M :&= \true \gang \false \gang 0 \gang 1 \gang 2 \gang \cdots \\
					&\gang \succ(M) \gang \pred(M) \gang \zeroq(M) \gang M+M \gang M\times M\\
					&\gang \if M \then M \else M\\
					&\gang x \gang \lambda x:t \cdot M \gang M \; M
					\end{align*}
					$$
					Where 
					* numbers are in set/type $\num$ (often shortened to $n$ in writing in reality), and booleans are in set/type $\bool$ (often shorteend to $b$)
					* $\succ(M): \num\ra\num$ increments, and $\pred(M):\num \ra \num$ decrements. 
					* $\zeroq(M): \num \ra \bool$ is a zero check. 
					
					We can extend expressions with **pairs**:
					$$
					\begin{align*}
					t :&= \cdots \gang t * t\\
					M :&= \cdots \gang \langle M,M\rangle \gang \fst(M) \gang \snd(M)
					\end{align*}
					$$
					* e.g. $\langle 3, \true \rangle : \num * \bool$
					
					We can extend expressions with **let-declarations**:
					$$
					\begin{align*}
					M :&= \cdots \gang \let d \inn M\\
					d :&= (x = M) \gang d \and d
					\end{align*}
					$$
					Where
					* we have a new *declaration* syntax system.
					* e.g. $\let x = 3 \in x + 2$
				</div>
				
				For an if statement $\if M_1 \then M_2 \else M_3$ we can evaluate it (dynamic judgment) as follows:
				$$
				\frac{
					\begin{matrix}E \types M_1 \implies \true && E \types M_2 \implies v\end{matrix}
				}{
				E \types \if M_1 \then M_2 \else M_3 \implies v
				}
				$$
				The static type judgment goes:
				$$
				\frac{
					\begin{matrix}
					T \types M_1 \type bool &&
					T \types M_2 \type t &&
					T \types M_3 \type t
					\end{matrix}
				}{
				T \types \if M_1 \then M_2 \else M_3 \type t
				}
				$$
				
				For a zero check $\zeroq(M)$, to judge it dynamically we need to use a **meta-predicate**, as we need extra conditions to be met for the true case:
				$$
				\begin{array}{cc}
				\dfrac{ E \types M \implies o
				}{ E \types \zeroq(M) \implies \true
				} &
				v \neq 0\dfrac{E \types M \implies v
				}{E \types \zeroq(M) \implies \false
				}
				\end{array}
				$$
				The typing otoh is straight forward. 
				
				Declarations are trickier, and to do them properly we'd have to cheat somewhat, by resolving to a **new environment** rather than a value or a type. 
				$$
				\dfrac{E \types M \implies v}
				{E \types (x=M) \implies E;(x,v)}
				$$
				i.e. for a declaration $(x=M)$ we append the assigned value of $x$ into the environment. Thus the let statement goes 
				$$
				\dfrac{
				\begin{matrix}
				E \types d \implies E'
				&&
				E;E' \types M \implies v
				\end{matrix}
				}{
				E \types \let d \inn M \implies v
				}
				$$
				
				The static judgements go 
				$$
				\dfrac{T \types M \type t
				}{T \types (x=M) \type T;x:t
				}
				$$
				$$
				\dfrac{
				\begin{matrix}T \types d \type T' && T' \types M \type t\end{matrix}
				}{
				T;T' \types \let d \inn M \type t}
				$$
				
				Constants have judgement $T \vdash c : t$ (given $c \in t$)
				
				Variables have judgement $T \vdash x : t$ (given $x:t \in T$)
				
				Pairs have judgement 
				$$
				\dfrac{
					\begin{matrix}
					T \types e_1 \type t_1 && T \types e_2 \type T_2
					\end{matrix}
				}{T \types \langle e_1, e_2 \rangle \type t_1 * t_2}
				$$
				
				Conditionals have judgement 
				$$
				\dfrac{
				\begin{matrix}
				T \types b \type \bool && T \types e_1 \type t && T \types e_2 \type t
				\end{matrix}
				}{T \types \if b \then e_1 \else e_2 \type t}
				$$
				
				Functions have judgement 
				$$
				\dfrac{
				T, x:t_1 \types e \type t_2
				}{
				T \types \lambda x:t_1 \cdot e\type t_1 \ra t_2} 
				$$
				
				***Subject reduction.*** If $E \vdash e \implies v$ ($e$ evals to $v$ in the env) and $E \in T$ (the env has types) and $T \vdash e : t$ (e is type t), then $v \in t$ ($v$ is of type $t$)
				
				Similarly if $E \vdash d \implies E'$ and $E \in T$ and $T \vdash d : T'$. then $E' \in T'$.
			</div>

			<div class="md-conv" id="pcf-rec">
				### Recursion

				***Recursion.*** For an example of recursion, read this sentence again.

				Take a factorial function: 
				* `fac 0 = 1`
				* `fac n = n * fac (n-1)`
				
				In PCF, we may be temped to write 
				$$
				\begin{align*}
				f = \lambda n:\num \cdot \if \zeroq (n) \then \succ (n) \else n \times f(\pred(n))
				\end{align*}
				$$
				But $f$ is technically a free variable and thus it's not so trivial in regular PCF. 
				
				We can bind $f$: $\lambda f \cdot  \lambda n:\num \cdot \if \zeroq (n) \then \succ (n) \else n \times f(\pred(n))$, but finding the $f$ to pass in is difficult.
				
				This is where the **Y-combinator** comes in. It's defined above, but we can redefine it as 
				$$
				Y = ZZ : Z = \lambda x\; z \cdot x \; (z\; z\; x)
				$$
				We can do $Y (\lambda f \cdot \cdots)$ to "layer" our recursion. 
				
				Thus we write the whole thing using a mu $\mu f$:
				$$
				\mu (f:\num\ra\num) \cdot  \lambda (n:\num) \cdot \if \zeroq(n) \then \succ(n) \else n \times f(\pred(n))
				$$
				
				Define the "**recursive wrapper function**" as $\mu f:t \cdot M$, with the reduction 
				$$
				\mu f:t \cdot M \lra [\mu f:t \cdot M / f] M
				$$
				
				The evaluation step is 
				$$
				\frac{
				\types [\mu f:t \cdot M / f] M \implies v}{
				\types \mu f:t \cdot M \implies v}
				$$
				
				Recursion can make infinite type trees, which break our eager-evaluation typing system.
				
				We can define some syntactic sugar for defining things:
				* Functions $f(x,y) = \cdots$
				* Recursions $\let \rec f(\cdots) = \cdots$
				* And declarations $\let x=M;; \cdots$
				This syntax comes from ocaml.
			</div>

			<div class="md-conv" id="pcf-sos">
				### Structural Operational Semantics

				**Natural semantics** describe reductions to normal form.

				**Structural semantics** describes a **single reduction step** - a small step. "Small step semantics".
				
				Here we're constructing proof trees for a single step. 
				
				We do this because the double arrow natural semantics don't specify execution order, whilst single arrow small steps do.
				
				Small steps have some **axiom**s/fundamental rules:
				$$
				\begin{align*}
				n_0 + n_1 &\lra n &\iff n=n_0+n_1\\
				n_0=n_1 & \lra \true &\iff n_0=n_1\\
				n_0=n_1 &\lra \false &\iff n+0 \neq n_1\\
				\if \true \then e_0 \else e_1 &\lra e_0\\
				\if \false \then e_0 \else e_1 & \lra e_1\\
				(\lambda x:t \cdot  e_0)\; e_1 &\lra [e_1/x]e_0\\
				\mu x:t \cdot e &\lra [\mu x:t \cdot e / x]e &\pod{\textrm{Single recursion}}
				\end{align*}
				$$
				
				And we have inductive cases that need to be propagated, such as if:
				$$
				\frac{
				e \lra e'
				}{
				\if e \then e_0 \else e_1 \lra \if e' \then e_0 \else e_1
				}
				$$
				
				We also say operations resolve left hand first:
				$$
				\begin{matrix}
				\dfrac{e \lra e'}{e+e_0 \lra e'+e_0} && \dfrac{e \lra e'}{n + e \lra n+e'} \\ 
				\dfrac{e \lra e'}{e = e_0 \lra e' = e_0} && \dfrac{e \lra e'}{n = e \lra n = e'}
				\end{matrix}
				$$
				And application is done lazily (**call-by-name PCF**)
				$$
				\frac{e \lra e'}{e\; e_0 \lra e' \; e_0}
				$$
				
				We can also do call by value instead. 
			</div>
		</div>

		<div class="colourband">
			<h2 id="miniml">MiniML</h2>

			
		</div>

		<div class="cbox">

			<div class="md-conv">
				### Contents 

				1. [MiniML](#miniml-intro)
				1. [Lists and Type Quantification](#miniml-lists)
				1. [Hindley Damas Milner Type Inference](#miniml-hdma)
				1. [Cardelli Type Judgement](#cardelli)
			</div>
			<div class="md-conv" id="miniml-intro">
				### MiniML

				Whilst PCF has explicit types, some people may prefer implict types and type inference.
				
				This is exactly what MiniML does.
				
				It lets us perform type-level operations in our proof trees and thus do type inference. We have the following type operations (for which we take out the middle because it's not important, apparently):
				$$
				\begin{matrix}
				\dfrac{}{T, :t \types \type t}id && 
				\dfrac{\begin{matrix}T \types \type t_1 & T \types \type t_2\end{matrix}}{T \types \type t_1 * t_2}pair  \\ 
				\dfrac{T, :t_1 \types \type t_2}{T \types \type t_1 \ra t_2}abs &&
				\dfrac{T \types \type t_1 * t_2}{T \types \type t_1}pair^{-1}  \\ 
				\dfrac{\begin{matrix}T\types\type t_1 \ra t_2 & T \types\type t_1\end{matrix}}{T \types \type t_2}appl &&
				\dfrac{T \type\types t_1 * t_2}{T \type\types t_2}pair^{-1}
				\end{matrix}
				$$
				Type annotations can behave like propositional logic statements, or vice-versa.
				
				For example if we take the fact that $(A \land B) \implies C \equiv A \implies (B \implies C)$, then we can say $f\;\langle x, y\rangle \equiv (f \; x) \; y$. I.e. **currying functions**.
				
				<div class="blue">

					**MiniML** is a language that has type polymorphism, type inference and type checking. It makes use of **type variables** for unknown types. 
				
					It has the following expression $e$ syntax:
					$$
					\DeclareMathOperator*{\fn}{fn}
					\begin{align*}
					e :&= c_\bool \gang c_\num \gang \langle e_1, e_2\rangle \gang \fst e \gang \snd e \\
					&\gang e_1\; e_2 \gang \fn x \cdot e \gang \let d \inn e \gang \if e \then e_0 \else e_1
					\end{align*}
					$$
					We use $\fn$ rather than $\lambda$ to denote that it's implicitly typed. $c_\bool$ and $c_\num$ are boolean and numeric constants. 
					
					It has the following type syntax:
					$$
					t := \num \gang \bool \gang t_1*t_2\gang t_1 \ra t_2 \gang \alpha, \beta, \gamma, \cdots
					$$
					Note that tau $\tau$ may be used for type, but since `\tau` is a whole three letters more, I'll mostly be using $t$. $\alpha, \beta, \gamma$ are **type variables**. 
				</div>
				
				
				We can find the types of some terms by just inspection. 
				
				E.g. $\fn f \cdot \fn x \cdot f\;(f\;x)$. 
				
				We can see that this depends on the type of $x$, so give it a type variable $x:\alpha$. $f$ must be an $\alpha \ra \alpha$ for $f\; x$ to work, and which also makes $f\;(f\;x)$ work. Thus the expression should be typed $(\alpha \ra \alpha) \ra \alpha \ra \alpha$.
				
				E.g. $\fn x \cdot \fn y \cdot \fn z \cdot \if (\fst x = \fst y) \then ((\snd x) + z) \else ((\snd y) + z)$.
				
				We know $x, y$ must be pairs with the first type matching: $x:\alpha * ?,\; y:\alpha * ?$. We know due to the $+$ that $z$ and the second elements of $x, y$ must be numbers. Thus $x, y : \alpha * \num, z : \num$. Thus the whole function should be typed $(\alpha * \num) \ra (\alpha * \num) \ra \num \ra \num$.
				
				We can also systematically find types of things, by 
				1. Building an inference tree 
				2. Putting in temporary type variables 
				3. Make type inference equations with those variables and resolve down.
				
				The abstraction rule is
				$$
				\frac{T;x:t_2 \types e \type t_3}{T \types \fn x \cdot e \type t_1} \implies t_1 \sim t_2 \ra t_3
				$$
				The application rule is 
				$$
				\frac{\begin{matrix}T \types e_1 \type t_2 && T \types e_2 \type e_3\end{matrix}}{T \types e_1 \; e_2 \type t_1}\implies t_2 \sim t_3 \ra t_1
				$$

				<hr>
				
				***Example.*** Type $\types \fn f \cdot \fn x \cdot f\;(f\;x)$.
				
				***Answer.*** The tree is constructed 
				$$
				\frac{
					\dfrac{
						\dfrac{
							\begin{matrix}
							\dfrac{}{T \types f \type t_4}(4) &&
							\dfrac{
								\begin{matrix}
								\dfrac{}{T \types f \type t_6}(6) && \dfrac{}{T \types x \type t_7} (7)
								\end{matrix}
							}{
							T \types f \; x \type t_5
							}(5)
							\end{matrix}
						}{
						T = f:\tau_1, x:\tau_2 \types f\;(f\;x) \type t_3
						}(3)
					}{
					f:\tau_1 \types \fn x \cdot f \; (f \; x) \type t_2
					}(2)
				}{
				\types \fn f \cdot \fn x \cdot f\;(f\;x) \type t_1
				}(1)
				$$
				
				The constraints are thus 
				$$
				\begin{matrix}
				\tau_1 \sim t_4 \pod{4} & t_6 \sim t_7 \ra t_5 \pod{5} \\ 
				\tau_1 \sim t_6 \pod{6} & t_4 \sim t_5 \ra t_3 \pod{3}\\
				\tau_2 \sim t_7 \pod{7} & t_2 \sim \tau_2 \ra t_3 \pod{2} \\ 
				& t_1 \sim \tau_1 \ra t_2 \pod{1}
				\end{matrix}
				$$
				And our goal is to find $t_1$.
				$$
				\implies 
				\begin{matrix}
				\tau_1 \sim \tau_2 \ra t_5 \\ \tau_1 \sim t_5 \ra t_3 \\ t_2 \sim \tau_2 \ra t_3 \\ t_1 \sim \tau_1 \ra t_2
				\end{matrix}
				\implies \tau_2 \sim t_5 \land t_5 \sim t_3 \implies \tau_1 \sim \tau_2 \ra \tau_2
				$$
				$$
				\implies 
				\begin{matrix}
				\tau_1 \sim \tau_2 \ra \tau_2 \\ \tau_1 \sim \tau_2 \ra \tau_2 \\ t_2 \sim \tau_2 \ra \tau_2 \\ t_1 \sim \tau_1 \ra t_2
				\end{matrix} \implies t_1 \sim \tau_1 \ra (\tau_2 \ra \tau_2) \implies t_1 \sim (\tau_2 \ra \tau_2) \ra (\tau_2 \ra \tau_2)
				$$
				Thus $t_1 \sim (\tau_2 \ra \tau_2) \ra \tau_2 \ra \tau_2$ via currying.
				

				<hr>
			</div>

			<div class="md-conv" id="miniml-lists">
				#### Lists and Type Quantification 

				Since we have polymorphism, maybe we want to store lists. 
				
				<div class="blue">

					Lists in functional languages tend to be linked lists because it's easiest to write them recursively. We modify MiniML to include 
				
					$$
					\DeclareMathOperator*{\nil}{nil}
					\DeclareMathOperator*{\hd}{hd}
					\DeclareMathOperator*{\tl}{tl}
					\DeclareMathOperator*{\len}{len}
					\DeclareMathOperator*{\null}{null}
					\DeclareMathOperator*{\nth}{nth}
					\DeclareMathOperator*{\list}{list}
					\begin{align*}
					e :&= \cdots \gang \nil \gang e_1::e_2 \gang \hd e \gang \tl e \gang \len e \gang \null e \gang \nth e\\
					t :&= \cdots \gang t \list
					\end{align*}
					$$
					
					Where $\nil$ is the empty list `[]`, $::$ is the "cons" operator, which links the previous element to the list, hd is head (first elem), tl is tail (everything else), len is length, null is an is-empty check, and nth is index. 
				</div>

				
				
				An example of a list is $a :: b::c::d::\nil$. We can also write this with syntax sugar as $[a,b,c,d]$.

				Lists are homogeneously typed.

				* Numbers: $ 3::4::5::6:: \nil \type \num \list$
				* Pairs: $\langle 3,\true\rangle :: \langle4,\false\rangle :: \nil \type (\num \ast \bool) \list$.
				
				Now we get to the difficult thing of typing $\nil$. It isn't any one type, because it appears at the end of all lists, so we may have to include ... *universal type quantification?*
				$$
				\begin{align*}
				\nil &\type \forall \alpha \cdot \alpha \list,\\
				(::) &\type \forall \alpha \cdot \alpha * \alpha \list \ra \alpha \list.
				\end{align*}
				$$
				This is different from before in that beforehand we had some slightly vague generic type thing with type variables. Now, we're explicitly using quantification representation.

				
				\begin{align}
				\hd &\type \forall \alpha \cdot \alpha \list \ra \alpha\\
				\tl &\type \forall \alpha \cdot \alpha\list \ra \alpha \list \\
				\len &\type \forall \alpha \cdot \alpha\list \ra \num \\
				\null &\type \forall \alpha \cdot \alpha\list \ra \bool \\
				\nth &\type \forall \alpha \cdot \alpha \list \ra \alpha
				\end{align}
				
				
				We can do something similar with pairs 
				$$
				\begin{align*}
				\fst &\type \forall \alpha \beta \cdot \alpha * \beta \ra \alpha \\
				\snd &\type \forall \alpha \beta \cdot \alpha * \beta \ra \beta \\
				\langle\;,\;\rangle &\type \forall \alpha \beta \cdot \alpha \ra \beta \ra \alpha * \beta 
				\end{align*}
				$$
				
				Quantified types must be **instantiated explicitly**. We denote "is instantiated to" using the symbol $\inst$.
				
				For example, $\forall \alpha \cdot \alpha \ra \alpha \inst \num \ra \num$.
				
				Formally, a quantified type expression $\sigma \inst \tau$ an explicit type expression. But only if 
				* $\sigma$ is of the form $\forall t \cdot \sigma'$.
				* And $\tau$ is of the form $[t_x / t]\sigma$ where $t_x$ is the explicit type we instantiate to.
				
				We can now construct our proof trees using these. 

				<hr>
				
				***Example.*** Type $\fn l_1 \cdot \fn l_2 \cdot (\hd l_1 + 2) :: (\tl l_2)$.
				
				***Answer.*** Yes, this tree is very wide. 
				
				$$
				\dfrac{
					\dfrac{
					\begin{matrix}
					\dfrac{}{T,(::):\forall \alpha \cdot \alpha * \alpha \list \ra \alpha \list \types (::) \type t_5}
					&&
					\dfrac{
						\begin{matrix}
						\dfrac{}{T \types (+) \type \num\ra\num\ra\num} &&
						\dfrac{\begin{matrix}
						\dfrac{}
						{T, \hd:\forall \alpha \cdot \alpha\list \ra \alpha \types \hd \type t_{10}} &&
						\dfrac{}{T \types l_1 \type t_9}
						\end{matrix}}
						{T \types \hd l_1 : t_8} &&
						\dfrac{}{T \types 2 \type \num}
						\end{matrix}
					}{T \types \hd l_1 + 2 \type t_3}
					&&
					\dfrac{
						\begin{matrix}
						\dfrac{}{T, \forall \alpha \cdot \alpha\list \ra \alpha\list \types \tl \type t_6}
						&&
						\dfrac{}{T \types l_2 \type t_7}
						\end{matrix}
					}{T \types \tl l_2 \type t_4}
					\end{matrix}
					}{
					T = l_1:s_1,l_2:s_2 \types (\hd l_1 + 2) :: (\tl l_2) \type t_1
					} appl
				}{
				\types \fn l_1 \cdot \fn l_2 \cdot (\hd l_1 + 2) ::(\tl l_2) \types t_0
				}abs \times 2
				$$
				
				Thus (let $n$ stand for $\num$)
				$$
				\begin{align*}
				t_0 &\sim s_1 \ra s_2 \ra t_1 \pod{1}\\
				t_5 &\sim t_3 \ra t_4 \ra t_1 \pod{2} & t_5 &\sim \forall \alpha\cdot \alpha * \alpha\list \ra \alpha\list \pod{3}\\
				t_6 &\sim t_7 \ra t_4 \pod{4} & t_6 &\sim \forall \alpha \cdot \alpha\list \ra \alpha\list \pod{5}\\
				t_7 &\sim s_2 \pod{6}\\
				n\ra n\ra n&\sim t_8 \ra n \ra t_3 \pod{7}\\
				t_9 &\sim s_1 \pod{8}\\
				t_{10} &\sim t_9 \ra t_8 \pod{9} & t_{10} &\sim \forall \alpha \cdot \alpha\list \ra \alpha \pod{10}
				\end{align*}
				$$
				Start collapsing. 
				
				You should get something like $\type \num\list \ra \num\list \ra \num\list$.
				
				No I'm not writing the working. This has got to be some of the worst latex that I've had to write.
				
				<hr>
				
			</div>

			<div class="md-conv" id="miniml-hdma">
				#### Hindley Damas Milner Type Inference

				A **Type Scheme** is an expression with quantifiers. Free type variables are unbound, and quantification binds those variables. This process is called **closing**. 
				
				For a type expr with free vars $\sigma$, denote closing as $CL_T(\sigma) = \forall \alpha_1 \cdots \alpha_n \cdot \sigma$, for free variables $\alpha_1 \cdots \alpha_n$ in the expression that do not occur in $T$.
				
				The closure of a type environment is the closure of all assignments. 
				
				This allows us to type more difficult things like $\let f = \fn x \cdot x \in \langle f \true, f \nil\rangle$. We can declare and close in our let rule. 
				
				$$
				\dfrac{
				\begin{matrix}
				\dfrac{}{T \types \fn x \cdot x \type \alpha\ra \alpha} &&
				\dfrac{
				 \begin{matrix}
				 T,f:\forall \alpha \cdot \alpha \ra \alpha \types f \true \type \bool&&  T,f:\forall \alpha \cdot \alpha \ra \alpha \types f \nil \type \nil
				 \end{matrix}
				}{
				T,f:\forall \alpha \cdot \alpha \ra \alpha \types \langle f \true , f \nil \rangle \type \bool * \nil
				}
				\end{matrix}
				}{
				T \types \let f = \fn x \cdot x \in \langle f \true , f \nil \rangle \type \bool * \nil
				}
				$$
				
				Confused at this point? Don't worry, I am too.
				
				An algorithm for type inference.. The approach goes: 
				
				1. Build a tree of the term 
				2. To each occurence of a constant, set an arbitrary instance of its universally quantified type 
				3. Build a set of constants based on typing rules 
				4. Solve equations by replacing variables to **unify** types. 
				
				Unification is like 
				</div>

			<div>
				<p>
					\[ \begin{array}{c|ccccc:c}
				& \num & \bool & t_1 * t_2 & t_1 \list & t_1\ra t_2 & \alpha \\\hline
				\num & \checkmark & \times & \times & \times & \times & (\num) \la \alpha \\
				\bool & \times & \checkmark & \times & \times & \times & (\bool) \la \alpha \\ 
				t_3 *t_4 & \times & \times & \begin{matrix}t_3\sim t_1 \\ t_4\sim t_2\end{matrix} & \times & \times & ((t_3)*(t_4)) \la \alpha \\
				t_2 \list & \times & \times & \times & t_1\sim t_2 &\times & ((t_2) \list) \la \alpha \\
				t_3 \ra t_4 & \times & \times & \times & \times & \begin{matrix}t_3\sim t_1 \\ t_4\sim t_2\end{matrix} & ((t_3) \ra (t_4))\la \alpha \\ \hdashline
				\beta & \textrm{same} & \textrm{system} & \textrm{as the} & \alpha & \textrm{column}
				\end{array} \]
				</p>
			</div>
				
				
			<div class="md-conv">
				For the type variables $\alpha, \beta$, what the thing denotes is take alpha and put it into the set/bag of all elements of that type. i.e. $(\num) \la\alpha$ means put alpha into the set of number types. 
				
				We need to also check that the type variables do not exist already. Otherwise it's a bad unification, such as trying to do $\lambda x \cdot x\;x$. If this happens we stop with a type error. 
				
				As any closed MiniML statement gives a type that is a propositional tautology.
				
				
			</div>

			<div class="md-conv" id="cardelli">
				### Cardelli Type Judgement 
				
				Luca Cardelli provides some more notation (yay...) for defining whether or not a statement, or a type environment is well-formed. 
				
				* $\Gamma \types M \type A$ reads "M has type A in type environment $\Gamma$".
				* $\Gamma \types A$ reads "A is a well formed type expression [in $\Gamma$]"
				* $\Gamma \types \diamond$ reads "The environment $\Gamma$ is well-formed"
				* $\varnothing \types \diamond$ reads "The empty environment is well formed", and is a fundamental rule. 
				
				So what we have to do now is to prove that the type system is well formed as well as type our statement. Are you screaming yet? (:
				
				For PCF, we can have the following well-formed type rules 
				$$
				\frac{}{\varnothing \types \diamond} \pod{\textrm{Empty environment}}
				$$
				$$
				\frac{
				\begin{matrix}\Gamma \types A && x \not\in \rm{domain}(\Gamma)\end{matrix}
				}{
				\Gamma, x:A \types \diamond 
				} \pod{\textrm{A new variable }x \textrm{ is well typed}}
				$$
				$$
				\frac{
				\begin{matrix}\Gamma \types \diamond && K \in \textrm{Basic types}\end{matrix}
				}{
				\Gamma \types K
				} \pod{\textrm{Type Constants}}
				$$
				$$
				\frac{
				\begin{matrix}\Gamma \types A && \Gamma \types B\end{matrix}
				}{
				\Gamma \types A \ra B
				} \pod{\textrm{arrow type }\ra}
				$$
				$$
				\frac{
				\Gamma,x:A \types \diamond
				}{
				\Gamma, x:A \types x:A
				} \pod{\textrm{Identity typing}}
				$$
				$$
				\frac{\Gamma,x:A \types M \type B}{\Gamma \types \lambda x:A \cdot M \type A \ra B} \pod{\textrm{Function typing}}
				$$
				$$
				\frac{
				\begin{matrix}\Gamma \types M \type A \ra B  && \Gamma \types N \type A \end{matrix}
				}{
				\Gamma \types M \; N \type B
				} \pod{\textrm{Apply typing}}
				$$
				The PCF type system is also called $F_1$, by the way. Why? We shall never know.
				
				Whilst this add precision this also adds a lot of pedantry, like 
				$$
				\frac{\Gamma \types \diamond}{\Gamma \types 0 \type \num}
				$$
				Well thanks for telling me zero is a number, I would've never known. But unfortunately we need this to do our next thing... $F_2$?
			</div>
		</div>

		<div class="colourband">
			<h2 id="plc">Polymorphic Lambda Calculus</h2>
		</div>

		<div class="cbox">
			<div class="md-conv">
				### Contents 

				1. [PLC](#plc-intro)
				1. [Records and Subtypes](#records)
			</div>

			<div class="md-conv" id="plc-intro">
				### PLC 

				Let's move back to PCF, with its explicit typing, but now we add in quantification.

				In MiniML, quantifiers **cannot** appear in terms, and can only exist in type propositions. In PLC, we have term-based quantified type hinting.
				
				Introduce 
				$$
				e := \cdots \gang \Lambda V \cdot e \gang e[t]
				$$
				Where $V$ is a type variable, and big lambda $\Lambda$ is used for universal quantification. The $e[t]$ syntax instantiates a function to a type, like using **generics**. 
				
				Introduce 
				$$
				t := \num \gang \bool \gang t_1 \ra t_2 \gang \cdots \gang \forall V \cdot t
				$$
				**Note** that $\num$ is often rendered $\textrm{int}$ but that would clash with another MathJax command and so I'll use $\num$ which is already working perfectly fine. 
				
				We have some more proof rules 
				$$
				\begin{matrix}
				\textrm{Over terms} && \textrm{Over types} \\\\
				\dfrac{\Gamma,x:t \types e \type t_2}{\Gamma \types \lambda x:t_1\cdot e \type t_1 \ra t_2} && \dfrac{\Gamma, V \types e \type t}{\Gamma \types \Lambda V \cdot e \type \forall V \cdot t} \\\\ 
				\dfrac{
				\begin{matrix}\Gamma \types e_1 \type t_1\ra t_2 && \Gamma \types e_2 \type t_1\end{matrix}
				}{
				\Gamma \types e_1 \; e_2 \type t_2
				} &&
				\dfrac{
				\Gamma \types e \type \forall V \cdot t_2
				}{\Gamma \types e[t_1] \type t_2[t_1/V]}
				\end{matrix}
				$$
				
				PLC makes type checking an undecidable problem. But they're still useful; 
				
				Previously in church encodings we found out that 0 equaled false. We separated them out into $\num, \bool$ sets, but this relatively is a bit of a hack. 
				
				With PLC, we can write type expressions for all of the types:
				$$
				\begin{align*}
				\bool &= \forall \alpha \cdot \alpha \ra \alpha \ra \alpha & \num &= \forall \alpha \cdot (\alpha \ra \alpha) \ra (\alpha\ra \alpha)\\\\
				\false &= \Lambda \alpha \cdot \lambda(x:\alpha, y:\alpha)\cdot y & 0 &= \Lambda \alpha \cdot \lambda (f:\alpha\ra\alpha, x:a) \cdot x\\
				\true &= \Lambda \alpha \cdot \lambda(x:\alpha, y : \alpha) \cdot x & 1 &= \Lambda \alpha \cdot \lambda (f:\alpha\ra \alpha, x:\alpha) \cdot f \; x
				\end{align*}
				$$
				
				This is called **System F**.
				
				<hr>
				
				***Example.*** Prove that $\true$ is type $\bool$
				
				***Answer.***
				$$
				\dfrac{
					\dfrac{
						\alpha, x:\alpha, y:\alpha \types x \type \alpha
					}{
					\alpha \types \lambda x:\alpha \cdot \lambda y \cdot \alpha \cdot x \type \alpha \ra \alpha \ra \alpha
					}
				}{
				\types \Lambda \alpha \cdot \lambda x:\alpha \cdot \lambda y:\alpha \cdot x \type \forall \alpha \cdot \alpha \ra \alpha \ra \alpha
				}
				$$
				
				<hr>
				
				When using PLC, we *must* instantiate the type every single time we use it. For example, take the IF function: 
				$$
				IF = \Lambda t \cdot \lambda b:\bool \cdot \lambda x_1:t \cdot \lambda x_2 : t \cdot b[t] \; x_1 \; x_2
				$$
				Because $b$ is a boolean of type $\forall \alpha \cdot \alpha \ra \alpha \ra \alpha$, we must instantiate this using the type $t$ to be able to use it with $x$s of type $t$. 
				
				This can make some... rather clunky writings. Take $\fn l_1 \cdot \fn l_2 \cdot (\hd l_1 + 2) :: (\tl l_2)$ from before. In PLC, this would become 
				$$
				\lambda(l_1:\num\list)\cdot \lambda(l_2 :\num\list) \cdot (\hd[\num]\; l_1 + 2 ) ::_{[\num]} (\tl[\num] l_2)
				$$
				Yes, you even have to instantiate the cons. 
			</div>

			<div class="md-conv" id="records">
				### Records and Subtypes

				We have lists, but we want something more flexible -- the **record structure**.
				
				A record is made of key/value pairs, like a dictionary, a struct, a table (lua), etc. And records are very, very powerful. 
				
				Define **Record** notation and types as:
				$$
				\begin{align*}
				e :&= \cdots \gang \{l_1 =e_1, \cdots ,l_n=e_n\} \gang e@l\\
				t :&= \cdots \gang \{l_1:T_1, \cdots, l_n:T_n\}
				\end{align*}
				$$
				Where $e@l$ means retrieve the value at key $l$ of record $e$. This is called **projection**.
				
				Note how records are not homogenous, every entry has its own type. 
				
				An example: $a = \{\textrm{one} = 4, \textrm{two} = \false\}$. We can get 4 by doing $a@\textrm{one}$.
				
				The record typing rule is as follows:
				$$
				\dfrac{
				\Gamma \types e_i \type t_i \textrm{ for each }i
				}{
				\Gamma \types \{l_1 = e_1, \cdots, l_n=e_n\} \type \{l_1:t_1 \cdots l_n :t_n\} 
				}
				$$
				The projection typing rule is:
				$$
				\dfrac{
				\Gamma \types e \type \{l_1:t_1 \cdots l_n:t_n\}
				}{
				\Gamma \types e@l_i \type t_i
				}
				$$
				
				We run into some problems with some typing things with records. e.g. take $f = \lambda x \cdot x@\textrm{one}$. We want to know the type of $f$ and $x$. 
				
				* Naturally, $x$ is any record with a field named "one".
				* Thus $f$ should take a record with a field "one" of any type... $\Lambda t \cdot \{\textrm{one} : t\} \ra t$ ... right?
				
				Well what if we pass in $\{\textrm{one}: 13, \textrm{two}: \true\}$? This would not be allowed under $f$'s type, but ostensibly should be. 
				
				Thus, we introduce **subtyping**. 
				
				Denote **subtyping** as $S \lt: T$, read "$S$ is a subtype of $T$", if $S$ can be passed in anywhere where type $T$ is required. 
				
				For example, $\{\textrm{one}:\num, \textrm{two}:\bool\} \lt: \{\textrm{one}:\num\}$.
				
				Given subtyping we introduce the **subsumption rule**:
				$$
				\newcommand{\subtype}{\;\lt:\;}
				\dfrac{
				\begin{matrix}\Gamma \types e \type S && \Gamma \types S \subtype T\end{matrix}
				}{
				\Gamma \types e \type T
				}
				$$
				We can now introduce the **sub-record rule**:
				$$
				\dfrac{
				\begin{matrix}
				\{l_1:T_1 \cdots l_n:T_n\} \subseteq \{k_1:S_1 \cdots k_m:S_m\} && k_i = l_j \implies S_i \subtype T_j
				\end{matrix}
				}{
				\Gamma \types \{k_1:S_1 \cdots k_m:S_m\} \subtype \{l_1:T_1 \cdots l_n:T_n\}
				}
				$$
				And introduce the **sub-function rule**:
				$$
				\dfrac{
				\begin{matrix}\Gamma \types T_1 \subtype S_1 && \Gamma \types S_2 \subtype T_2\end{matrix}
				}{
				\Gamma \types S_1 \ra S_2 \subtype T_1 \ra T_2
				}
				$$
				
				**Every type is a subtype of itself**, so perhaps we could be doing... $\leq:$? But no. Let's not change notation as we've done that enough already. 
				
				Subsumption rule however can lead to infinite loops and non-deterministic proof trees, as:
				$$
				\frac{
					\dfrac{
						\begin{matrix}\dfrac{\vdots}{\Gamma,x:s \types e \type t }
						&& \dfrac{}{t \subtype t'} \end{matrix}
					}{
					\Gamma, x:s \types e \type t'
					}
				}{
				\Gamma \types \lambda x:s \cdot e \type s \ra t'
				}
				$$
				Note how we can loop the left a forever. 
				
				So rather than this we should integrate subsumption into other rules. 
				
				Replacing **application**:
				$$
				\dfrac{
				\begin{matrix}
				\Gamma \types e_1 \type t_1\ra t_2 && \Gamma \types e_2 \type u && \Gamma \types u \subtype t_1
				\end{matrix}
				}{
				\Gamma \types e_1\;e_2 \type t_2
				}
				$$
				
				Replacing **abstraction**:
				$$
				\frac{
				\begin{matrix}\Gamma \types t_1 \subtype s_1 && \Gamma \types s_2 \subtype t_2\end{matrix}
				}{
				\Gamma \types s_1 \ra s_2 \subtype t_1 \ra t_2
				}
				$$
				Note which way round the types are. Input is subtype, but output is supertype. Input can be more specific, and output can be less specific. 
				
				**Covariance** is when a subtype returns nothing that the supertype would not reutrn,
				
				**Contravariance** is when a subtype must accept all arguments accepted by a supertype. 
				
				The **Liskov Substitution Principle** is that if $S \lt: T$, then any program that takes $T$ can be replaced with $S$ without changing the program. I.e. a child class can be used anywhere that can be used in the parent class.
				
				And this whole subtyping rigamarole leads us to...
			</div>
		</div>

		<div class="colourband">
			<h2 id="sls">Simple Language with Subtypes</h2>
		</div>

		<div class="cbox">
			<div class="md-conv">
				### Contents 

				1. [SLS](#sls-1)
				2. [Existential Quantification](#sls-2)
				3. [Abstract Data Types](#sls-3)
				4. [Further Type Construction](#sls-4)
			</div>

			<div class="md-conv" id="sls-1">
				### SLS 

				We now have simple functions, polymorphic functions, polymorphic functions with subtyping, and combinations of all of them. We can unify them into one notation under the **"Simple" Language with Subtypes**:

				$$
				\DeclareMathOperator*{\Top}{Top}
				f = \Lambda (a \lt: \Top) \cdot \Lambda (b \lt: \{\textrm{one}:a\}) \cdot \lambda x:b \cdot x@\textrm{one}
				$$
				We introduce a type $\Top$ that is the superest of supertypes -- anything subtypes top. The type of $f$ is thus 
				$$
				f \type \forall a \lt: \Top \cdot \forall b \lt: \{\textrm{one}: \num\}
				$$

				Now we can 
				$$
				f[\num][\{\textrm{one}:\num, \textrm{two}:\bool\}] \; \{\textrm{one} = 2, \textrm{two} = \true\}
				$$

				Remember how we mentioned that PCF was system $F_1$? This is $F_2$. 

				$F_2$ has types 
				$$
				s, t := X \gang \Top \gang s \ra t \gang \forall X \lt: s \cdot t
				$$
				Type variables, top, functions, foralls. 

				Its basic terms now go
				$$
				M, N := x, \gang \lambda x:t \cdot M \gang M \; N \gang \Lambda s \lt: t \cdot M \gang M[t]
				$$
				Typed abstraction, application, type abstraction, and type instantiation.

				We now have expressions of the form type -> type, type -> term, term -> term... and we start to get the "lambda cube" (search it up yourself).
			</div>

			<div class="md-conv" id="sls-2">
				### Existential <strike>Crisis</strike> Quantification 

				We have $\forall$, why not also have $\exists$? Well of course we can! 
				
				It gets... even more difficult though. 
				
				A trivial type is $T \types x \type \exists a \cdot a$. This really says that $x$ can be any type. 
				
				We could say $\dfrac{\types 3 \type \num}{\types 3 \type \exists a \cdot a}$, thus $a \sim \num$. 
				
				We could say $\types \langle [4,9,5], \len \rangle \type \exists a \cdot a$, thus we could say $a \sim (\num\list) \ast (\num\list \ra \num)$.
				
				We could be more specific and say $\types \langle 3, \succ \rangle \type \exists a \cdot a \ast (a \ra a)$. Thus $a \sim \num$. We could be even more specific and say $\types \langle 3, \succ \rangle \type \exists a \cdot a \ast (a \ra \num)$.
				
				But $\langle [4,9,5], \len\rangle \type \exists a \cdot a \ast (a \ra \num)$ as well, so these two both have the same existential quantification!? 
				
				Yes, because **existential quantification hides types**, rather it shows the **relationship** between types, abstractly. 
				
				They almost act as function interfaces. Thus we have 
				* Content: $\langle 3, \succ \rangle$
				* Interface: $a \ast (a \ra \num)$
				* Representation: $a \sim \num$.
			</div>

			<div class="md-conv" id="sls-3">
				### Abstract Data Types 

				Let's focus on making a stack data type. 
				
				We need to specify an interface, hide information, typecheck the declaration with full information, and typecheck invocations with partial infomation.
				
				To help this we include the following declaration syntax:
				$$
				\DeclareMathOperator*{\pack}{pack} 
				\DeclareMathOperator*{\with}{with} 
				\DeclareMathOperator*{\new}{new} 
				d := \cdots \gang p = \pack[\tau=t_1 \inn t_2] \with M \gang x, \tau = \new p
				$$
				Where the first expression is a declaration, and the second expression is an instantiation. 
				
				For example: 
				* $\let p = \pack[a=\num \inn a \ast (a \ra \num)]\with \; \langle3, \succ\rangle ;$
				* $\let x, t = \new p;$
				* $(\snd x) \; (\fst x);$
				
				Note in line 2 we create simultaneously a new variable and a type variable. 
				
				Now we can create a stack: <span class="grey">I am crying formatting this</span>
				
			</div>

			<div class="codediv">\( \let \textrm{ StackL } = \)
	\( \Lambda \textrm{ item } \cdot \pack [X = \textrm{ item } \list \inn \{ \)
		\( \textrm{new} : X, \)
		\( \textrm{push} : \textrm{item} \ra X \ra X \),
		\( \textrm{pop} : X \ra X \),
		\( \textrm{top} : X \ra \textrm{item} \)
	\( \}] \with \{ \) 
		\( \textrm{new} = [] \),
		\( \textrm{push} = \lambda (i:\textrm{item}) \cdot \lambda (j:\textrm{item}\list) \cdot i :: j \),
		\( \textrm{pop} = \lambda(i : \textrm{item} \list) \cdot \tl i \)
		\( \textrm{top} = \lambda (i : \textrm{item} \list) \cdot \hd i \)
	\( \}; \)
\( \let p, S = \new \textrm{StackL}[\num] \)
\( p @ \textrm{top} \; (p@\textrm{push} \; 3 \; p@\textrm{new}) \)			</div>

			<div class="md-conv">
				The type of $p$ is thus $\types \{\new : S , \textrm{push} : \num \ra S \ra S, \textrm{pop} : S \ra S , \textrm{top} : S \ra \textrm{item}\}$. Note that $p@\textrm{new}$ is not the special keyword $\new$, but rather just a method.  
				
				Where $S$ is the packed type variable that we declare with $p$. 
				
				Note that once we declare a type variable, it cannot be reused. So we can't push a variable using $p@\textrm{push}$ into another stack $q$, since the type variable of $p$ would not be that of $q$.
				
				The typing rules for $\pack, \new$ are given:
				$$
				\pack \frac{
				\Gamma \types M \type [t_1 / \tau] t_2
				}{
				\Gamma \types p = \pack [\tau = t_1 \inn t_2] \with M  \type \Gamma, p:\exists \tau \cdot t_2
				}
				$$
				$$
				\new \frac{
				\Gamma \types p \type \exists t_1 \cdot \tau 
				}{
				\Gamma \types x_1,t2 = \new p \type \Gamma, [t_2 / t_1] \tau, t_2
				}
				$$
				
				In this stack, the $X$ type is **hidden**. Thus we can't really do much with types of this hidden type. 
				
				To address this we can selectively leak an interface. 

				#### Bounded Existential Quantification (or Subtypes, bascially)

				Suppose we want to create a point structure with $x, y$ fields. In the declaration we can selectively "leak" an interface: 
				$$
				\pack [pt \lt: \{x:\num, y:\num\} = \{\cdots\}] \inn \{\cdots\};
				$$
				Now we can change the pack typing rule to 
				$$
				\frac{
				\begin{matrix}\Gamma \types C \lt: A && \Gamma \types [C / X] M \type [C / X ] B\end{matrix}
				}{
				\Gamma \types \pack [X \lt: A = C \inn B] \with M \type \Gamma, p:\exists X\lt:A \cdot B
				}
				$$
				We can thus define a point, which we can think of like an entity in the game, that is either alive (red) or dead (grey), this way:
			</div>

			<div class="codediv">\( \let \textrm{points} = \pack [ \)
	\( pt \lt: \{x:\num, y:\num\} = \{x:\num, y:\num, rgb:\num\} \inn\; \{ \)
		\( init : n \ra n \ra pt, \)
		\( kill : pt \ra pt \)
	\( \} \)
\( ]\; \with \; \{ \)
	\( init = \lambda a:\num \cdot \lambda b:\num \cdot \{x=a, y=b, rgb=\textrm{Red}\}, \)
	\( kill = \lambda p : pt \cdot \{x=p@x, y=p@y, rgb=\textrm{Grey}\} \)
\( \}; \)</div>
				
			<div class="md-conv">
				For this new bounded existential quantification we have the following rules: 
				$$
				\frac{
				\Gamma \types M \type \exists X \lt: A \cdot B
				}{
				\Gamma \types x,T = \new M \type \Gamma, x:[T / X]B, T \lt: A
				}
				$$
				$$
				\frac{
				\begin{matrix}\Gamma\types A \subtype A' && \Gamma, X\lt:A \types B \subtype B'\end{matrix}
				}{
				\Gamma \types (\exists X\lt:A \cdot B) \subtype (\exists X \lt: A' \cdot B')
				}
				$$
				
				We also have the rules for well formed types 
				$$
				\frac{
				\Gamma, X\lt: A \types B
				}{
				\Gamma \types \exists X \lt:A \cdot B
				}
				$$
				And well-formed scope rules 
				$$
				\begin{matrix}
				\dfrac{\Gamma, t\lt:\alpha \types \beta}{\Gamma \types \forall t \lt: \alpha \cdot \beta}
				&&
				\dfrac{\Gamma, t\lt:\alpha \types \beta}{\Gamma \types \exists t \lt: \alpha \cdot \beta} \\ \\
				\dfrac{\Gamma, t \lt: \alpha \types \diamond}{\Gamma, t \lt: \alpha \types t \lt: \alpha} &&
				\dfrac{\begin{matrix}\Gamma \types \alpha && t \not\in \textrm{dom}(\Gamma)\end{matrix}}{\Gamma, t \lt: \alpha \types \diamond}
				\end{matrix}
				$$
				
			</div>

			<div class="md-conv" id="sls-4">
				### Further Type Construction 

				We now have expressions for logical and (through pair) and logical imply (through arrow), as well as universal and existential quantification. 
				
				Now we introduce the most fundamental **unit type**: 
				$$
				\DeclareMathOperator*{\unit}{unit}
				\frac{\Gamma \types \diamond}{\Gamma \types () \type \unit}
				$$
				There is only one value with this type, the brackets. 
				
				We introduce **union types**: i.e. multiple types being stored under one variable. An example of this is given (ocaml):	
			</div>
			<div class="codediv">type num = Fl of float | In of int ;;
[Fl 0.2 ; In 3] ;; <span class="grey">: num list</span> </div>

			<div class="md-conv">
				There are no type errors, because each type of the union type must be preceded by its keyword. 
				
				This is now our logical or. 
				
				We can add this to our language using the $+$ type operator, with the following syntax:
				
				$$
				\DeclareMathOperator*{\of}{of}
				\frac{
				\Gamma \types x : t_n
				}{
				\Gamma \types l_n \; x \type (\cdots + l_n \of t_n + \cdots)
				}
				$$
				
				So given $e : \textrm{float}$ we can say $Fl \;e \type (Fl \of \textrm{float}, In \of \textrm{int})$.
				
				We can now **pattern match** over types, though we must account for all cases (ocaml):
			</div>
			<div class="codediv">let addone x = match x with Fl x -> x + 1.0 |
							In x -> x + 1</div>

			<div class="md-conv">
				
				We can now do **recursive types**, which is useful for recursive data structures like binary trees:
				
				type $\alpha$ bintree = Leaf of $\alpha$ 
					| Btree of ($\alpha$ bintree * $\alpha$ bintree) ;;
				
				We must then figure out how to type this ... why not reuse the $\mu$ operator?
				$$
				BTree_A = \mu X \cdot (A + (X \ast X))
				$$
				We can label the various union types:
				$$
				BTree_A = \mu X \cdot (\textrm{inr}:A + \textrm{inl}:(X \ast X))
				$$
				
				We can also redefine list this way 
				$$
				List_A = \mu X \cdot (\unit ,(A * X))
				$$
				But this makes terms more complicated. 
				
				Now we're getting crazy.
				
				But we can get crazier with *gasp* imperative languages...
			</div>
		</div>


		<div class="colourband">
			<h2 id="imp">Imperative Programming</h2>
			<p class="subtitle">A Language Called IMP</p>
		</div>

		<div class="cbox">
			<div class="md-conv">
				### Contents 

				1. [IMP](#imp-1)
				2. [BLOCK](#block)
				3. [PROC](#proc)

				$
				\newcommand{\A}{\cal{A}}
				\newcommand{\B}{\cal{B}}
				\newcommand{\N}{\cal{N}}
				\newcommand{\C}{\cal{C}}
				\DeclareMathOperator*{\true}{true}
				\DeclareMathOperator*{\false}{false}
				\DeclareMathOperator*{\skip}{skip}
				\DeclareMathOperator*{\if}{if}
				\DeclareMathOperator*{\then}{then}
				\DeclareMathOperator*{\else}{else}
				\DeclareMathOperator*{\while}{while}
				\DeclareMathOperator*{\do}{do}
				\DeclareMathOperator*{\AND}{AND}
				$
			</div>

			<div class="md-conv" id="imp-1">
				
				### IMP
				
				Bear with me here, believe it or not it's going to get even worse. 
				
				Imperative programming, which has mutable *shudder* variables. We will introduce a language called IMP. 
				
				An example of IMP is: `y := 1; while ~(x = 1) do (y := y * x; x := x - 1)`
				
				<div class="blue">

					**IMP** has the following operations:
				
					* Arithmetic operations: $a := n \gang v \gang a_1 + a_2 \gang a_1 - a_2 \gang a_1 * a_2$.
					* Boolean operations: $b := \true \gang \false \gang a_1 = a_2 \gang a_1 \leq a_2 \gang \sim b \gang b_1 \land b_2$ (or can be done with and and de-morgans). 
					* Commands: $c := \skip \gang v := a \gang c_1 ; c_2 \gang \if b \then c_1 \else c_2 \gang \while b \do c$
				</div>
				
				
				PCF has judgements and declarations. For IMP we have a **store**, which maps variables to values, which are going to be integer values.
				$$
				s = [a \mapsto 3, b \mapsto 2]
				$$
				IMP is treated like a series of strings, and we kinda functionally parse our way through. The following operations are **bindings** or **parsings** (I made up that word)
				
				I.e. whilst on the surface we use imperative language, behind the scenes we are secretly just using lambda functions and shunting a store data structure all across the place.
				
				***Arithmetic.*** $\cal{A}[v] = \lambda s \cdot s[v]$. 
				
				A takes a variable string and a store, and retrieves the integer value in the store mapped from that variable. Over operations, arithmetic will recursively parse all variables and calculate them into values:
				$$
				\cal{A}[a_1 + a_2]\; s = \cal{A}[a_1]\; s + \cal{A}[a_2] \; s
				$$
				Note that + inside the brackets is a *string*, whilst the + outside of the brackets is mathematical add. Yes, this notation is about as readable as Haskell type-kinding, but oh well. 
				
				***Numeric.*** $\cal{N}[n] = n$. 
				
				N takes a string literal number, and parses its corresponding number. This is similar to A in that e.g. $\cal{N}[67] \equiv \cal{A}[67]\;s  \equiv 67$. The difference is N does not take a store, as literal numbers do not need a store, whilst A requires a store to retrieve variable values. 
				
				***Boolean.*** $\cal{B}[b] \in \{\top, \bot\}$ 
				
				B takes a string literal boolean $\true, \false$, and a store, and parses its corresponding truth value $\top, \bot$. So
				* $\cal{B}[\true] = \lambda s \cdot \top$ and $\cal{B}[\false] = \lambda s \cdot \bot$
				* $\cal{B}[a_1 = a_2] = \lambda s \cdot \if \A[a_1] \; s = \A[a_2] \; s \then \top \else \bot$
				* $\B[b_1 \land b_2] = \lambda s \cdot \B[b_1] \; s \AND \B[b_2]$. **Note:** The notation used by Sara is "&&" for the and operation (not the string) but since that does not play as nice with $\LaTeX$ I will use capitalised **AND**. 
				
				***Command.*** $\C[c]$.
				
				C takes a string literal command, and a store, and parses command operations based on what it gets:
				* $\C[\skip] = \lambda s \cdot s$ (no operation)
				* $\C[v:=a] = \lambda s \cdot s[v \mapsto \A[a]\;s]$ (variable assignment)
				* $\C[c_1; c_2] = \lambda s \cdot \C[c_2] \; (\C[c_1] \; s)$ (sequencing)
				* $C[\while b \do c] = \lambda s \cdot \C[\if b \then \C[\while b \do c]\; s]\; s$. (while loops -- this is recursion which can get... strange).
				
				A, N, B, C convert IMP to **natural semantics**.
				
				<hr>
				
				We can also do **operational semantics**, using **configurations**.
				
				***Configuration.*** A configuration is a pair of command and store: $\langle c, s \rangle \implies \cdots$.
				
				E.g. 
				$$
				\begin{align*}
				\langle v:=a, s\rangle &\implies s[v \mapsto \A[a]\; s]\\
				\langle \skip, s\rangle &\implies s\\
				\langle c_1; c_2, s\rangle &\implies \cdots
				\end{align*}
				$$
				Where if we have sequencing, we can use our natural deduction style:
				$$
				\frac{
				\begin{matrix}\langle c_1, s\rangle \implies s'  && \langle c_2, s'\rangle \implies s''\end{matrix}
				}{
				\langle c_1; c_2, s\rangle  \implies s''
				}
				$$
				$$
				\frac{
				\begin{matrix}\B[b] \; s  = \top && \langle c_1, s \rangle \implies s'\end{matrix}
				}{
				\langle \if b \then c_1 \else c_2, s\rangle \implies s'
				}
				$$
				And so on and so forth.
				
				***Example.*** Verify the program `y := 1; while ~(x=1) do (y := y * x; x := x - 1)` in the store $s_0 = [x \mapsto 2]$
				
				***Answer.*** Proof trees! Wide.
				$$
				\frac{
					\begin{matrix}
					\dfrac{}{\langle y:=1, [x \mapsto 2]\rangle \implies [x \mapsto 2, y \mapsto 1] = s_1} & 
					\dfrac{
						\begin{matrix}
						\dfrac{}{\langle \B[\lnot (x=1)], s_1 \rangle \implies \bot} & 
						\dfrac{
						\vdots
						}{
						\langle y := y * x; x := x - 1, s_1\rangle \implies [x \mapsto 1, y \mapsto 2] = s_2 
						}
						&
						\dfrac{\vdots}{\while \sim(x=1) \do (c_1, c_2), s_2\rangle \implies s_2} 
						\end{matrix}
					}{
					\langle \while \sim(x=1) \do (c_1, c_2), s_1\rangle
					\implies s_2
					}
					\end{matrix}
				}{
				\langle y:=1; \while \sim(x=1) \do (c_1, c_2), [x \mapsto 2]\rangle \implies s_2 = [y \mapsto 2, x \mapsto 1]
				}
				$$
				You get the idea. 
				
				<hr>
				
				We can also do **small step semantics**, proving one specific reduction 
				
				$$
				\frac{
				\langle c_1, s\rangle \lra s'
				}{
				\langle c_1;c_2 , s \rangle \lra \langle c_2, s \rangle 
				}
				$$
				$$
				\frac{
				\langle c_1, s \rangle \lra \langle c_1', s' \rangle 
				}{
				\langle c_1; c_2 , s \rangle \lra \langle c_1' ; c_2 , s'\rangle 
				}
				$$
				Which is when $c_1$ can be broken down -- i.e. it consists of multiple single commands. 
				
				$$
				\langle \while b \do c, s\rangle \lra \langle \if b \then (c; \while b \do c) \else \skip, s \rangle
				$$
				
				In imperative languages, because we have *shudder* side effects, we may actually get into a stuck state, in which $\implies$ is difficult to get but $\lra$ is still perfectly fine (until the stuck).
			</div>

			<div class="md-conv" id="block">
				### BLOCK 

				**BLOCK** is an extension to IMP that introduces **variable scope**. An example is given below:
			</div>

			<div class="codediv">begin 
	var x := 0; var y := 1;
	begin 
		var x := 2; y := x + 1
	end;
	x := y + x
end;</div>

			<div class="md-conv">
				We extend the command syntax to include a block declaration:
				
				$$
				\DeclareMathOperator*{\scope}{begin}
				\DeclareMathOperator*{\close}{end}
				\DeclareMathOperator*{\var}{var}
				\newcommand{\DV}{\cal{DV}}
				\newcommand{\dimplies}{\underset{D}{\implies}}
				\begin{align*}
					c & := \cdots \gang \scope d_v \; c \close \\
					d_v & := \var v := a; d_v \gang \epsilon
				\end{align*}
				$$

				And we have the following declaration rules of $\dimplies$, which is resolution of a declaration.
				\[
				\frac{
					\langle d_v, s[v \mapsto \A[a]\; s] \rangle \dimplies s'
				}{
				\langle \var v := a ; d_v , s \rangle \dimplies s'
				}
				\]
				Which is a new binding -- we update it as such in the store and repeat for all declarations.
				\[
				\frac{}{\langle \epsilon, s \rangle \dimplies s}
				\]
				Which is the base case of emptiness.
				\[
				\frac{
					\begin{matrix}
						\langle d_v, s \rangle \dimplies s' && \langle c, s' \rangle \implies s''
					\end{matrix}
				}{
				\langle \scope d_v \; c \close, s \rangle \implies s''[\DV (d_v) \mapsto s]
				}
				\]
				* Top left: we evaluate the declaration to get all variable bindings in scope. 
				* Top right: we evaluate the commands under the scope made 
				* Bottom: Reassign any variables from the outer scope that were changed in the inner scope. $\DV$ 

				***Example.*** Let's take the code mentioned before and track the stores
			</div>

			<div class="codediv">				   			<span class="grey">\(s_0 = []\)</span>
begin var x := 0;  			<span class="grey">\(s_1 = [x \mapsto 0]\)</span>
	  var y := 1;  			<span class="grey">\(s_2 = [x \mapsto 0, y \mapsto 1]\)</span>
	  begin var x := 2; 	<span class="grey">\(s_3 = [x \mapsto 2, y \mapsto 1]\)</span>
			y := x + 1  	<span class="grey">\(s_4 = [x \mapsto 2, y \mapsto 3]\)</span>
	  end;					<span class="grey">\(s_5 = [x \mapsto 0, y \mapsto 3]\) (return to outer x)</span>
	  x := y + x 			<span class="grey">\(s_6 = [x \mapsto 3, y \mapsto 3]\)</span>
end;			</div>

			<div class="md-conv">

				<hr>

				It can be somewhat difficult to keep track of things when name collisions are a thing, such as the rescoping of `x`. 

				So we introduce **environments**, and now variables are **pointers to environments**. This way we get around the name collision. 

				* Now **Environments** map **variables -> locations** 
				* **Stores** map **locations -> values**.

				So now we can modify things, like A to become 
				$$
				\A[v] = \lambda E_v \cdot \lambda s \cdot s \; (E_v \; v)
				$$
				Variable -> env -> location -> store -> value.

				Now we can use environments in our proofs $E_v \types \langle c, s \rangle \implies s'$.

				We now modify the `begin end` rule to 
				\[
				\frac{
					\begin{matrix}
						\langle d_v,E_v, s \rangle \dimplies E'_v, s' && E_v \types \langle c, s' \rangle \implies s''
					\end{matrix}
				}{
				E_v \types \langle \scope d_v \; c \close, s \rangle \implies s''[\DV (d_v) \mapsto s]
				}
				\]
				The declaration now takes an environment. 

				A single variable declaration looks like 
				\[
				\langle \var x := a, E_v, s \rangle \dimplies E_v[x \mapsto l], s[l \mapsto \A[a] \; E_v \; s]
				\]

				Thus a list of declarations looks like 
				\[
				\frac{
					\langle d_v,E_v, s \rangle \dimplies E'_v, s'
				}{
					\langle \var x := a; d_v, E_v, s \rangle \dimplies E'_v[x \mapsto l], s'[l \mapsto \A[a] \; E_v \; s]
				}
				\]
			</div>

			<div class="md-conv" id="proc">
				### PROC

				I'll get onto this later... famous last words.
			</div>
		</div>

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="../../js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
	<script type="text/javascript" src="../../js/markdown.js"></script>
</body>
</html>