<!DOCTYPE html>
<html>
<head>
	<title>CS342</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="./" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="./blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="./about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS342</h1>
					<p class="subheading">Machine Learning</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Contents</h1>
			</div>
		</header>

		<!-- <div class="buttonwrapper beside" >
			<a href="./index-zh.html">简体中文</a>
		</div> -->
		<!-- REMEMBER TO DO! -->

		<div class="cbox">
			<ol>
				<li><a href="#basics">The Basics</a></li>
				<li><a href="#desctree">Decision Trees</a></li>
				<li><a href="#bayes">Naive Bayes</a></li>
				<li><a href="#knn">K-nearest Neighbours</a></li>
				<li><a href="#linreg">Linear Regression</a></li>
				<li><a href="#robreg">Robust Regression</a></li>
				<li><a href="#clustering">Clustering</a></li>
				<li><a href="#pca">Principle Component Analysis (PCA)</a></li>
				<li><a href="#linclass">Linear Classification (+ Perceptron)</a></li>
				<li><a href="#kernel">The Kernel Trick</a></li>
			</ol>
			\( 
				\renewcommand{\vec}{\mathbf}
				\renewcommand{\epsilon}{\varepsilon}

				\DeclareMathOperator*{\argmax}{argmax}
				\DeclareMathOperator*{\argmin}{argmin}

				\def\xtrain{{\vec{x}_i}}  
				\def\xtest{{\vec{\tilde{x}}_i}}  
				\def\ytrain{{y_i}} 
				\def\ytest{{\tilde{y}_i}} 
				\def\ypred{{\hat{y}_i}} 
				\def\wtrans{{\vec{w}^\top}} 
				\def\bb#1{{\mathbb{#1}}}
				
				\def\x{{\vec{x}}}
				\def\X{{\vec{X}}}
				\def\w{{\vec{w}}}
				\def\W{{\vec{W}}}
				\def\y{{\vec{y}}}
				\def\t{{^\top}}
				\def\z{{\vec{z}}}
				\def\Z{{\vec{Z}}}

				\def\lra{{\longrightarrow}}
				\def\lla{{\longleftarrow}}
				\newcommand{\fr}{\frac}
				\newcommand{\vecu}{\underline}
				\newcommand{\rm}{\textrm}
				\newcommand{\bb}{\mathbb}
				\newcommand{\nm}{\overline}

			\)
		</div>

		
		<div class="colourband">
			<h2 id="basics">The Basics</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<p>
				<b>Machine Learning</b> is the process of getting machines to learn to model stuff, that is, given experiences <i>E</i> to learn from, a set of tests <i>T</i> to measure against, for which we measure a performance <i>P</i>. 
			</p>
			<p>
				Patterns are learned from <i>features</i>, qualities that describe the data, which change depending on the data in question. 
			</p>
			<p class="centre">
				Input -> Extracting features (manual) -> Training and classification -> Results
			</p>
			<p>
				Getting the machine to extract features itself is called <i>representation learning</i>, and is generally used in deep neural networks.
			</p>

			<h3>Recap</h3>

			<p>
				Machine learning takes basis on probability (and a lot of stats) - especially joint probability distributions (like \(p(A, B)\)).
			</p>

			<p>
				Recall:
			</p>
			<ul>
				<li>The <b>marginalisation rule</b> \(p(A) = \sum\limits_{\forall x} p(A, B=x)\)</li>
				<li><b>Random Variables</b> X, with an event \(X = x\) which occurs with some probability, the sum of all possible events is 1.</li>
				<li><b>Conditional probability</b> \(p(A|B) = \frac{p(A,B)}{p(B)} \)</li>
				<li><b>Bayes Rule</b> \(p(A|B) = \frac{p(B|A)p(A)}{p(B)}\)</li>
			</ul>

			<p>
				Datasets are collections of samples - the range of values are limited and probably in some sort of <i>distribution</i>, which can take many forms, such as uniform (straight line) or gaussian/normal (bell curve).
			</p>
			<p>
				Datasets can have a <b>mean</b> \(\mu\) and <b>standard deviation</b> \(\sigma^2\) -- variance is stddev-squared.
			</p>

			<div class="cornell">
				<div>
					<p>Distribution Functions</p>
				</div>
				<div>
					<p>
						<b>Probability density (PDF)</b>: probability of observing a value, i.e. \(p(X = x)\)

						\[ p(x|\mu, \sigma^2 ) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{(x-\mu)^2}{\sigma^2}) \]
					</p>
					<p>
						<b>Cumulative density (CDF)</b>: probility of observing &leq; the value \(p(X \leq x)\)

						\[p(X \leq x|\mu, \sigma^2) = \int_0^x \textrm{(curve)} dx\]
					</p>
				</div>
			</div>

			<p>
				<b>Expectation</b>, the expected value, is denoted \(\mathbb{E} \), and given as 
				\[\mathbb{E}[x] = \sum_{\forall x} p(X = x) \cdot x\]
			</p>

			<p>
				<i>Notation:</i> the module uses standard notations for everything: \
			</p>
			<div class="cornell">
				<div>
					<p>Scalar</p>
				</div>
				<div>
					<p>
						\(x\): lowercase (or upper case) letter.
					</p>
				</div>
				<div>
					<p>Column Vector</p>
				</div>
				<div>
					<p>
						\(\vec{x}\): bold lowercase letter. Vectors are <i>always</i> columns, so row vectors are <i>always</i> represented as \(\vec{x}^\top\). Dot products between vectors are often represented \(\vec{w}^\top \vec{x}\) (a matrix multiplication), though I may also use \(\vec{w} \cdot \vec{x}\)
					</p>
				</div>
				<div>
					<p>Matrix</p>
				</div>
				<div>
					<p>
						\[\vec{X} = \begin{bmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{bmatrix}\]
						Upper case bold letter. If a matrix is symmetric, \(\vec{X}^\top = \vec{X} \).
					</p>
				</div>
			</div>

			<p>
				<b>Supervised learning</b> is when both questions and answers are given, and the model has to learn to predict answers from samples. 
			</p>
			<p>
				<b>Unsupervised learning</b> is when no answers are given, and the model has to find relationships between samples.
			</p>

			<h3>General Data Notation</h3>

			<p>
				
				Samples are stored in a matrix, generally denoted \(\vec{X}\), where each row is a sample, a <b>feature vector</b>, and each column is an individual feature. \(\vec{X} \in \bb{R}^{n \times d}\), which means that X has \(n\) rows (for n samples) and \(d\) columns (for d features/dimensions)
				
				<ul>
					<li>
						\(\vec{x}_i\) is a sample row, represented as a column (as a row, it is consistently denoted \(\vec{x}_i^\top\)).
					</li>
					<li>
						\(\vec{x}_j \) is a column of features. 
					</li>
					<li>
						\(x_{ij}\) is feature \(j\) of column \(i\).
					</li>
				</ul>
			</p>
			<p>
				Labels are stored in vector \(\vec{y}\), \(y_i\) being the label for an \(\vec{x}_i\). This is also called the <b>ground truth</b>
			</p>
			<p>
				Given a new sample \(\tilde{\vec{x}}_i\) (tilde represents test data), we can make a prediction \(\hat{y}_i\) for it (the hat represents predicted). 
			</p>
		</div>

		<div class="colourband">
			<h2 id="desctree">Decision Trees</h2>
		</div>
		
		<div class="cbox">
			<h3>The Scenario</h3>

			<p>
				Suppose you start to frequently get upset stomachs, and worry that it is due to an allergy that you don't know about, perhaps to one food, perhaps to a combination. So, you decide to keep a food diary, detailing what allergen-inducing foods you consume each day, in what amounts, and whether or not you had an upset stomach that day. 
			</p>

			<table>
				<tr>
					<th>Milk</th>
					<th>Eggs</th>
					<th>Peanuts</th>
					<th>Crustaceans</th>
					<th>Upset?</th>
				</tr>
				<tr>
					<td>0.3</td>
					<td>0.2</td>
					<td>0</td>
					<td>0.7</td>
					<td>1</td>
				</tr>
				<tr>
					<td>...</td>
					<td>...</td>
					<td>...</td>
					<td>...</td>
					<td>...</td>
				</tr>
			</table>

			<p>
				This is a type of <i>supervised learning</i> problem, we have our features (foods and amounts) and our classes (upset stomach / not), and want to train a <i>classification</i> algorithm to predict things. 
			</p>

			<h3>The Decision Tree</h3>

			<p>
				A simple classifier is a <b>decision tree</b>
			</p>

			<p>
				A decision tree is essentially a nested series of if-elif-elses, that split the dataset up -- these are <i>splitting rules</i>. 
			</p>

			<figure>
				<img src="./assets/desctree-ex.png" alt="Decision tree with milk and eggs" style="max-width: 550px;">
			</figure>

			<p>
				Our aim is then to find these rules, which can be done recursively:
			</p>

			<h3>Stumps</h3>

			<p>
				The most basic decision tree is a <b>stump</b>. A stump is just one single splitting rule, the smallest possible tree. 
			</p>
			<p>
				To find a tree, we must first find a stump, but we need to find the stump that splits the best. How? With some scoring metric. 
			</p>
			<p>
				Ex. Could use plain accuracy, take a rule like <code>eggs > 0.2</code> and set a True and False coming off it - we now have a stump that can predict data. Pick the rule with the highest amount of data correctly predicted. 
			</p>
			<p>
				In our stump finding then, our training error would be just the percentage of \(\hat{y}_i \neq y_i\), i.e. the accuracy.
			</p>

			<h3>Greedy Recursive Splitting</h3>

			<p>
				A stump, however, can only split on one thing, and if we have \(d\) features we kinda want more splits, otherwise we're only ever using one of the features. 
			</p>
			<p>
				A common algorithm is <b>greedy recursive splitting</b> (GRS), which goes as follows:
			</p>
			<ol>
				<li>Find a stump with the best score metric, and split the dataset into two smaller ones at that stump. <span class="grey">(The new split datasets would probably still have a mix of \(y\)s, but hopefully are much more uniform than before)</span></li>
				<li>
					<i>Recurse:</i> "fit" (find) a new stump onto each of the smaller datasets, this is linked onto the previous stump.
				</li>
				<li>
					Repeat until accuracy threshold reached <span class="grey">(or depth reached, or some other limit. Accuracy <i>can</i> be 100%, but this may result in an overcomplex tree and not always be helpful, "overfitting").</span>
				</li>
			</ol>
			<p>
				Currently, we have accuracy as a possible metric, but this may not always be good, especially when there's multiple possible stumps that increase accuracy by the same amount, or the data is spread in such a way that a single line does not do much for accuracy. 
			</p>
			<p>
				A better way is if we could prioritise variance reduction, some sort of ... <i>information gain</i>:
			</p>

			<div class="cornell">
				<div>
					<p>Entropy</p>
				</div>
				<div>
					<p>
						<b>Entropy</b> of labels is essentially the randomness of their values. For a total of \(C\) categories, this can be calculated as
						\[E = -\sum_{c=1}^C p_c \cdot \log_2 (p_c)\]
						where \(p_c\) is the <i>proportion</i> of \(c\) out of all total labels in that (sub)set. Note that \(0\log_2 0\) is defined to just be 0.
					</p>
					<p>
						A lower entropy (min. 0) means less randomness, which is better. A higher entropy (max. \(log_2 C\), or 1 for binary classes) is worse. 
					</p>
				</div>
				<div>
					<p>Information Gain</p>
				</div>
				<div>
					<p>
						The principle of <b>Information Gain</b> (IG), is that we want the biggest difference in entropy pre-split to post-split. 
					</p>
					<p>
						For a single rule, we can split the set of \(n\) elements into \(n_{yes}\) (for the samples that match the rule) and \(n_{no}\) for the ones that don't. Thus IG is defined
						\[
						IG = E(\vec{y}) - \frac{n_{yes}}{n} E(\vec{y}_{yes}) - \frac{n_{no}}{n} E(\vec{y}_{no})
						\]
						i.e the entropy of combined labels \(\vec{y}\) minus the entropy of split labels individually.
					</p>
					<p>
						The higher this difference is, the better the split is.
					</p>
				</div>
			</div>

			<h3>Overfitting and Learning Theory</h3>

			<p>
				If we have 100% accuracy on our training data, this does not imply that we have 100% accuracy on any new test data. 
			</p>

			<ul>
				<li>
					<b>Test Accuracy</b> is the accuracy on new data
				</li>
				<li>
					<b>Overfitting</b> or <b>Optimisation Bias</b> is high accuracy on training data but low on test data 
				</li>
				<li>
					<b>Underfitting</b> is low accuracy on training data.
				</li>
			</ul>
			<p>
				We naturally want the right amount of fitting, but this is difficult. 
			</p>
			<p>
				We can use a test/verification set \(\tilde{\vec{X}}\) to verify, but we must ensure that we <i>do not train using the verification set</i>.
			</p>
			<p>
				The <b>IID Assumption</b> (Independent and Identically Distributed) states that we assume that the training data is a good representation of all data - that is, it has the same distribution as test data. 
			</p>
			<p>
				Whilst this is often not true, it is generally a good assumptuion. 
			</p>
			<p>
				<b>Learning Theory</b> gives us a metric for measurement, \(E_{approx}\) the <b>approximation error</b>, the error between training and testing:
				\[E_{test} = E_{approx} + E_{train}\]
				where \(E_{approx} = E_{test} - E_{train}\). If this is small, then we can say that the training data is good for test data. It tends to go down as the training set gets larger, and up as the model gets more complex (and thus overfit). 
			</p>

			<p>
				However, this approximation error leads to a <b>fundamental tradeoff</b>: minimise \(E_{approx}\) whilst minimising \(E_{train}\): make the model not overfit whilst having it be fit as well as possible. 
			</p>

			<h3>Validation</h3>

			<p>
				Usually, we split a training set into two: a larger <code>x_train</code> (the "training set"), and a smaller <code>x_test</code>, what's called the <b>validation</b> (or colloquially test) set. 
			</p>

			<p>
				Once we train on <code>x_train</code>, we can compute the accuracy of predicting <code>x_test</code>, and use this to adjust our model, such as manually setting <i>hyperparameters</i>.
			</p>

			<p>
				<b>Hyperparameters</b> are settings in a model which we <i>do not learn</i>. For decision trees, this may be the maximum depth of the tree, or the accuracy threshold. For more complex models like Neural Networks, the entire architecture is hyperparameter, whilst the weights and biases are learned, regular parameters.
			</p>

			<p>
				With simpler models, it is often good to train it multiple times with different hyperparameter settings (like tree depth), and see which one minimises both training error and approximation error the best.
			</p>
			<p>
				Sometimes, once hyperparameters are set, a model is trained again, with <code>x_test</code> merged back into the training set (no validation).
			</p>

			<p>
				That said, we may end up overfitting the validation set. If we search too many and too specific hyperparameters, we may end up finding one that's really good for the validation set, but not so good for any new data. 
			</p>

			<ul>
				<li>
					Optimisation bias increases as we search for more model configurations 
				</li>
				<li>
					It decreases as the training data gets larger.
				</li>
			</ul>

			<p>
				This leads to <b>another fundamental tradeoff</b>: increasing validation size means our model is less likely to be overfit. However, increasing validation size decreases test size, which makes our model train less well.
			</p>

			<h4>Cross Validation</h4>

			<p>
				Validation takes from test data, if test data is precious and hard to get, we may not want to waste it.
			</p>
			<div class="blue">
				<span class="nopad">
					<p>
						<b>K-fold cross validation</b> gives a good estimate without wasting too much data.
					</p>
	
					<p>
						For K folds, we train K times, each time taking \(\frac{1}{k}
						\) of \(\vec{X}\) as validation, and the rest \(\frac{k-1}{k}\) as train. The average validation accuracy is taken as the overall accuracy
					</p>
					
				</span>
				
			</div>

			<p>
				<i>Leave One Out (LOO)</i> is a special type of K-fold validation where \(k = n\).
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="bayes">Naive Bayes</h2>
		</div>

		<div class="cbox">
			<h3>The Scenario</h3>

			<p>
				You want to filter out which emails incoming are spam or not. For this, you've collected many, many emails, and labelled them correctly.
			</p>
			<p>
				Your features are going to be keywords in emails, thus, \(x_{ij} = 1\) if the word \(j\) is in email \(i\).
			</p>

			<h3>Probabilistic Classification</h3>

			<p>
				Traditional filtering/classification methods used something called <b>Naive Bayes</b> - a probability based classifier based on <i>bayes rule</i>. 
			</p>
			<p>
				The idea is, we want to model \(p(y_i | \vec{x}_i)\): probability of it having that label, given those features. If \(y_i\) was binary, then we could classify \(\vec{x}_i\) as true if \(p(y_i | \vec{x}_i) > p(\lnot y_i | \vec{x}_i)\).
			</p>
			<p>
				Given bayes rule, and a y-label \(spam, \; \lnot spam\), we can use Bayes rule:
				\[
				p(spam | \vec{x}_i) = \frac{p(\vec{x}_i | spam)p(spam)}{p(\vec{x_i})}
				\]
				(And similar for non-spam.) \(p(\vec{x}_i)\) is the probability that email \(i\) has those words in it.
			</p>
			<div class="cornell">
				<div>
					<p>\(p(spam)\)</p>
				</div>
				<div>
					<p>
						<b>Very easy</b> to calculate, it's just the proportion of positive to all \(p(spam) = \frac{\sum spam}{n}\)
					</p>
				</div>
				<div>
					<p>\(p(\vec{x})_i\)</p>
				</div>
				<div>
					<p>
						Very difficult to calculate, however due to the classification rule we use:
						\begin{align}
							p(y_i | \vec{x}_i) &> p(\lnot y_i | \vec{x}_i) \\
							\implies \frac{p(\vec{x}_i | y_i)p(y_i)}{p(\vec{x_i})} &> \frac{p(\vec{x}_i | \lnot y_i)p(\lnot y_i)}{p(\vec{x_i})} \\
							\implies p(\vec{x}_i | y_i)p(y_i) &> p(\vec{x}_i | \lnot y_i)p( \lnot y_i)
						\end{align}
						We can just <b>ignore</b> the term.
					</p>
				</div>
				<div>
					<p>\(p(\vec{x_i} | spam)\)</p>
				</div>
				<div>
					<p>
						Equals \(\frac{\sum \textrm{ spam emails with } \vec{x}_i}{\sum spam}\), which is very hard to work out, and no easy workaround:
					</p>
					<p>
						Make a <b>conditionally independent assumption</b>: assume the occurence of every \(x_{ij}\) is independent from every other \(x_{ij'}\). This is why the Bayes is <i>naive</i>.
					</p>
					<p>
						Thus \(p(\vec{x_i} | spam) \approx \prod\limits_{j=1}^{d} p(x_{ij} | spam)\). 
					</p>
					<p>
						Whilst this is rarely true, it is in some cases a good enough approximation.
					</p>
				</div>
			</div>

			<p>
				More generally, when predicting data, we find the class \(c\) of \(y\) that has the highest probability given an \(\tilde{\vec{x}}_i\).
			</p>

			<p>
				There is one other problem: when data is missing from our set but we still want to account for it. For example, if you want to detect the chance an email containing "millionaire" is spam, but none of your collected spam emails have that word. 
			</p>
			<p>
				We can use <b>laplace smoothing</b> to account for this: if a feature \(x_{ij}\) has \(k\) possible values, for some \(\beta \in \bb{R}\):
				\[
				\frac{p(x_{ij} = e) + \beta}{\textrm{total }+ k\beta}
				\]
				i.e. add a fake value to the proportion of emails that have the feature we want, but add everything else in the same proportion to the total, so proportions are kept consistent. 
			</p>
			<p>
				For binary features (and \(\beta = 1\)), we can do
				\[
				
				\frac{
					\#\textrm{spam with 'millionaire' } + 1
				}{
					\#\textrm{total spam } + 2
				}
				\]
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="knn">K-Nearest Neighbours</h2>
		</div>

		<div class="cbox">
			<p>
				The models looked at previously were <i>parametric</i> -- that is, they had a fixed number of parameters (fixed by hyperparmameters, the model size is O(1) w.r.t. data size)
			</p>
			<p>
				<b>K-nearest neighbours</b> (KNN) is in the class of <i>non-parametric</i> models, that is, the larger the dataset, the larger the model gets. 
			</p>
			<p>
				KNN is a classification model, that works by taking a new point \(\vec{\tilde{x}_i}\) and comparing it to its \(k\) "closest" neighbours from the training set, classifying it into the majority category.
			</p>
			<p>
				It works off the <b>Lipschitzness Assumption</b>, that if two feature vectors are "close", they are likely to be similar. 
			</p>
			<p>
				This closeness is most commonly done using <b>L2 Norm: euclidean distance</b> \(\lVert \vec{x}_i - \vec{\tilde{x}}_i \rVert\).
			</p>
			<p class="grey">
				Note that euclidean distance is often written \(|\vec{a}|\) in mathematics, or with double bars and an explict L2-norm indicator \(||\vec{a}||_2\), seen in ML. 
			</p>
			<p>
				The way KNN calculates its \(k\) nearest neighbours is simply by calculating euclidean distance to all \(\vec{x} \in \vec{X}\) and finding the closest ones. Thus,
			</p>
			<ul>
				<li>There's no traditional learning (<b>lazy learning</b>), simply store the data</li>
				<li>Non parametric size, size \(\propto O(nd)\) for \(n\) samples and \(d\) dimensions</li>
				<li>Predicting is expensive, \(O(ndt)\) for \(t\) test samples</li>
			</ul>

			<p>
				A <b>Norm</b> is a way of "calculating distance", so to say. There are three (maybe four) common norms:
			</p>

			<table>
				<tr>
					<th>L2</th>
					<th>L1</th>
					<th>(L0)</th>
					<th>L\(\infty\)</th>
				</tr>
				<tr>
					<td>\[\lVert \vec{r} \rVert_2 = \sqrt{r_1^2 + r_2^2}\]</td>
					<td>\[\lVert \vec{r} \rVert_1 = |r_1| + |r_2|\]</td>
					<td>\[\lVert \vec{r} \rVert_0 = nz(r_1) + nz(r_2)\] \[\textrm{s.t. }  nz(x) = \begin{cases} 1 & x \neq 0 \\ 0 & x = 0 \end{cases}\]</td>
					<td>\[\lVert \vec{r} \rVert_\infty = \max(|r_1|, |r_2|)\]</td>
				</tr>
				<tr>
					<td>Euclidean distance</td>
					<td>Manhattan distance</td>
					<td>Counts nonzero values</td>
					<td>Max</td>
				</tr>
				<tr>
					<td>Emphasises larger values</td>
					<td>All values are equal</td>
					<td>(Not actually a norm)</td>
					<td>Only the largest component matters</td>
				</tr>
			</table>

			<p>
				The numbers are given with reason: the \(L_p\) norm for \(p\geq 1 : \lVert \vec{x} \rVert_p = \left( \sum\limits_{j=1}^d |x_j|^p \right)^{\frac{1}{p}}\)
			</p>

			<p>
				A <b>1NN</b> classifier is just nearest neighbour, every point defines a cell, within which everything belongs to its category (a "voronoi tesselation"). Boundaries between cells are <b>decision boundaries</b>, assignment here can be arbitrary.
			</p>

			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Euclidean_Voronoi_diagram.svg/1200px-Euclidean_Voronoi_diagram.svg.png" alt="Voronoi tesselation - wikipedia" style="max-width: 300px;">
				<figcaption>Voronoi Tesselation (Wikipedia)</figcaption>
			</figure>

			<h3>Test Errors, and Optimal Bayes</h3>

			<p>
				We can compare the error of a KNN (or 1NN) classifier against the <b>Optimal Bayes Classifier</b>.
			</p>
			<p>
				Recall <i>Naive Bayes</i>: we assume that all features are independent. <b>Optimal Bayes</b> is a theoretical Bayes classifier that does <i>not</i> make this assumption. 
			</p>
			<p>
				Optimal Bayes will then predict \(\hat{y_i} = \argmax\limits_{y_i} p(y_i | \vec{x}_i)\), taking into account all conditional probabilities.
			</p>
			<p>
				Note that Optimal Bayes is <i>not</i> a perfect classifier, it still has an error \(\epsilon_{bayes} = 1 - p(\hat{y}_i | \vec{x}_i)\), but this error is agood <b>lower bound</b> on potential error of classifier. 
			</p>

			<p class="side">
				<b><i>Claim.</i></b> For a 1-NN classifier, as \(n \longrightarrow \infty\) its error \(\epsilon_{1NN} \longrightarrow 2\epsilon_{bayes}\)
			</p>
			<button class="collapsible">Proof... </button>
			<div class="ccontent">
				<p>
					<b><i>Proof.</i></b> Assume a two class 1NN, with classes + and -.
				</p>
				<ul>
					<li>
						With more samples, the distance between \(\vec{\tilde{x}}_i\) and its closest neighbour tends to 0 (imagine the space filling up with points.)
					</li>
					<li>
						We want to find the probability that it is a false classification, \(p(\hat{y} \neq \tilde{y} | \vec{\tilde{x}})\)
					</li>
					<li>
						If positive \(p(+|\vec{x}_i)\), the classifier can predict rightly with probability \(p(+|\vec{x}_i) p(+|\vec{\tilde{x}}_i)\) or wrongly \(p(+|\vec{x}_i) (1-p(+|\vec{\tilde{x}}_i))\)
					</li>
					<li>
						Similarly if negative it can get it correct \((1-p(+|\vec{\tilde{x}}_i))(1-p(+|\vec{\tilde{x}}_i))\) or wrong \( (1-p(+|\vec{\tilde{x}}_i))p(+|\vec{x}_i)\)
					</li>
					<li>
						Note that there are two false probabilities,
						\begin{align}
							\epsilon_{1NN} &= p(\hat{y}_i|\vec{x}_i) (1-p(\hat{y}_i|\vec{\tilde{x}}_i)) + (1-p(\hat{y}_i|\vec{\tilde{x}}_i))p(\hat{y}_i|\vec{x}_i) \\
							&\geq (1-p(\hat{y}_i)|\vec{x}_i) + (1-p(\hat{y}_i)|\vec{x}_i) \pod{\textrm{ignore tilde}} \\
							&\geq 2(1-p(\hat{y}_i)|\vec{x}_i)\\
							& \geq 2\epsilon_{bayes}
						\end{align}

						$$\tag*{$\Box$}$$
					</li>
				</ul>
			</div>

			<p>
				The error of k-NN classification tends to \((i+\sqrt{\frac{2}{k}} ) \epsilon_{bayes}\). 
			</p>

			<h3>The Curse of Dimensionality (Runaway Exponents)</h3>

			<p>
				In practice though, we cannot get that good of an error. Because we need a very large \(n\), to get a crowded enough feature space.
			</p>
			<p>
				In 2D, we can imagine our feature space to be a circle, with area \(O(r^2)\), thus \(n = O(r^2)\). In 3D, this increases to \(O(r^3)\) and realistically ML has way more than 3 dimensions, and so the amount of data we need \(n \approx c^d\), for \(d\) dimensions and a constant \(c\).
			</p>
			<p>
				We also have an <b>upper bound</b> error: the <b>constant classifier</b>, that always returns the most common label: simply set \(k=n\). Naturally, our model is only a success if it does (significantly) better than n-NN. 
			</p>
		</div>

		<div class="colourband">
			<h2 id="linreg">Linear Regression</h2>
			<p class="subtitle">
				Get ready for some Linear Algebra
			</p>
		</div>

		<div class="cbox">
			<p style="display: none;">
				\(\newcommand{\cal}{\mathcal}\)
			</p>
			<p>
				<b>Classification</b> predicts best classes, when labels are <i>discrete</i>. <b>Regression</b> predicts best fit, when labels are <i>continuous</i>.
			</p>

			<p>
				<b>Linear Regression</b> is a tool for modelling linear relationships - literally a line of best fit. 
			</p>
			<p>
				For training data \(\vec{X} \), labels \(\vec{y} \), we can define a regression function \(h:\vec{X} \longrightarrow \vec{y}\) as
				\[\hat{y}_i = h(\vec{x}_i) = \vec{w} \cdot \vec{x_i}\]
				Note that this is literally \(y = mx + c\) but without the \(+c\). \(\vec{w} \) (the weights) is the slope of the line, and this is what we want to find. 
			</p>
			<p>
				We find this by <b>minimising a loss function</b>, a function that describes the error of our predictions w.r.t. the training data. 
			</p>
			<p class="blue">
				The most common loss function used is <b>mean squared error</b> (MSE):
				\[
				\mathcal{L}(h) = \frac{1}{n} \sum_{i=1}^n (h(x_i | w) - y_i) ^ 2
				\]
				Basically, sum of the difference between predicted and actual \(y\), squared. Thus we find \(\argmin\limits_\vec{w} \cal{L}(h)\). This is known as finding <b>least squares</b>.
			</p>
			<p class="grey">
				Note that the loss function can be denoted \(\cal{L}(\cdot)\) or just \(f(\cdot)\), I often prefer the former when note taking but you see the latter used quite often. 
			</p>
			<p>
				 The loss function is quadratic in \(\vec{w} \) (in this case), and so we can find a minimum easily. 
			</p>
			<p>
				<b><i>Note.</i></b> Minimising \(\frac{1}{n} \sum(\cdots)\) is the same as minimising \(\sum(\cdots)\) or any other constant multiplier. \(1/2\) is often used for convenience of differentiation.
			</p>

			<h3>1D Regression</h3>

			<p>
				1D regression is the easiest case. We have a function \(\cal{L}(w) = \frac{1 }{2 } \sum\limits_{i=1}^n (wx_i - y_i)^2\) for a scalar \(w\).
				\begin{align}
					\therefore \cal{L}(w) &= \frac{1 }{2 }\sum_{i=1}^n (w^2x_i^2 - 2wx_iy_i + y_i^2) \\
					&= \frac{w^2 }{2} (\sum x_i^2) - \frac{2w }{2 } (\sum x_i y_i) + \frac{1}{2} (\sum y_i^2) \\
					&= \frac{a}{2} w^2 - bw + c 
 				\end{align}
				(Since the sum terms are constant)
			</p>
			<p>
				\(\cal{L}' (w) = wa-b\), set to 0 and solve for \(w\), \(w = \frac{b}{a}\)
			</p>
			<p>
				We can also use the second derivative to make sure it is a minimum. 
			</p>
			<p>
				This finds the best \(w\) ... for a line \(\hat{y}_i = wx_i\) that passes through 0, because we haven't accounted for <i>bias</i>.
			</p>

			<h3>Bias, Extending Dimensions</h3>

			<p>
				A <b>bias</b> is an extra constant added on, that shifts the result up or down. For 1D, we change the equation to \(\hat{y}_i = wx_i + w_0\) where \(w_0\) is the bias. 
				<!-- \(\newcommand{\xtrain}{\vec{x}_i} \newcommand{\xtest}{\vec{\tilde{x}}_i \newcommand{\yhi}{\hat{y}_i} \) -->
				
			</p>
			<p>
				We can add bias by <i>changing basis</i>: we make a "fake feature", with 1 for all the values:
				\[
					\vec{X} = \begin{bmatrix}
						x_i \\ x_2 \\ \vdots \\ x_n
					\end{bmatrix} \longrightarrow
					\vec{Z} = \begin{bmatrix}
						1 & x_1 \\ 1 & x_2 \\ \vdots &\vdots \\ 1 & x_n
					\end{bmatrix}
				\]
				Thus, we are effectively using \(\vec{Z}\) as our feature set, and have to solve
				\[
				y_i = v_1 z_{i1} + v_2 z_{i2} \equiv w_0 + w_1x_i\]
				Note that because we changed basis, our transformed features are now denoted \(z\) and our transformed wieghts are denoted \(v\).
			</p>
			<p>
				This creates 2D linear regression. 
			</p>
			<p>
				When we learn in 1D, we are fitting a line of best fit. In 2D, then, we are fitting a <i>plane</i> of best fit. And the dimensions of the (hyper)plain just increases. 
			</p>
			<p>
				For \(d\) features (ignoring bias), we have a
				\begin{align}
					\ypred &= w_1 x_{i1} + \cdots + w_d x_{id} \\
					&= \sum_{j=1}^d w_j x_{ij} \\
					&= \vec{w}^\top \xtrain \equiv \vec{w} \cdot \xtrain.
				\end{align}
				The loss is then the MSE:
				\[
				\cal{L} = \frac{1}{2} \sum_{i=1}^n (\wtrans \xtrain - y_i)^2
				\]
			</p>
			<p>
				Whilst finding derivatives for this can be difficult from a partial differentiation perspective, we can actually differentiate matrices fine, which we'll soon see.
			</p>

			<h4>Matrix Magic</h4>

			<p>
				It is probably good now to mention the dimensions of the data.
			</p>
			<ul>
				<li>
					\(\vec{y}: n \times 1 \)
				</li>
				<li>
					\(\xtrain : d \times 1\)
				</li>
				<li>
					\(\vec{X} : n \times d\); can be thought of as rows of \(\xtrain^\top\) (transposed so coln -> row)
				</li>
			</ul>
			<p>
				A single prediction \(\ypred = \vec{w} \cdot \xtrain\). All \(n\) samples can then be gotten from \(\vec{\hat{y}} = \vec{X} \vec{w}\).
			</p>
			<p>
				We say that \(\vec{\hat{y}} - \vec{y} = \vec{r}\), the <b>residual vector</b>, and being the error difference, we can write MSE using it:
				\[
				\cal{L}(\vec{w}) = \frac{1}{2} \lVert \vec{r}^2 \rVert^2 = \frac{1 }{2 }\lVert \vec{X} \vec{w} - \vec{y} \rVert^2
				\]
			</p>
			<p>
				Thus we can find an error using <i>matrix calculus</i>; a symbol will be used here called "nabla" \(\nabla\), which represents multi-dimensional differentiation. For the case of vectors, think of deriving each term element-wise. 
			</p>

			\begin{align}
				\implies \cal{L}(\vec{w}) &= \frac{1 }{2 } \vec{w}^\top \vec{X}^\top \vec{X} \vec{w} - \vec{w}^\top \vec{X}^\top \vec{y} + \frac{1 }{2 }\vec{y}^\top \vec{y} \\

				&= \frac{1 }{2 }\vec{A w^\top w} - \vec{b w^\top} + \frac{1 }{2 }c \pod{\vec{X^\top X, X^\top y, y^\top y} \textrm{ are constant}}  \\

				\nabla \cal{L} (\vec{w}) &= \nabla(\frac{1 }{2 }\vec{A w^\top w}) - \nabla (\vec{bw^\top}) + \nabla(\frac{1 }{2 }c) \\ 

				&= \vec{Aw-b}. \pod{\textrm{if } \vec{A} \textrm{ symmetric}}
			\end{align}

			<p>
				Then, all we need to find is \(\vec{Aw - b} = 0\), or equivalently \(\vec{X^\top X w - x^\top y} = 0\)
			</p>
			\[
			\implies \vec{w = (X^\top X)}^{-1} \vec{X^\top y}.
			\]

			<p>
				MSE has some issues however.
			</p>
			<ul>
				<li>
					MSE is sensitive to outliers, as large differences are empasised more greatly.
				</li>
				<li>
					Finding a model can be memory and time intensive - there are a lot of large matrix multiplications, and large systems of equations. 
				</li>
				<li>
					May <i>extrapolate</i>: predicting values outside of the measured range.
				</li>
				<li>
					Solution may not be unique: <i>colinear features</i>, where 2 features are identical for all samples, which means that both, say, \(y_i = w_1 x_1 + w_2 x_2 \equiv y_i = 0 x_1 + (w_1 + w_2)x_2\), are valid solutions.
				</li>
				<li>
					And finally, linear regression assumes a <b>linear</b> relationship.
				</li>
			</ul>
			
			<p class="blue">
				<b><i>Note.</i></b> The term <b>convex</b> refers to functions with a shape like \(\cup, \cap\), meaning they have only one minima (or maxima). It is easy to optimise these functions (not so for non-convex ones)
			</p>

			<h3>Polynomial Regression</h3>

			<p>
				Often, there is <i>not</i> a linear relationship. 
			</p>

			<p>
				So what do we do? Well, we can very easily "hijack" least squares linear regression to do polynomial regression, in the same method that we use to add a bias. 
			</p>

			<p>
				Suppose again 1D samples \(\vec{X}\), that we suspect has a polynomial relationship to \(\vec{y}\), we can also change the basis to account for squares:
			</p>

			\[
				\vec{X} = \begin{bmatrix}
					x_i \\ x_2 \\ \vdots \\ x_n
				\end{bmatrix} \longrightarrow
				\vec{Z} = \begin{bmatrix}
					1 & x_1 & x_1^2 \\ 1 & x_2 & x_2^2 \\ \vdots &&\vdots \\ 1 & x_n & x_n^2
				\end{bmatrix}
			\]

			<p>
				And would you look at that, we now have the squared features as a separate feature. Now simply fit a line \(\ypred = v_1 z_{i1} + v_2 z_{i2} + v_3 z_{i3}\).
			</p>
			<p>
				We can also naturally extend this to higher polynomials. <i>However</i>, too high polynomials leads to overfitting - whilst training error may go down, test error goes up. 
			</p>
			<p>
				That is, fit new parameters \(\vec{V}\) on transformed featuers \(\vec{Z}\), by solving for \(\vec{ZZ^\top v = Z^\top y} \). 
			</p>
			<p>
				To predict on a validation set \(\vec{\tilde{X}}\), first transform it into the polynomial form \(\vec{\tilde{Z}}\), then do \(\vec{y = \tilde{Z} v }\).
			</p>

			<h4>L2 Regularisation</h4>

			<p>
				Training a regression model may produce excessively large weights.Exessively large weights are very sensitive, thus are more likely to be overfit -> penalise large weights. 
			</p>

			<p class="blue">
				<b>L2 Regularisation</b> is an extra bit we add to our MSE loss:
				\[
				\cal{L}(\vec{w}) = \frac{1}{2} \lVert \vec{Xw - y}\rVert^2 + \frac{\lambda}{2}\lVert \vec{w} \rVert^2 
				\]
				Note the new \(\frac{\lambda}{2}\lVert \vec{w} \rVert^2 \) term. This new lambda parameter controls how much emphasis we want on small weights. A high \(\lambda\) means that we want small weights more than least squares, whereas \(\lambda = 0\) removes the penalty altogether.
			</p>
			<p class="grey">
				\(\frac{\lambda}{2 }\) is done for ease of differentiation.
			</p>
			<p>
				Furthermore, the derivative \(\nabla \cal{L}(\vec{w}) = \vec{X^\top X w - X^\top y + } \lambda \vec{w}\) solves for the equation \((\vec{X^\top X + \lambda I}) \vec{ w = X^\top y}\), and the matrix that multiplies into \(\vec{w}\) is <i>always</i> invertible.
			</p>
		</div>

		<div class="colourband">
			<h2 id="robreg">Robust Regression</h2>
		</div>

		<div class="cbox">
			<h3>Outliers and Loss</h3>

			<p>
				As mentioned, MSE is greatly affected by outliers, as large deviations are emphasised more. Instead, we can use absolute error (instead of squared error) to de-emphasise large deviations. 
			</p>
			<p>
				Absolute (L1) loss is defined as 
				\[
				\cal{L}(\w) = \sum_{i=1}^n |\w\t \x_i - y_i|
				\]
			</p>
			<p>
				Minimising L1 though is much harder, as the absolute function is <i>non-differentiable</i> at \(x=0\), and the gradient is not smooth.
			</p>
			<p>
				This poses problems for our minimisation equations, problems that L2 doesn't. Thus, we want a loss function that gets the best of both worlds.
			</p>
			<p class="blue">
				The <b>Huber loss function</b> is defined
				\[
				\cal{L}(\w) = \sum_{i=1}^n h(\w\t \x_i - y_i) 
				\]
				Where
				\[
				h(r_i) = \begin{cases} \frac{1 }{2 }r_i^2 & |r_i| \leq \epsilon \\ \epsilon(|r_i| - \frac{1 }{2 }\epsilon) & \textrm{otherwise. } \end{cases}
				\]
				For a small number \(\epsilon\). 
			</p>
			<p>
				Huber function is differentiable, becoming
				\[
				h'(r_i) = \begin{cases} r_i & |r_i| \leq \epsilon \\ \epsilon \cdot \textrm{sign}(r_i) & \textrm{otherwise} \end{cases}
				\]
				Where \(\textrm{sign}(x) = 1 \textrm{ if } x > 0 \textrm{ else } -1\).
			</p>

			<h3>Standardising Features</h3>

			<p>
				Standardising features involves rescaling features so that they're all proportional to each other. 
			</p>
			<p>
				In some cases, we don't need to -- for basic models like Bayes and Decision Trees, we don't need to change. For least squares, though weights may be different, fundamentally the model is still the same. 
			</p>
			<p>
				However, standardisation is important for KNN (as large numbers can skew distance unnecessarily) and for regularised least squares (as penalty must be proportional).
			</p>
			<p>
				Data is standardised <b>feature-wise</b>; for each feature \(j\):
			</p>
			<ul>
				<li>
					Compute mean \(\mu_j = \frac{1}{n} \sum_{i=1}^n x_{ij}\)
				</li>
				<li>
					Compute standard deviation \(\sigma_j = \sqrt{ \frac{1 }{n} \sum_{i=1}^n (x_{ij} - \mu_j)^2 }\)
				</li>
				<li>
					Replace every \(x_{ij}\) by \(\frac{x_{ij} - \mu_j}{\sigma_j}\)
				</li>
			</ul>
			<p class="side">
				<b>HOWEVER</b>, when standardising test data, <b>do not use test data mean/stdev</b>s, because technically, we don't know test data, and our assumption is that the training set is a good representation of any new data.
			</p>
			<p>
				Labels/targets/ground truths can also be standardised, in the same way. 
			</p>

			<h3>Feature Selection and Regularisation</h3>

			<p>
				Sometimes, we have a lot of features, but only a few of them matter. 
			</p>
			<p>
				We'd like the model to learn which features of \(\X\) are important. 
			</p>
			<p>
				One way we can do this is to look at the trained weights \(\w\), and take all features \(j : |w_j| >\) a predifined threshold. 
			</p>
			<p>
				Unfortunately, this has some downsides:
			</p>
			<ul>
				<li>
					It struggles with coliniearily and may give wrong results then, as if \(x_1, x_2\) are coliniear, then \(w_1 x_1 + w_2 x_2 \sim (w_1 + w_2) x_1 + 0x_2\).
				</li>
				<li>
					May pick up irrelevant features which happen to cancel: given irrelevant \(x_a, x_b\), then \(0x_a + 0x_b \sim -1000x_a + 1000x_b\).
				</li>
			</ul>

			<p>
				Alternatively we can use a tactic of <b>search and score</b>, by defining (or using a function) \(f\) that computes a "score" of how good this set of features is -- such as the loss function.
			</p>
			<p>
				Of course, this risks overfitting, especially since we're iterating over all \(2^d\) subsets, the optimisation bias can be high. 
			</p>

			<p>
				Instead, we modify the loss function to add a <b>complexity penalty</b>, such as
				\[
				\cal{L}(s) = \frac{1 }{2 }\sum_{i=1}^n (\w_s \cdot \x_{is} - y_i)^2 + \textrm{size}(s)
				\]
				for a <i>subset</i> of features \(s\). The size function counts the number of nonzero weights -- the <b>L<sub>0</sub> norm</b>;
			</p>

			<p>
				Our Loss score becomes 
				\[
				\cal{L}(\w) = \frac{1}{2} \lVert \X\w - \y \rVert^2 + \lambda \lVert \w \rVert_0
				\]
				With a parameter lambda.
			</p>

			<p>
				The problem is, L0 is not convex ... nor differentiable, whereas L2 was very nice indeed, so if we can find a best of both worlds:
			</p>

			<p class="blue">
				Use the convex and (mostly) differentiable <b>L<sub>1</sub> loss</b>:

				\[
				\cal{L}(\w) = \frac{1}{2} \lVert \X\w - \y \rVert^2 + \lambda \lVert \w \rVert_1
				\]

				Also called <b>LASSO</b> regularisation.
			</p>

			<p>
				A <b>sparse</b> matrix has many zeros: this is a good sign of feature selection.
			</p>

			<p>
				Both L0 and L1 (LASSO) prefer sparsity, whilst L2 (Ridge) doesn't.
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="clustering">Clustering</h2>
		</div>

		<div class="cbox">

			<h3>Introduction</h3>

			<p>
				<b>Clustering</b> is a type of <b>unsupervised learning</b>.
			</p>
			<p>
				In unsupervised learning, we are given no labels, and have to infer the underlying "latent" structure. 
			</p>
			<p>
				This is especially useful for outlier detection, grouping, <i>latent factor models</i> (later), and visualisation, among others.
			</p>

			<p>
				For clustering, we are trying to group data into clusters based on their "similarity", and discover the distribution of data. 
			</p>
			<p>
				Often this data is <i>multimodal</i>, where there are more than one "peak" in its distribution (vs, say, a normal distribution, which has one mode).
			</p>

			<p>
				Some assumptions:
			</p>
			<ul>
				<li>The features \(\X = \{\x_1 \cdots \x_n\}\) are in <b>euclidean space</b>.</li>
				<li>\(\X\) can be grouped into \(K\) groups or clusters (this <i>is</i> a hyperparameter);</li>
			</ul>

			<h3>K-Means Clustering</h3>

			<p>
				Assuming that there are K clusters, and that each sample is close to a cluster centre. <span class="grey">In the off chance you're like me and misread K-means, "means" refers to "averages" like of data, and not "methods".</span>
			</p>

			<p>
				We use a <b>heuristic-based</b> improvement: starting with initial random means, and iterate by:
			</p>
			<ul>
				<li><b>Fix means, optimise points.</b> Assigning each \(\x_i\) to its closest mean.</li>
				<li><b>Fix points, optimise means.</b> Recompute means</li>
				<li>Refine until convergence (the means do not move or do not move enough)</li>
			</ul>
			<p>
				And our metric of goodness (i.e. loss function) will be defined as:
			</p>
			<p class="blue">
				For cluster means \(\vec{m} \) and assignments \(\vec{r} \), the <b>mean squared distance</b>:

				\[
				\cal{L}(\{\vec{m}\}, \{\vec{r}\}) = \sum_{i=1}^n \sum_{j=1}^K r_j^i \lVert \vec{m}_j - \x_i \rVert^2 \pod{r^i_j \in \{0,1\}}
				\]

				Where \(r^i_j\) of data \(i\) to cluster \(j\) is 1 if that data is assigned to that cluster, or 0 otherwise. 
			</p>
			<p>
				Note: thus each \(\x_i\) will have a \(\vec{r}\) being a <a href="https://en.wikipedia.org/wiki/One-hot"><i>one-hot-encoding</i></a> of its cluster.
			</p>

			<p>
				The way a sample is assigned to a mean \(k\) is via euclidean distance \(k_i = \argmin\limits_k \lVert \vec{m}_k - \vec{x}_i \rVert^2\)
			</p>
			<p>
				And means are adjusted via \(\vec{m}_k = \dfrac{\sum_i r^i_k \x_i}{\sum_i r^i_k }\)  -- we need to include \(r^i_k\) to ensure we don't include features not assigned to that mean currently.
			</p>

			<p>
				While K-means' loss heuristic is non-convex (i.e. not guaranteed global minimum), we <i>are</i> guaranteed a <i>local</i> minimum, we won't diverge ever. 
			</p>

			<p>
				Some issues though:
			</p>
			<ul>
				<li>
					\(K\) is a hyperparameter - if you don't know how many there should be, welp, sucks for you.
				</li>
				<li>
					Each example can only be assinged to one cluster (fixable)
				</li>
				<li>
					May converge to sub-optimal groups (can use methods such as random restart, non-local split and merge to fix)
				</li>
			</ul>

			<h3>Soft K-means</h3>

			<p>
				Soft K-Means makes <b>partial assigments</b> -- a sample can have partial membership of two clusters. For example, if an \(\x\) is a point that is very close to mean A and further but still close to mean B, it could be given the assignment \(\vec{r} = \{A = 0.84, B = 0.16\}\), like a <i>probability</i>.
			</p>
			<p>
				We also call this <i>fuzzy k-means</i>.
			</p>
			<p class="blue">
				We must set that all assignments must sum to one, using an important function called <b>softMax</b>:

				\[
				r^i_k = \frac{\exp(-\beta \lVert \vec{m}_k -\x_i \rVert^2)}{
					\sum\limits_{j=1}^K \exp(-\beta \lVert \vec{m}_j -\x_i \rVert^2))
				}
				\]

				Where \(\exp(x)\) is the exponential function: \(e^x\). <br>
				\(\beta\) is the <b>stiffness parameter</b>, controlling how "soft" assignments are -- emphasising one main class or allowing a more even spread.
			</p>
			<p>
				If you plot the softMax curve, the stiffness controls how steep the center bit is. 
			</p>
			<p>
				For Soft K-means, we alter the loss function slightly. We say that now \(r^i_j \in [0,1]\), that we can get a range of values. Then, for one mean the loss is \(\newcommand{\m}{\vec{m}} \m_l = \argmin_\m \sum\limits_{i=1}^n r^i_l \lVert \m - \x_i \rVert^2\), and overall
				\[
				\cal{L}(\m) = \sum_{l=1}^n \vec{r}^i \lVert \m - \x_i \rVert^2
				\]
			</p>
			<p>
				This loss function is actually quadratic - there is a unique solution. 
			</p>
			<p>
				Said unique solution is given by solving the first derivative \(\nabla \cal{L} (\m) = \sum_{i=1}^n \vec{r}^i(2\m - 2\x_i).\)
			</p>

			<p>
				What we can do with K-means is actually do a primitive form of data reduction - approximating the data onto a lower space. In this case, by replacing all data points with their closest means, we do <b>vector quantisation</b>, which has this effect.
			</p>

			<p>
				Thus overall, the soft k-means algorithm is as thus:
			</p>

			<ol>
				<li>
					Assing points via \(r_k^i = \textrm{softMax}_\beta (\lVert \m_k - \x_i \rVert^2 \)
				</li>
				<li>
					Update points via \(\m_k = \dfrac{\sum_{i=1}^n r_k^i \x_i}{\sum_{i=1}^n r_k^i }\)
				</li>
			</ol>
		</div>

		<div class="colourband">
			<h2 id="pca">Principle Component Analysis (PCA)</h2>
		</div>

		<div class="cbox">

			<h3>Introduction</h3>
			<p>
				<b>Principal Component Analysis</b> is fundamentally about approximating a feature set in a lower-dimensional <b>"latent space"</b> or <b>subspace</b>. i.e. reduce the dimensions, to extract higher level features, or to store less data overall.
			</p>
			<p>
				One can think about images: images are comprised of possibly hundreds or thousands of pixels, but we can extract larger regions, to act as "components" to reconstruct said image. 
			</p>
			<p>
				<b>Latent Factor Models</b> (LFMs) are ML models that convert data to latent spaces.
			</p>
			<p>
				We call the original dataset \(\X\), and can call the transformed "latent" set \(\Z\).
			</p>

			<p>
				One method of this is <b>vector quantisation</b> - by doing k-means and then replacing each feature with its mean, we have replaced a sample sized \(1 \times d\) (d features) with \(1 \times k\) (k means). Now of course, if \(k > d\), this is hardly helpful.
			</p>
			<p>
				Instead, we want to somehow learn the <i>basic components</i> that make up a sample \(x\), and weight them appropriately to reconstruct the original sample (as well as possible).
			</p>

			<h3>PCA</h3>

			<p>
				The PCA algorithm takes an \(\X\), and produces two vectors, \(\Z, \W\), which are in the following form:
			</p>

			<figure>
				<img src="./assets/pca-matrix.png" alt="PCA matrix" style="max-width: 500px;">
				<figcaption>
					Such that \(\X = \Z\W\), note the dimensions, \((n\times d) = (n \times k) \times (k \times d)\)
				</figcaption>
			</figure>

			<ul>
				<li>
					\(\W\) are our <b>principal components</b>, the components needed to reconstruct the features of \(\X\), ranging from the most fundamental component at the top (the first <b>row</b>), to the least important component at the bottom. We have \(k\) of these. 
				</li>
				<li>
					\(\Z\) are the <b>weights</b> that reconstruct \(\W\) into \(\X\) - each \(\x_i = \W\t \z_i\). One single feature of one sample, \(x_{ij} \) involves multiplying row \(i\) of <b>Z</b> with column \(j\) of <b>W</b>.
				</li>
				<li>
					\(k \leq d\). Note that if \(k \ll d\), we can save a lot of storage space. <span class="grey"><i>The proof is trivial and left as an excersise to the reader.</i></span>
				</li>
			</ul>
			<p>
				PCA is a <b>linear</b> reduction - it can't extract non-linear features on its own. It's essentially just doing a matrix factorisation.
			</p>
			<p>
				PCA is also useful especially to visualise high-dimensional data, as we only live in 3D. 
			</p>

			<h4>The Theory</h4>

			<p>
				[1] To do PCA, we must first <b>center</b> data. Given an \(\X = \{\x_1, \x_2, \dots, \x_d\}\), compute the mean of its <b>features</b>
				\[
				\vecu{\mu} = \fr{1}{n} \sum_{i=1}^n \x_i
				\]
				and then take it away from \(\X\) row-wise; \(X - \vecu{\mu}\). <span class="grey">I'm using underline vector notation for mu, since LaTeX doesn't boldface greek fonts.</span>
			</p>

			<p>
				[2] To solve PCA, we need to learn weights, and adjust based on <b>reconstruction error</b>. We can use MSE for this:
				
				\begin{align}
				\cal{L}(\W, \Z) &= \sum_{i=1}^n \lVert \W\t \z_i - \x_i \rVert^2 \\
				&= \lVert \Z\W - \X \rVert^2_F
				\end{align}

				for two matrices, \(\W, \X\). Note the subscript F is the <b>frobenius norm</b> -- L2 norm but over matrices. 
			</p>

			<p>
				[3] The PCA objective function is non-convex -- it doesn't have a unique global minimum. This makes finding a solution hard.
			</p>
			<p>
				We can enforce a few constraints on each row/component to help us reduce bias and redundancy: 
				<ol>
					<li>Normalisation: enforce \(\lVert \w_c \rVert = 1\)</li>
					<li>Orthogonality: enforce \(w_c \cdot w_{c'} = 0 \forall c \neq c'\)</li>
					<li>Sequentially fit components (fit \(w_0\) first, and \(w_d\) last)</li>
				</ol>
				These constraints are important for our PCA to be effective. Below we can see what this does to a 2D space.
			</p>

			<figure>
				<img src="https://miro.medium.com/max/900/1*_G3fNG6wHJMm9ZpJ7Mw9dg.png" alt="" style="max-width: 600px;" title="">
				<figcaption>
					PCA effectively finds new <i>basis</i> vectors for a set of data. At \(d\) vectors, we can use this to perfectly reconstruct the data -- though of course this no longer is dimension reducing. <a href="https://medium.com/@gaurav_bio/creating-visualizations-to-better-understand-your-data-and-models-part-1-a51e7e5af9c0">[img]</a>
				</figcaption>
			</figure>

			<p>
				It is easiest to find the first, or "main" axis of the data, and then find subsequent ones. We can also see all the data has been centered -- this ensures our components are not skewed one way or another, and originate from 0.
			</p>

			<p>
				[4] Finally, to actually solve this, we use <b>Single Value Decomposition</b> (SVD).
			</p>

			<h4>SVD</h4>

			<div class="blue">
				<p>
					Given an \(\X \in \bb{R}^{n \times d}\), SVD decomposes \(\X\) into the following:
					\[
					\X = \vec{U} \vec{\Sigma} \vec{V}\t
					\]
				</p>
				<ul>
					<li>
						\(\vec{\Sigma} \in \bb{R}^{n \times d} \) is a diagonal matrix of eigenvalues
					</li>
					<li>
						\(\vec{U} \in \bb{R}^{n \times n}\) is an orthonormal matrix (orthogonal* and normal columns)
					</li>
					<li>
						\(\vec{V}\t \in \bb{R}^{d \times d}\) is our orthogonal matrix of components.
					</li>
				</ul>
				<p>
					In linear PCA, \(\vec{V}\t = \W\). We can then take the top \(k\) rows.
				</p>
				<p>
					*a matrix <b>A</b> is orthogonal if \(\vec{A}^{-1} = \vec{A}\t\).
				</p>
			</div>

			<p>
				The eigenvalues in \(\vec{\Sigma}\) are <i>related</i> to \(\nm{\X}\t \nm{\X} \) (where \(\nm{\X}\) is the normalised matrix X). The actual eigenvalues are \(\fr{\vec{\Sigma}.^2}{n-1}\) (\(.^2 \) is element wise squared), though I have not found this to be useful.
			</p>
			<p>
				\(\vec{U}\) is simply not useful. And \(\vec{V}\t \equiv \W\) is <b>very</b> useful.
			</p>

			<div class="side">

				<p>
					To approximate on test data, we need our mean \(\vecu{\mu_j}\) from the <i>training data</i>, and our components \(\W\).
				</p>

				<p>
					Given \(\tilde{\X}\), center it using \(\vecu{\mu_j}\), then compute the new \(\tilde{\Z}\):

					\[
					\tilde{\Z} = \tilde{\X}\W\t
					\]

					And we can use \(\Z\). We can work out the reconstruction error by doing \(\lVert \tilde{\Z} \W - \tilde{\X} \rVert_F^2\)
				</p>
			</div>

		</div>

		<div class="colourband">
			<h2 id="linclass">Linear Classification</h2>
		</div>

		<div class="cbox">

		</div>

		<div class="colourband">
			<h2 id="kernel">Kernel Trick</h2>
		</div>

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="../../js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
</body>
</html>