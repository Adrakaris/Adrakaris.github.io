<!DOCTYPE html>
<html>
<head>
	<title>CS342</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="./" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="./blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="./about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS342</h1>
					<p class="subheading">Machine Learning</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Contents</h1>
			</div>
		</header>

		<!-- <div class="buttonwrapper beside" >
			<a href="./index-zh.html">简体中文</a>
		</div> -->
		<!-- REMEMBER TO DO! -->

		<div class="cbox">
			<ol>
				<li><a href="#basics">The Basics</a></li>
				<li><a href="#desctree">Decision Trees</a></li>
				<li><a href="#bayes">Naive Bayes</a></li>
				<li><a href="#knn">K-nearest Neighbours</a></li>
			</ol>
		</div>

		
		<div class="colourband">
			<h2 id="basics">The Basics</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<p>
				<b>Machine Learning</b> is the process of getting machines to learn to model stuff, that is, given experiences <i>E</i> to learn from, a set of tests <i>T</i> to measure against, for which we measure a performance <i>P</i>. 
			</p>
			<p>
				Patterns are learned from <i>features</i>, qualities that describe the data, which change depending on the data in question. 
			</p>
			<p class="centre">
				Input -> Extracting features (manual) -> Training and classification -> Results
			</p>
			<p>
				Getting the machine to extract features itself is called <i>representation learning</i>, and is generally used in deep neural networks.
			</p>

			<h3>Recap</h3>

			<p>
				Machine learning takes basis on probability (and a lot of stats) - especially joint probability distributions (like \(p(A, B)\)).
			</p>

			<p>
				Recall:
			</p>
			<ul>
				<li>The <b>marginalisation rule</b> \(p(A) = \sum\limits_{\forall x} p(A, B=x)\)</li>
				<li><b>Random Variables</b> X, with an event \(X = x\) which occurs with some probability, the sum of all possible events is 1.</li>
				<li><b>Conditional probability</b> \(p(A|B) = \frac{p(A,B)}{p(B)} \)</li>
				<li><b>Bayes Rule</b> \(p(A|B) = \frac{p(B|A)p(A)}{p(B)}\)</li>
			</ul>

			<p>
				Datasets are collections of samples - the range of values are limited and probably in some sort of <i>distribution</i>, which can take many forms, such as uniform (straight line) or gaussian/normal (bell curve).
			</p>
			<p>
				Datasets can have a <b>mean</b> \(\mu\) and <b>standard deviation</b> \(\sigma^2\) -- variance is stddev-squared.
			</p>

			<div class="cornell">
				<div>
					<p>Distribution Functions</p>
				</div>
				<div>
					<p>
						<b>Probability density (PDF)</b>: probability of observing a value, i.e. \(p(X = x)\)

						\[ p(x|\mu, \sigma^2 ) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{(x-\mu)^2}{\sigma^2}) \]
					</p>
					<p>
						<b>Cumulative density (CDF)</b>: probility of observing &leq; the value \(p(X \leq x)\)

						\[p(X \leq x|\mu, \sigma^2) = \int_0^x \textrm{(curve)} dx\]
					</p>
				</div>
			</div>

			<p>
				<b>Expectation</b>, the expected value, is denoted \(\mathbb{E} \), and given as 
				\[\mathbb{E}[x] = \sum_{\forall x} p(X = x) \cdot x\]
			</p>

			<p>
				<i>Notation:</i> the module uses standard notations for everything: \(\renewcommand{\vec}{\mathbf}\)
			</p>
			<div class="cornell">
				<div>
					<p>Scalar</p>
				</div>
				<div>
					<p>
						\(x\): lowercase (or upper case) letter.
					</p>
				</div>
				<div>
					<p>Column Vector</p>
				</div>
				<div>
					<p>
						\(\vec{x}\): bold lowercase letter. Vectors are <i>always</i> columns, so row vectors are <i>always</i> represented as \(\vec{x}^\top\). Dot products between vectors are often represented \(\vec{w}^\top \vec{x}\) (a matrix multiplication), though I may also use \(\vec{w} \cdot \vec{x}\)
					</p>
				</div>
				<div>
					<p>Matrix</p>
				</div>
				<div>
					<p>
						\[\vec{X} = \begin{bmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{bmatrix}\]
						Upper case bold letter. If a matrix is symmetric, \(\vec{X}^\top = \vec{X} \).
					</p>
				</div>
			</div>

			<p>
				<b>Supervised learning</b> is when both questions and answers are given, and the model has to learn to predict answers from samples. 
			</p>
			<p>
				<b>Unsupervised learning</b> is when no answers are given, and the model has to find relationships between samples.
			</p>

			<h3>General Data Notation</h3>

			<p>
				\(\def\bb#1{{\mathbb{#1}}}\)
				Samples are stored in a matrix, generally denoted \(\vec{X}\), where each row is a sample, a <b>feature vector</b>, and each column is an individual feature. \(\vec{X} \in \bb{R}^{n \times d}\), which means that X has \(n\) rows (for n samples) and \(d\) columns (for d features/dimensions)
				
				<ul>
					<li>
						\(\vec{x}_i\) is a sample row, represented as a column (as a row, it is consistently denoted \(\vec{x}_i^\top\)).
					</li>
					<li>
						\(\vec{x}_j \) is a column of features. 
					</li>
					<li>
						\(x_{ij}\) is feature \(j\) of column \(i\).
					</li>
				</ul>
			</p>
			<p>
				Labels are stored in vector \(\vec{y}\), \(y_i\) being the label for an \(\vec{x}_i\). This is also called the <b>ground truth</b>
			</p>
			<p>
				Given a new sample \(\tilde{\vec{x}}_i\) (tilde represents test data), we can make a prediction \(\hat{y}_i\) for it (the hat represents predicted). 
			</p>
		</div>

		<div class="colourband">
			<h2 id="desctree">Decision Trees</h2>
		</div>
		
		<div class="cbox">
			<h3>The Scenario</h3>

			<p>
				Suppose you start to frequently get upset stomachs, and worry that it is due to an allergy that you don't know about, perhaps to one food, perhaps to a combination. So, you decide to keep a food diary, detailing what allergen-inducing foods you consume each day, in what amounts, and whether or not you had an upset stomach that day. 
			</p>

			<table>
				<tr>
					<th>Milk</th>
					<th>Eggs</th>
					<th>Peanuts</th>
					<th>Crustaceans</th>
					<th>Upset?</th>
				</tr>
				<tr>
					<td>0.3</td>
					<td>0.2</td>
					<td>0</td>
					<td>0.7</td>
					<td>1</td>
				</tr>
				<tr>
					<td>...</td>
					<td>...</td>
					<td>...</td>
					<td>...</td>
					<td>...</td>
				</tr>
			</table>

			<p>
				This is a type of <i>supervised learning</i> problem, we have our features (foods and amounts) and our classes (upset stomach / not), and want to train a <i>classification</i> algorithm to predict things. 
			</p>

			<h3>The Decision Tree</h3>

			<p>
				A simple classifier is a <b>decision tree</b>
			</p>

			<p>
				A decision tree is essentially a nested series of if-elif-elses, that split the dataset up -- these are <i>splitting rules</i>. 
			</p>

			<figure>
				<img src="./assets/desctree-ex.png" alt="Decision tree with milk and eggs" style="max-width: 550px;">
			</figure>

			<p>
				Our aim is then to find these rules, which can be done recursively:
			</p>

			<h3>Stumps</h3>

			<p>
				The most basic decision tree is a <b>stump</b>. A stump is just one single splitting rule, the smallest possible tree. 
			</p>
			<p>
				To find a tree, we must first find a stump, but we need to find the stump that splits the best. How? With some scoring metric. 
			</p>
			<p>
				Ex. Could use plain accuracy, take a rule like <code>eggs > 0.2</code> and set a True and False coming off it - we now have a stump that can predict data. Pick the rule with the highest amount of data correctly predicted. 
			</p>
			<p>
				In our stump finding then, our training error would be just the percentage of \(\hat{y}_i \neq y_i\), i.e. the accuracy.
			</p>

			<h3>Greedy Recursive Splitting</h3>

			<p>
				A stump, however, can only split on one thing, and if we have \(d\) features we kinda want more splits, otherwise we're only ever using one of the features. 
			</p>
			<p>
				A common algorithm is <b>greedy recursive splitting</b> (GRS), which goes as follows:
			</p>
			<ol>
				<li>Find a stump with the best score metric, and split the dataset into two smaller ones at that stump. <span class="grey">(The new split datasets would probably still have a mix of \(y\)s, but hopefully are much more uniform than before)</span></li>
				<li>
					<i>Recurse:</i> "fit" (find) a new stump onto each of the smaller datasets, this is linked onto the previous stump.
				</li>
				<li>
					Repeat until accuracy threshold reached <span class="grey">(or depth reached, or some other limit. Accuracy <i>can</i> be 100%, but this may result in an overcomplex tree and not always be helpful, "overfitting").</span>
				</li>
			</ol>
			<p>
				Currently, we have accuracy as a possible metric, but this may not always be good, especially when there's multiple possible stumps that increase accuracy by the same amount, or the data is spread in such a way that a single line does not do much for accuracy. 
			</p>
			<p>
				A better way is if we could prioritise variance reduction, some sort of ... <i>information gain</i>:
			</p>

			<div class="cornell">
				<div>
					<p>Entropy</p>
				</div>
				<div>
					<p>
						<b>Entropy</b> of labels is essentially the randomness of their values. For a total of \(C\) categories, this can be calculated as
						\[E = -\sum_{c=1}^C p_c \cdot \log_2 (p_c)\]
						where \(p_c\) is the <i>proportion</i> of \(c\) out of all total labels in that (sub)set. Note that \(0\log_2 0\) is defined to just be 0.
					</p>
					<p>
						A lower entropy (min. 0) means less randomness, which is better. A higher entropy (max. \(log_2 C\), or 1 for binary classes) is worse. 
					</p>
				</div>
				<div>
					<p>Information Gain</p>
				</div>
				<div>
					<p>
						The principle of <b>Information Gain</b> (IG), is that we want the biggest difference in entropy pre-split to post-split. 
					</p>
					<p>
						For a single rule, we can split the set of \(n\) elements into \(n_{yes}\) (for the samples that match the rule) and \(n_{no}\) for the ones that don't. Thus IG is defined
						\[
						IG = E(\vec{y}) - \frac{n_{yes}}{n} E(\vec{y}_{yes}) - \frac{n_{no}}{n} E(\vec{y}_{no})
						\]
						i.e the entropy of combined labels \(\vec{y}\) minus the entropy of split labels individually.
					</p>
					<p>
						The higher this difference is, the better the split is.
					</p>
				</div>
			</div>

			<h3>Overfitting and Learning Theory</h3>

			<p>
				If we have 100% accuracy on our training data, this does not imply that we have 100% accuracy on any new test data. 
			</p>

			<ul>
				<li>
					<b>Test Accuracy</b> is the accuracy on new data
				</li>
				<li>
					<b>Overfitting</b> or <b>Optimisation Bias</b> is high accuracy on training data but low on test data 
				</li>
				<li>
					<b>Underfitting</b> is low accuracy on training data.
				</li>
			</ul>
			<p>
				We naturally want the right amount of fitting, but this is difficult. 
			</p>
			<p>
				We can use a test/verification set \(\tilde{\vec{X}}\) to verify, but we must ensure that we <i>do not train using the verification set</i>.
			</p>
			<p>
				The <b>IID Assumption</b> (Independent and Identically Distributed) states that we assume that the training data is a good representation of all data - that is, it has the same distribution as test data. 
			</p>
			<p>
				Whilst this is often not true, it is generally a good assumptuion. 
			</p>
			<p>
				<b>Learning Theory</b> gives us a metric for measurement, \(E_{approx}\) the <b>approximation error</b>, the error between training and testing:
				\[E_{test} = E_{approx} + E_{train}\]
				where \(E_{approx} = E_{test} - E_{train}\). If this is small, then we can say that the training data is good for test data. It tends to go down as the training set gets larger, and up as the model gets more complex (and thus overfit). 
			</p>

			<p>
				However, this approximation error leads to a <b>fundamental tradeoff</b>: minimise \(E_{approx}\) whilst minimising \(E_{train}\): make the model not overfit whilst having it be fit as well as possible. 
			</p>

			<h3>Validation</h3>

			<p>
				Usually, we split a training set into two: a larger <code>x_train</code> (the "training set"), and a smaller <code>x_test</code>, what's called the <b>validation</b> (or colloquially test) set. 
			</p>

			<p>
				Once we train on <code>x_train</code>, we can compute the accuracy of predicting <code>x_test</code>, and use this to adjust our model, such as manually setting <i>hyperparameters</i>.
			</p>

			<p>
				<b>Hyperparameters</b> are settings in a model which we <i>do not learn</i>. For decision trees, this may be the maximum depth of the tree, or the accuracy threshold. For more complex models like Neural Networks, the entire architecture is hyperparameter, whilst the weights and biases are learned, regular parameters.
			</p>

			<p>
				With simpler models, it is often good to train it multiple times with different hyperparameter settings (like tree depth), and see which one minimises both training error and approximation error the best.
			</p>
			<p>
				Sometimes, once hyperparameters are set, a model is trained again, with <code>x_test</code> merged back into the training set (no validation).
			</p>

			<p>
				That said, we may end up overfitting the validation set. If we search too many and too specific hyperparameters, we may end up finding one that's really good for the validation set, but not so good for any new data. 
			</p>

			<ul>
				<li>
					Optimisation bias increases as we search for more model configurations 
				</li>
				<li>
					It decreases as the training data gets larger.
				</li>
			</ul>

			<p>
				This leads to <b>another fundamental tradeoff</b>: increasing validation size means our model is less likely to be overfit. However, increasing validation size decreases test size, which makes our model train less well.
			</p>

			<h4>Cross Validation</h4>

			<p>
				Validation takes from test data, if test data is precious and hard to get, we may not want to waste it.
			</p>
			<div class="blue">
				<span class="nopad">
					<p>
						<b>K-fold cross validation</b> gives a good estimate without wasting too much data.
					</p>
	
					<p>
						For K folds, we train K times, each time taking \(\frac{1}{k}
						\) of \(\vec{X}\) as validation, and the rest \(\frac{k-1}{k}\) as train. The average validation accuracy is taken as the overall accuracy
					</p>
					
				</span>
				
			</div>

			<p>
				<i>Leave One Out (LOO)</i> is a special type of K-fold validation where \(k = n\).
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="bayes">Naive Bayes</h2>
		</div>

		<div class="cbox">
			<h3>The Scenario</h3>

			<p>
				You want to filter out which emails incoming are spam or not. For this, you've collected many, many emails, and labelled them correctly.
			</p>
			<p>
				Your features are going to be keywords in emails, thus, \(x_{ij} = 1\) if the word \(j\) is in email \(i\).
			</p>

			<h3>Probabilistic Classification</h3>

			<p>
				Traditional filtering/classification methods used something called <b>Naive Bayes</b> - a probability based classifier based on <i>bayes rule</i>. 
			</p>
			<p>
				The idea is, we want to model \(p(y_i | \vec{x}_i)\): probability of it having that label, given those features. If \(y_i\) was binary, then we could classify \(\vec{x}_i\) as true if \(p(y_i | \vec{x}_i) > p(\lnot y_i | \vec{x}_i)\).
			</p>
			<p>
				Given bayes rule, and a y-label \(spam, \; \lnot spam\), we can use Bayes rule:
				\[
				p(spam | \vec{x}_i) = \frac{p(\vec{x}_i | spam)p(spam)}{p(\vec{x_i})}
				\]
				(And similar for non-spam.) \(p(\vec{x}_i)\) is the probability that email \(i\) has those words in it.
			</p>
			<div class="cornell">
				<div>
					<p>\(p(spam)\)</p>
				</div>
				<div>
					<p>
						<b>Very easy</b> to calculate, it's just the proportion of positive to all \(p(spam) = \frac{\sum spam}{n}\)
					</p>
				</div>
				<div>
					<p>\(p(\vec{x})_i\)</p>
				</div>
				<div>
					<p>
						Very difficult to calculate, however due to the classification rule we use:
						\begin{align}
							p(y_i | \vec{x}_i) &> p(\lnot y_i | \vec{x}_i) \\
							\implies \frac{p(\vec{x}_i | y_i)p(y_i)}{p(\vec{x_i})} &> \frac{p(\vec{x}_i | \lnot y_i)p(\lnot y_i)}{p(\vec{x_i})} \\
							\implies p(\vec{x}_i | y_i)p(y_i) &> p(\vec{x}_i | \lnot y_i)p( \lnot y_i)
						\end{align}
						We can just <b>ignore</b> the term.
					</p>
				</div>
				<div>
					<p>\(p(\vec{x_i} | spam)\)</p>
				</div>
				<div>
					<p>
						Equals \(\frac{\sum \textrm{ spam emails with } \vec{x}_i}{\sum spam}\), which is very hard to work out, and no easy workaround:
					</p>
					<p>
						Make a <b>conditionally independent assumption</b>: assume the occurence of every \(x_{ij}\) is independent from every other \(x_{ij'}\). This is why the Bayes is <i>naive</i>.
					</p>
					<p>
						Thus \(p(\vec{x_i} | spam) \approx \prod\limits_{j=1}^{d} p(x_{ij} | spam)\). 
					</p>
					<p>
						Whilst this is rarely true, it is in some cases a good enough approximation.
					</p>
				</div>
			</div>

			<p>
				More generally, when predicting data, we find the class \(c\) of \(y\) that has the highest probability given an \(\tilde{\vec{x}}_i\).
			</p>

			<p>
				There is one other problem: when data is missing from our set but we still want to account for it. For example, if you want to detect the chance an email containing "millionaire" is spam, but none of your collected spam emails have that word. 
			</p>
			<p>
				We can use <b>laplace smoothing</b> to account for this: if a feature \(x_{ij}\) has \(k\) possible values, for some \(\beta \in \bb{R}\):
				\[
				\frac{p(x_{ij} = e) + \beta}{\textrm{total }+ k\beta}
				\]
				i.e. add a fake value to the proportion of emails that have the feature we want, but add everything else in the same proportion to the total, so proportions are kept consistent. 
			</p>
			<p>
				For binary features (and \(\beta = 1\)), we can do
				\[
				
				\frac{
					\#\textrm{spam with 'millionaire' } + 1
				}{
					\#\textrm{total spam } + 2
				}
				\]
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="knn">K-Nearest Neighbours</h2>
		</div>

		<div class="cbox">
			<p>
				The models looked at previously were <i>parametric</i> -- that is, they had a fixed number of parameters (fixed by hyperparmameters, the model size is O(1) w.r.t. data size)
			</p>
			<p>
				<b>K-nearest neighbours</b> (KNN) is in the class of <i>non-parametric</i> models, that is, the larger the dataset, the larger the model gets. 
			</p>
			<p>
				KNN is a classification model, that works by taking a new point \(\vec{\tilde{x}_i}\) and comparing it to its \(k\) "closest" neighbours from the training set, classifying it into the majority category.
			</p>
			<p>
				It works off the <b>Lipschitzness Assumption</b>, that if two feature vectors are "close", they are likely to be similar. 
			</p>
			<p>
				This closeness is most commonly done using <b>L2 Norm: euclidean distance</b> \(\lVert \vec{x}_i - \vec{\tilde{x}}_i \rVert\).
			</p>
			<p class="grey">
				Note that euclidean distance is often written \(|\vec{a}|\) in mathematics, or with double bars and an explict L2-norm indicator \(||\vec{a}||_2\), seen in ML. 
			</p>
			<p>
				The way KNN calculates its \(k\) nearest neighbours is simply by calculating euclidean distance to all \(\vec{x} \in \vec{X}\) and finding the closest ones. Thus,
			</p>
			<ul>
				<li>There's no traditional learning (<b>lazy learning</b>), simply store the data</li>
				<li>Non parametric size, size \(\propto O(nd)\) for \(n\) samples and \(d\) dimensions</li>
				<li>Predicting is expensive, \(O(ndt)\) for \(t\) test samples</li>
			</ul>

			<p>
				A <b>Norm</b> is a way of "calculating distance", so to say. There are three (maybe four) common norms:
			</p>

			<table>
				<tr>
					<th>L2</th>
					<th>L1</th>
					<th>(L0)</th>
					<th>L\(\infty\)</th>
				</tr>
				<tr>
					<td>\[\lVert \vec{r} \rVert_2 = \sqrt{r_1^2 + r_2^2}\]</td>
					<td>\[\lVert \vec{r} \rVert_1 = |r_1| + |r_2|\]</td>
					<td>\[\lVert \vec{r} \rVert_0 = nz(r_1) + nz(r_2)\] \[\textrm{s.t. }  nz(x) = \begin{cases} 1 & x \neq 0 \\ 0 & x = 0 \end{cases}\]</td>
					<td>\[\lVert \vec{r} \rVert_\infty = \max(|r_1|, |r_2|)\]</td>
				</tr>
				<tr>
					<td>Euclidean distance</td>
					<td>Manhattan distance</td>
					<td>Counts nonzero values</td>
					<td>Max</td>
				</tr>
				<tr>
					<td>Emphasises larger values</td>
					<td>All values are equal</td>
					<td>(Not actually a norm)</td>
					<td>Only the largest component matters</td>
				</tr>
			</table>

			<p>
				The numbers are given with reason: the \(L_p\) norm for \(p\geq 1 : \lVert \vec{x} \rVert_p = \left( \sum\limits_{j=1}^d |x_j|^p \right)^{\frac{1}{p}}\)
			</p>

			<p>
				A <b>1NN</b> classifier is just nearest neighbour, every point defines a cell, within which everything belongs to its category (a "voronoi tesselation"). Boundaries between cells are <b>decision boundaries</b>, assignment here can be arbitrary.
			</p>

			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Euclidean_Voronoi_diagram.svg/1200px-Euclidean_Voronoi_diagram.svg.png" alt="Voronoi tesselation - wikipedia" style="max-width: 300px;">
				<figcaption>Voronoi Tesselation (Wikipedia)</figcaption>
			</figure>

			<h3>Test Errors, and Optimal Bayes</h3>

			<p>
				We can compare the error of a KNN (or 1NN) classifier against the <b>Optimal Bayes Classifier</b>.
			</p>
			<p>
				Recall <i>Naive Bayes</i>: we assume that all features are independent. <b>Optimal Bayes</b> is a theoretical Bayes classifier that does <i>not</i> make this assumption. \(\DeclareMathOperator*{\argmax}{argmax}
				\DeclareMathOperator*{\argmin}{argmin}
				\renewcommand{\epsilon}{\varepsilon}\)
			</p>
			<p>
				Optimal Bayes will then predict \(\hat{y_i} = \argmax\limits_{y_i} p(y_i | \vec{x}_i)\), taking into account all conditional probabilities.
			</p>
			<p>
				Note that Optimal Bayes is <i>not</i> a perfect classifier, it still has an error \(\epsilon_{bayes} = 1 - p(\hat{y}_i | \vec{x}_i)\), but this error is agood <b>lower bound</b> on potential error of classifier. 
			</p>

			<p class="side">
				<b><i>Claim.</i></b> For a 1-NN classifier, as \(n \longrightarrow \infty\) its error \(\epsilon_{1NN} \longrightarrow 2\epsilon_{bayes}\)
			</p>
			<button class="collapsible">Proof... </button>
			<div class="ccontent">
				<p>
					<b><i>Proof.</i></b> Assume a two class 1NN, with classes + and -.
				</p>
				<ul>
					<li>
						With more samples, the distance between \(\vec{\tilde{x}}_i\) and its closest neighbour tends to 0 (imagine the space filling up with points.)
					</li>
					<li>
						We want to find the probability that it is a false classification, \(p(\hat{y} \neq \tilde{y} | \vec{\tilde{x}})\)
					</li>
					<li>
						If positive \(p(+|\vec{x}_i)\), the classifier can predict rightly with probability \(p(+|\vec{x}_i) p(+|\vec{\tilde{x}}_i)\) or wrongly \(p(+|\vec{x}_i) (1-p(+|\vec{\tilde{x}}_i))\)
					</li>
					<li>
						Similarly if negative it can get it correct \((1-p(+|\vec{\tilde{x}}_i))(1-p(+|\vec{\tilde{x}}_i))\) or wrong \( (1-p(+|\vec{\tilde{x}}_i))p(+|\vec{x}_i)\)
					</li>
					<li>
						Note that there are two false probabilities,
						\begin{align}
							\epsilon_{1NN} &= p(\hat{y}_i|\vec{x}_i) (1-p(\hat{y}_i|\vec{\tilde{x}}_i)) + (1-p(\hat{y}_i|\vec{\tilde{x}}_i))p(\hat{y}_i|\vec{x}_i) \\
							&\geq (1-p(\hat{y}_i)|\vec{x}_i) + (1-p(\hat{y}_i)|\vec{x}_i) \pod{\textrm{ignore tilde}} \\
							&\geq 2(1-p(\hat{y}_i)|\vec{x}_i)\\
							& \geq 2\epsilon_{bayes}
						\end{align}

						$$\tag*{$\Box$}$$
					</li>
				</ul>
			</div>

			<p>
				The error of k-NN classification tends to \((i+\sqrt{\frac{2}{k}} ) \epsilon_{bayes}\). 
			</p>

			<h3>The Curse of Dimensionality (Runaway Exponents)</h3>

			<p>
				In practice though, we cannot get that good of an error. Because we need a very large \(n\), to get a crowded enough feature space.
			</p>
			<p>
				In 2D, we can imagine our feature space to be a circle, with area \(O(r^2)\), thus \(n = O(r^2)\). In 3D, this increases to \(O(r^3)\) and realistically ML has way more than 3 dimensions, and so the amount of data we need \(n \approx c^d\), for \(d\) dimensions and a constant \(c\).
			</p>
			<p>
				We also have an <b>upper bound</b> error: the <b>constant classifier</b>, that always returns the most common label: simply set \(k=n\). Naturally, our model is only a success if it does (significantly) better than n-NN. 
			</p>
		</div>

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="../../js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
</body>
</html>