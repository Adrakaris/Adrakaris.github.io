<!DOCTYPE html>
<html>
<head>
	<title>CS342</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

	<div class="hidden">
		<header>
			<div class="parallax parsmaller">
				<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 8fr 1fr; grid-column-gap: 10px; padding: 5px; ">
					<div class="column tinycolumn">
						<a href="../../" class="nav">Home</a>
					</div>
					<div class="column tinycolumn">
						<a href="../../blog.html" class="nav">Blog</a>
					</div>
					<div class="column tinycolumn">
						
						<a href="../../about.html" class="nav">About</a>
					</div>
					<div></div>
					<div class="column">
						<button class="nav dark-light">Dark Mode</button>
					</div>
				</div>
				<div class="cbox"> 		
					<h1>CS342</h1>
					<p class="subheading">Machine Learning</p>
				</div>
			</div>
		</header>

		<header>
			<div class="cbox">
				<h1>Contents</h1>
			</div>
		</header>

		<!-- <div class="buttonwrapper beside" >
			<a href="./index-zh.html">简体中文</a>
		</div> -->
		<!-- REMEMBER TO DO! -->

		<div class="cbox">
			<ol>
				<li><a href="#basics">The Basics</a></li>
				<li><a href="#desctree">Decision Trees</a></li>
				<li><a href="#bayes">Naive Bayes</a></li>
				<li><a href="#knn">K-nearest Neighbours</a></li>
				<li><a href="#linreg">Linear Regression</a></li>
				<li><a href="#robreg">Robust Regression</a></li>
				<li><a href="#clustering">Clustering</a></li>
				<li><a href="#pca">Principle Component Analysis (PCA)</a></li>
				<li><a href="#linclass">Linear Classification (+ Perceptron)</a></li>
				<li><a href="#kernel">The Kernel Trick</a></li>
				<li><a href="#mle">MLE and MAP estimation</a></li>
				<li><a href="#recom">Recommender Systems</a></li>
				<li><a href="#nns">Neural Networks</a></li>
			</ol>
			\( 
				\renewcommand{\vec}{\mathbf}
				\renewcommand{\epsilon}{\varepsilon}

				\DeclareMathOperator*{\argmax}{argmax}
				\DeclareMathOperator*{\argmin}{argmin}

				\def\xtrain{{\vec{x}_i}}  
				\def\xtest{{\vec{\tilde{x}}_i}}  
				\def\ytrain{{y_i}} 
				\def\ytest{{\tilde{y}_i}} 
				\def\ypred{{\hat{y}_i}} 
				\def\yhat{{\hat{y}}} 
				\def\wtrans{{\vec{w}^\top}} 
				\def\bb#1{{\mathbb{#1}}}
				
				\def\x{{\vec{x}}}
				\def\X{{\vec{X}}}
				\def\w{{\vec{w}}}
				\def\W{{\vec{W}}}
				\def\y{{\vec{y}}}
				\def\Y{{\vec{Y}}}
				\def\t{{^\top}}
				\def\z{{\vec{z}}}
				\def\Z{{\vec{Z}}}
				\def\v{{\vec{v}}}

				\def\lra{{\longrightarrow}}
				\def\lla{{\longleftarrow}}
				\newcommand{\fr}{\frac}
				\newcommand{\vecu}{\underline}
				\newcommand{\rm}{\textrm}
				\newcommand{\bb}{\mathbb}
				\newcommand{\nm}{\overline}
				\newcommand{\tl}{\tilde}

			\)
		</div>

		
		<div class="colourband">
			<h2 id="basics">The Basics</h2>
		</div>

		<div class="cbox">
			<h3>Introduction</h3>

			<p>
				<b>Machine Learning</b> is the process of getting machines to learn to model stuff, that is, given experiences <i>E</i> to learn from, a set of tests <i>T</i> to measure against, for which we measure a performance <i>P</i>. 
			</p>
			<p>
				Patterns are learned from <i>features</i>, qualities that describe the data, which change depending on the data in question. 
			</p>
			<p class="centre">
				Input -> Extracting features (manual) -> Training and classification -> Results
			</p>
			<p>
				Getting the machine to extract features itself is called <i>representation learning</i>, and is generally used in deep neural networks.
			</p>

			<h3>Recap</h3>

			<p>
				Machine learning takes basis on probability (and a lot of stats) - especially joint probability distributions (like \(p(A, B)\)).
			</p>

			<p>
				Recall:
			</p>
			<ul>
				<li>The <b>marginalisation rule</b> \(p(A) = \sum\limits_{\forall x} p(A, B=x)\)</li>
				<li><b>Random Variables</b> X, with an event \(X = x\) which occurs with some probability, the sum of all possible events is 1.</li>
				<li><b>Conditional probability</b> \(p(A|B) = \frac{p(A,B)}{p(B)} \)</li>
				<li><b>Bayes Rule</b> \(p(A|B) = \frac{p(B|A)p(A)}{p(B)}\)</li>
			</ul>

			<p>
				Datasets are collections of samples - the range of values are limited and probably in some sort of <i>distribution</i>, which can take many forms, such as uniform (straight line) or gaussian/normal (bell curve).
			</p>
			<p>
				Datasets can have a <b>mean</b> \(\mu\) and <b>standard deviation</b> \(\sigma^2\) -- variance is stddev-squared.
			</p>

			<div class="cornell">
				<div>
					<p>Distribution Functions</p>
				</div>
				<div>
					<p>
						<b>Probability density (PDF)</b>: probability of observing a value, i.e. \(p(X = x)\)

						\[ p(x|\mu, \sigma^2 ) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp(-\frac{(x-\mu)^2}{\sigma^2}) \]
					</p>
					<p>
						<b>Cumulative density (CDF)</b>: probility of observing &leq; the value \(p(X \leq x)\)

						\[p(X \leq x|\mu, \sigma^2) = \int_0^x \textrm{(curve)} dx\]
					</p>
				</div>
			</div>

			<p>
				<b>Expectation</b>, the expected value, is denoted \(\mathbb{E} \), and given as 
				\[\mathbb{E}[x] = \sum_{\forall x} p(X = x) \cdot x\]
			</p>

			<p>
				<i>Notation:</i> the module uses standard notations for everything: \
			</p>
			<div class="cornell">
				<div>
					<p>Scalar</p>
				</div>
				<div>
					<p>
						\(x\): lowercase (or upper case) letter.
					</p>
				</div>
				<div>
					<p>Column Vector</p>
				</div>
				<div>
					<p>
						\(\vec{x}\): bold lowercase letter. Vectors are <i>always</i> columns, so row vectors are <i>always</i> represented as \(\vec{x}^\top\). Dot products between vectors are often represented \(\vec{w}^\top \vec{x}\) (a matrix multiplication), though I may also use \(\vec{w} \cdot \vec{x}\)
					</p>
				</div>
				<div>
					<p>Matrix</p>
				</div>
				<div>
					<p>
						\[\vec{X} = \begin{bmatrix} x_{11} & x_{12} \\ x_{21} & x_{22} \end{bmatrix}\]
						Upper case bold letter. If a matrix is symmetric, \(\vec{X}^\top = \vec{X} \).
					</p>
				</div>
			</div>

			<p>
				<b>Supervised learning</b> is when both questions and answers are given, and the model has to learn to predict answers from samples. 
			</p>
			<p>
				<b>Unsupervised learning</b> is when no answers are given, and the model has to find relationships between samples.
			</p>

			<h3>General Data Notation</h3>

			<p>
				
				Samples are stored in a matrix, generally denoted \(\vec{X}\), where each row is a sample, a <b>feature vector</b>, and each column is an individual feature. \(\vec{X} \in \bb{R}^{n \times d}\), which means that X has \(n\) rows (for n samples) and \(d\) columns (for d features/dimensions)
				
				<ul>
					<li>
						\(\vec{x}_i\) is a sample row, represented as a column (as a row, it is consistently denoted \(\vec{x}_i^\top\)).
					</li>
					<li>
						\(\vec{x}_j \) is a column of features. 
					</li>
					<li>
						\(x_{ij}\) is feature \(j\) of column \(i\).
					</li>
				</ul>
			</p>
			<p>
				Labels are stored in vector \(\vec{y}\), \(y_i\) being the label for an \(\vec{x}_i\). This is also called the <b>ground truth</b>
			</p>
			<p>
				Given a new sample \(\tilde{\vec{x}}_i\) (tilde represents test data), we can make a prediction \(\hat{y}_i\) for it (the hat represents predicted). 
			</p>
		</div>

		<div class="colourband">
			<h2 id="desctree">Decision Trees</h2>
		</div>
		
		<div class="cbox">
			<h3>The Scenario</h3>

			<p>
				Suppose you start to frequently get upset stomachs, and worry that it is due to an allergy that you don't know about, perhaps to one food, perhaps to a combination. So, you decide to keep a food diary, detailing what allergen-inducing foods you consume each day, in what amounts, and whether or not you had an upset stomach that day. 
			</p>

			<table>
				<tr>
					<th>Milk</th>
					<th>Eggs</th>
					<th>Peanuts</th>
					<th>Crustaceans</th>
					<th>Upset?</th>
				</tr>
				<tr>
					<td>0.3</td>
					<td>0.2</td>
					<td>0</td>
					<td>0.7</td>
					<td>1</td>
				</tr>
				<tr>
					<td>...</td>
					<td>...</td>
					<td>...</td>
					<td>...</td>
					<td>...</td>
				</tr>
			</table>

			<p>
				This is a type of <i>supervised learning</i> problem, we have our features (foods and amounts) and our classes (upset stomach / not), and want to train a <i>classification</i> algorithm to predict things. 
			</p>

			<h3>The Decision Tree</h3>

			<p>
				A simple classifier is a <b>decision tree</b>
			</p>

			<p>
				A decision tree is essentially a nested series of if-elif-elses, that split the dataset up -- these are <i>splitting rules</i>. 
			</p>

			<figure>
				<img src="./assets/desctree-ex.png" alt="Decision tree with milk and eggs" style="max-width: 550px;">
			</figure>

			<p>
				Our aim is then to find these rules, which can be done recursively:
			</p>

			<h3>Stumps</h3>

			<p>
				The most basic decision tree is a <b>stump</b>. A stump is just one single splitting rule, the smallest possible tree. 
			</p>
			<p>
				To find a tree, we must first find a stump, but we need to find the stump that splits the best. How? With some scoring metric. 
			</p>
			<p>
				Ex. Could use plain accuracy, take a rule like <code>eggs > 0.2</code> and set a True and False coming off it - we now have a stump that can predict data. Pick the rule with the highest amount of data correctly predicted. 
			</p>
			<p>
				In our stump finding then, our training error would be just the percentage of \(\hat{y}_i \neq y_i\), i.e. the accuracy.
			</p>

			<h3>Greedy Recursive Splitting</h3>

			<p>
				A stump, however, can only split on one thing, and if we have \(d\) features we kinda want more splits, otherwise we're only ever using one of the features. 
			</p>
			<p>
				A common algorithm is <b>greedy recursive splitting</b> (GRS), which goes as follows:
			</p>
			<ol>
				<li>Find a stump with the best score metric, and split the dataset into two smaller ones at that stump. <span class="grey">(The new split datasets would probably still have a mix of \(y\)s, but hopefully are much more uniform than before)</span></li>
				<li>
					<i>Recurse:</i> "fit" (find) a new stump onto each of the smaller datasets, this is linked onto the previous stump.
				</li>
				<li>
					Repeat until accuracy threshold reached <span class="grey">(or depth reached, or some other limit. Accuracy <i>can</i> be 100%, but this may result in an overcomplex tree and not always be helpful, "overfitting").</span>
				</li>
			</ol>
			<p>
				Currently, we have accuracy as a possible metric, but this may not always be good, especially when there's multiple possible stumps that increase accuracy by the same amount, or the data is spread in such a way that a single line does not do much for accuracy. 
			</p>
			<p>
				A better way is if we could prioritise variance reduction, some sort of ... <i>information gain</i>:
			</p>

			<div class="cornell">
				<div>
					<p>Entropy</p>
				</div>
				<div>
					<p>
						<b>Entropy</b> of labels is essentially the randomness of their values. For a total of \(C\) categories, this can be calculated as
						\[E = -\sum_{c=1}^C p_c \cdot \log_2 (p_c)\]
						where \(p_c\) is the <i>proportion</i> of \(c\) out of all total labels in that (sub)set. Note that \(0\log_2 0\) is defined to just be 0.
					</p>
					<p>
						A lower entropy (min. 0) means less randomness, which is better. A higher entropy (max. \(log_2 C\), or 1 for binary classes) is worse. 
					</p>
				</div>
				<div>
					<p>Information Gain</p>
				</div>
				<div>
					<p>
						The principle of <b>Information Gain</b> (IG), is that we want the biggest difference in entropy pre-split to post-split. 
					</p>
					<p>
						For a single rule, we can split the set of \(n\) elements into \(n_{yes}\) (for the samples that match the rule) and \(n_{no}\) for the ones that don't. Thus IG is defined
						\[
						IG = E(\vec{y}) - \frac{n_{yes}}{n} E(\vec{y}_{yes}) - \frac{n_{no}}{n} E(\vec{y}_{no})
						\]
						i.e the entropy of combined labels \(\vec{y}\) minus the entropy of split labels individually.
					</p>
					<p>
						The higher this difference is, the better the split is.
					</p>
				</div>
			</div>

			<h3>Overfitting and Learning Theory</h3>

			<p>
				If we have 100% accuracy on our training data, this does not imply that we have 100% accuracy on any new test data. 
			</p>

			<ul>
				<li>
					<b>Test Accuracy</b> is the accuracy on new data
				</li>
				<li>
					<b>Overfitting</b> or <b>Optimisation Bias</b> is high accuracy on training data but low on test data 
				</li>
				<li>
					<b>Underfitting</b> is low accuracy on training data.
				</li>
			</ul>
			<p>
				We naturally want the right amount of fitting, but this is difficult. 
			</p>
			<p>
				We can use a test/verification set \(\tilde{\vec{X}}\) to verify, but we must ensure that we <i>do not train using the verification set</i>.
			</p>
			<p>
				The <b>IID Assumption</b> (Independent and Identically Distributed) states that we assume that the training data is a good representation of all data - that is, it has the same distribution as test data. 
			</p>
			<p>
				Whilst this is often not true, it is generally a good assumptuion. 
			</p>
			<p>
				<b>Learning Theory</b> gives us a metric for measurement, \(E_{approx}\) the <b>approximation error</b>, the error between training and testing:
				\[E_{test} = E_{approx} + E_{train}\]
				where \(E_{approx} = E_{test} - E_{train}\). If this is small, then we can say that the training data is good for test data. It tends to go down as the training set gets larger, and up as the model gets more complex (and thus overfit). 
			</p>

			<p>
				However, this approximation error leads to a <b>fundamental tradeoff</b>: minimise \(E_{approx}\) whilst minimising \(E_{train}\): make the model not overfit whilst having it be fit as well as possible. 
			</p>

			<h3>Validation</h3>

			<p>
				Usually, we split a training set into two: a larger <code>x_train</code> (the "training set"), and a smaller <code>x_test</code>, what's called the <b>validation</b> (or colloquially test) set. 
			</p>

			<p>
				Once we train on <code>x_train</code>, we can compute the accuracy of predicting <code>x_test</code>, and use this to adjust our model, such as manually setting <i>hyperparameters</i>.
			</p>

			<p>
				<b>Hyperparameters</b> are settings in a model which we <i>do not learn</i>. For decision trees, this may be the maximum depth of the tree, or the accuracy threshold. For more complex models like Neural Networks, the entire architecture is hyperparameter, whilst the weights and biases are learned, regular parameters.
			</p>

			<p>
				With simpler models, it is often good to train it multiple times with different hyperparameter settings (like tree depth), and see which one minimises both training error and approximation error the best.
			</p>
			<p>
				Sometimes, once hyperparameters are set, a model is trained again, with <code>x_test</code> merged back into the training set (no validation).
			</p>

			<p>
				That said, we may end up overfitting the validation set. If we search too many and too specific hyperparameters, we may end up finding one that's really good for the validation set, but not so good for any new data. 
			</p>

			<ul>
				<li>
					Optimisation bias increases as we search for more model configurations 
				</li>
				<li>
					It decreases as the training data gets larger.
				</li>
			</ul>

			<p>
				This leads to <b>another fundamental tradeoff</b>: increasing validation size means our model is less likely to be overfit. However, increasing validation size decreases test size, which makes our model train less well.
			</p>

			<h4>Cross Validation</h4>

			<p>
				Validation takes from test data, if test data is precious and hard to get, we may not want to waste it.
			</p>
			<div class="blue">
				<span class="nopad">
					<p>
						<b>K-fold cross validation</b> gives a good estimate without wasting too much data.
					</p>
	
					<p>
						For K folds, we train K times, each time taking \(\frac{1}{k}
						\) of \(\vec{X}\) as validation, and the rest \(\frac{k-1}{k}\) as train. The average validation accuracy is taken as the overall accuracy
					</p>
					
				</span>
				
			</div>

			<p>
				<i>Leave One Out (LOO)</i> is a special type of K-fold validation where \(k = n\).
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="bayes">Naive Bayes</h2>
		</div>

		<div class="cbox">
			<h3>The Scenario</h3>

			<p>
				You want to filter out which emails incoming are spam or not. For this, you've collected many, many emails, and labelled them correctly.
			</p>
			<p>
				Your features are going to be keywords in emails, thus, \(x_{ij} = 1\) if the word \(j\) is in email \(i\).
			</p>

			<h3>Probabilistic Classification</h3>

			<p>
				Traditional filtering/classification methods used something called <b>Naive Bayes</b> - a probability based classifier based on <i>bayes rule</i>. 
			</p>
			<p>
				The idea is, we want to model \(p(y_i | \vec{x}_i)\): probability of it having that label, given those features. If \(y_i\) was binary, then we could classify \(\vec{x}_i\) as true if \(p(y_i | \vec{x}_i) > p(\lnot y_i | \vec{x}_i)\).
			</p>
			<p>
				Given bayes rule, and a y-label \(spam, \; \lnot spam\), we can use Bayes rule:
				\[
				p(spam | \vec{x}_i) = \frac{p(\vec{x}_i | spam)p(spam)}{p(\vec{x_i})}
				\]
				(And similar for non-spam.) \(p(\vec{x}_i)\) is the probability that email \(i\) has those words in it.
			</p>
			<div class="cornell">
				<div>
					<p>\(p(spam)\)</p>
				</div>
				<div>
					<p>
						<b>Very easy</b> to calculate, it's just the proportion of positive to all \(p(spam) = \frac{\sum spam}{n}\)
					</p>
				</div>
				<div>
					<p>\(p(\vec{x})_i\)</p>
				</div>
				<div>
					<p>
						Very difficult to calculate, however due to the classification rule we use:
						\begin{align}
							p(y_i | \vec{x}_i) &> p(\lnot y_i | \vec{x}_i) \\
							\implies \frac{p(\vec{x}_i | y_i)p(y_i)}{p(\vec{x_i})} &> \frac{p(\vec{x}_i | \lnot y_i)p(\lnot y_i)}{p(\vec{x_i})} \\
							\implies p(\vec{x}_i | y_i)p(y_i) &> p(\vec{x}_i | \lnot y_i)p( \lnot y_i)
						\end{align}
						We can just <b>ignore</b> the term.
					</p>
				</div>
				<div>
					<p>\(p(\vec{x_i} | spam)\)</p>
				</div>
				<div>
					<p>
						Equals \(\frac{\sum \textrm{ spam emails with } \vec{x}_i}{\sum spam}\), which is very hard to work out, and no easy workaround:
					</p>
					<p>
						Make a <b>conditionally independent assumption</b>: assume the occurence of every \(x_{ij}\) is independent from every other \(x_{ij'}\). This is why the Bayes is <i>naive</i>.
					</p>
					<p>
						Thus \(p(\vec{x_i} | spam) \approx \prod\limits_{j=1}^{d} p(x_{ij} | spam)\). 
					</p>
					<p>
						Whilst this is rarely true, it is in some cases a good enough approximation.
					</p>
				</div>
			</div>

			<p>
				More generally, when predicting data, we find the class \(c\) of \(y\) that has the highest probability given an \(\tilde{\vec{x}}_i\).
			</p>

			<p>
				There is one other problem: when data is missing from our set but we still want to account for it. For example, if you want to detect the chance an email containing "millionaire" is spam, but none of your collected spam emails have that word. 
			</p>
			<p>
				We can use <b>laplace smoothing</b> to account for this: if a feature \(x_{ij}\) has \(k\) possible values, for some \(\beta \in \bb{R}\):
				\[
				\frac{p(x_{ij} = e) + \beta}{\textrm{total }+ k\beta}
				\]
				i.e. add a fake value to the proportion of emails that have the feature we want, but add everything else in the same proportion to the total, so proportions are kept consistent. 
			</p>
			<p>
				For binary features (and \(\beta = 1\)), we can do
				\[
				
				\frac{
					\#\textrm{spam with 'millionaire' } + 1
				}{
					\#\textrm{total spam } + 2
				}
				\]
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="knn">K-Nearest Neighbours</h2>
		</div>

		<div class="cbox">
			<p>
				The models looked at previously were <i>parametric</i> -- that is, they had a fixed number of parameters (fixed by hyperparmameters, the model size is O(1) w.r.t. data size)
			</p>
			<p>
				<b>K-nearest neighbours</b> (KNN) is in the class of <i>non-parametric</i> models, that is, the larger the dataset, the larger the model gets. 
			</p>
			<p>
				KNN is a classification model, that works by taking a new point \(\vec{\tilde{x}_i}\) and comparing it to its \(k\) "closest" neighbours from the training set, classifying it into the majority category.
			</p>
			<p>
				It works off the <b>Lipschitzness Assumption</b>, that if two feature vectors are "close", they are likely to be similar. 
			</p>
			<p>
				This closeness is most commonly done using <b>L2 Norm: euclidean distance</b> \(\lVert \vec{x}_i - \vec{\tilde{x}}_i \rVert\).
			</p>
			<p class="grey">
				Note that euclidean distance is often written \(|\vec{a}|\) in mathematics, or with double bars and an explict L2-norm indicator \(||\vec{a}||_2\), seen in ML. 
			</p>
			<p>
				The way KNN calculates its \(k\) nearest neighbours is simply by calculating euclidean distance to all \(\vec{x} \in \vec{X}\) and finding the closest ones. Thus,
			</p>
			<ul>
				<li>There's no traditional learning (<b>lazy learning</b>), simply store the data</li>
				<li>Non parametric size, size \(\propto O(nd)\) for \(n\) samples and \(d\) dimensions</li>
				<li>Predicting is expensive, \(O(ndt)\) for \(t\) test samples</li>
			</ul>

			<p>
				A <b>Norm</b> is a way of "calculating distance", so to say. There are three (maybe four) common norms:
			</p>

			<table>
				<tr>
					<th>L2</th>
					<th>L1</th>
					<th>(L0)</th>
					<th>L\(\infty\)</th>
				</tr>
				<tr>
					<td>\[\lVert \vec{r} \rVert_2 = \sqrt{r_1^2 + r_2^2}\]</td>
					<td>\[\lVert \vec{r} \rVert_1 = |r_1| + |r_2|\]</td>
					<td>\[\lVert \vec{r} \rVert_0 = nz(r_1) + nz(r_2)\] \[\textrm{s.t. }  nz(x) = \begin{cases} 1 & x \neq 0 \\ 0 & x = 0 \end{cases}\]</td>
					<td>\[\lVert \vec{r} \rVert_\infty = \max(|r_1|, |r_2|)\]</td>
				</tr>
				<tr>
					<td>Euclidean distance</td>
					<td>Manhattan distance</td>
					<td>Counts nonzero values</td>
					<td>Max</td>
				</tr>
				<tr>
					<td>Emphasises larger values</td>
					<td>All values are equal</td>
					<td>(Not actually a norm)</td>
					<td>Only the largest component matters</td>
				</tr>
			</table>

			<p>
				The numbers are given with reason: the \(L_p\) norm for \(p\geq 1 : \lVert \vec{x} \rVert_p = \left( \sum\limits_{j=1}^d |x_j|^p \right)^{\frac{1}{p}}\)
			</p>

			<p>
				A <b>1NN</b> classifier is just nearest neighbour, every point defines a cell, within which everything belongs to its category (a "voronoi tesselation"). Boundaries between cells are <b>decision boundaries</b>, assignment here can be arbitrary.
			</p>

			<figure>
				<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/54/Euclidean_Voronoi_diagram.svg/1200px-Euclidean_Voronoi_diagram.svg.png" alt="Voronoi tesselation - wikipedia" style="max-width: 300px;">
				<figcaption>Voronoi Tesselation (Wikipedia)</figcaption>
			</figure>

			<h3>Test Errors, and Optimal Bayes</h3>

			<p>
				We can compare the error of a KNN (or 1NN) classifier against the <b>Optimal Bayes Classifier</b>.
			</p>
			<p>
				Recall <i>Naive Bayes</i>: we assume that all features are independent. <b>Optimal Bayes</b> is a theoretical Bayes classifier that does <i>not</i> make this assumption. 
			</p>
			<p>
				Optimal Bayes will then predict \(\hat{y_i} = \argmax\limits_{y_i} p(y_i | \vec{x}_i)\), taking into account all conditional probabilities.
			</p>
			<p>
				Note that Optimal Bayes is <i>not</i> a perfect classifier, it still has an error \(\epsilon_{bayes} = 1 - p(\hat{y}_i | \vec{x}_i)\), but this error is agood <b>lower bound</b> on potential error of classifier. 
			</p>

			<p class="side">
				<b><i>Claim.</i></b> For a 1-NN classifier, as \(n \longrightarrow \infty\) its error \(\epsilon_{1NN} \longrightarrow 2\epsilon_{bayes}\)
			</p>
			<button class="collapsible">Proof... </button>
			<div class="ccontent">
				<p>
					<b><i>Proof.</i></b> Assume a two class 1NN, with classes + and -.
				</p>
				<ul>
					<li>
						With more samples, the distance between \(\vec{\tilde{x}}_i\) and its closest neighbour tends to 0 (imagine the space filling up with points.)
					</li>
					<li>
						We want to find the probability that it is a false classification, \(p(\hat{y} \neq \tilde{y} | \vec{\tilde{x}})\)
					</li>
					<li>
						If positive \(p(+|\vec{x}_i)\), the classifier can predict rightly with probability \(p(+|\vec{x}_i) p(+|\vec{\tilde{x}}_i)\) or wrongly \(p(+|\vec{x}_i) (1-p(+|\vec{\tilde{x}}_i))\)
					</li>
					<li>
						Similarly if negative it can get it correct \((1-p(+|\vec{\tilde{x}}_i))(1-p(+|\vec{\tilde{x}}_i))\) or wrong \( (1-p(+|\vec{\tilde{x}}_i))p(+|\vec{x}_i)\)
					</li>
					<li>
						Note that there are two false probabilities,
						\begin{align}
							\epsilon_{1NN} &= p(\hat{y}_i|\vec{x}_i) (1-p(\hat{y}_i|\vec{\tilde{x}}_i)) + (1-p(\hat{y}_i|\vec{\tilde{x}}_i))p(\hat{y}_i|\vec{x}_i) \\
							&\geq (1-p(\hat{y}_i)|\vec{x}_i) + (1-p(\hat{y}_i)|\vec{x}_i) \pod{\textrm{ignore tilde}} \\
							&\geq 2(1-p(\hat{y}_i)|\vec{x}_i)\\
							& \geq 2\epsilon_{bayes}
						\end{align}

						$$\tag*{$\Box$}$$
					</li>
				</ul>
			</div>

			<p>
				The error of k-NN classification tends to \((i+\sqrt{\frac{2}{k}} ) \epsilon_{bayes}\). 
			</p>

			<h3>The Curse of Dimensionality (Runaway Exponents)</h3>

			<p>
				In practice though, we cannot get that good of an error. Because we need a very large \(n\), to get a crowded enough feature space.
			</p>
			<p>
				In 2D, we can imagine our feature space to be a circle, with area \(O(r^2)\), thus \(n = O(r^2)\). In 3D, this increases to \(O(r^3)\) and realistically ML has way more than 3 dimensions, and so the amount of data we need \(n \approx c^d\), for \(d\) dimensions and a constant \(c\).
			</p>
			<p>
				We also have an <b>upper bound</b> error: the <b>constant classifier</b>, that always returns the most common label: simply set \(k=n\). Naturally, our model is only a success if it does (significantly) better than n-NN. 
			</p>
		</div>

		<div class="colourband">
			<h2 id="linreg">Linear Regression</h2>
			<p class="subtitle">
				Get ready for some Linear Algebra
			</p>
		</div>

		<div class="cbox">
			<p style="display: none;">
				\(\newcommand{\cal}{\mathcal}\)
			</p>
			<p>
				<b>Classification</b> predicts best classes, when labels are <i>discrete</i>. <b>Regression</b> predicts best fit, when labels are <i>continuous</i>.
			</p>

			<p>
				<b>Linear Regression</b> is a tool for modelling linear relationships - literally a line of best fit. 
			</p>
			<p>
				For training data \(\vec{X} \), labels \(\vec{y} \), we can define a regression function \(h:\vec{X} \longrightarrow \vec{y}\) as
				\[\hat{y}_i = h(\vec{x}_i) = \vec{w} \cdot \vec{x_i}\]
				Note that this is literally \(y = mx + c\) but without the \(+c\). \(\vec{w} \) (the weights) is the slope of the line, and this is what we want to find. 
			</p>
			<p>
				We find this by <b>minimising a loss function</b>, a function that describes the error of our predictions w.r.t. the training data. 
			</p>
			<p class="blue">
				The most common loss function used is <b>mean squared error</b> (MSE):
				\[
				\mathcal{L}(h) = \frac{1}{n} \sum_{i=1}^n (h(x_i | w) - y_i) ^ 2
				\]
				Basically, sum of the difference between predicted and actual \(y\), squared. Thus we find \(\argmin\limits_\vec{w} \cal{L}(h)\). This is known as finding <b>least squares</b>.
			</p>
			<p class="grey">
				Note that the loss function can be denoted \(\cal{L}(\cdot)\) or just \(f(\cdot)\), I often prefer the former when note taking but you see the latter used quite often. 
			</p>
			<p>
				 The loss function is quadratic in \(\vec{w} \) (in this case), and so we can find a minimum easily. 
			</p>
			<p>
				<b><i>Note.</i></b> Minimising \(\frac{1}{n} \sum(\cdots)\) is the same as minimising \(\sum(\cdots)\) or any other constant multiplier. \(1/2\) is often used for convenience of differentiation.
			</p>

			<h3>1D Regression</h3>

			<p>
				1D regression is the easiest case. We have a function \(\cal{L}(w) = \frac{1 }{2 } \sum\limits_{i=1}^n (wx_i - y_i)^2\) for a scalar \(w\).
				\begin{align}
					\therefore \cal{L}(w) &= \frac{1 }{2 }\sum_{i=1}^n (w^2x_i^2 - 2wx_iy_i + y_i^2) \\
					&= \frac{w^2 }{2} (\sum x_i^2) - \frac{2w }{2 } (\sum x_i y_i) + \frac{1}{2} (\sum y_i^2) \\
					&= \frac{a}{2} w^2 - bw + c 
 				\end{align}
				(Since the sum terms are constant)
			</p>
			<p>
				\(\cal{L}' (w) = wa-b\), set to 0 and solve for \(w\), \(w = \frac{b}{a}\)
			</p>
			<p>
				We can also use the second derivative to make sure it is a minimum. 
			</p>
			<p>
				This finds the best \(w\) ... for a line \(\hat{y}_i = wx_i\) that passes through 0, because we haven't accounted for <i>bias</i>.
			</p>

			<h3>Bias, Extending Dimensions</h3>

			<p>
				A <b>bias</b> is an extra constant added on, that shifts the result up or down. For 1D, we change the equation to \(\hat{y}_i = wx_i + w_0\) where \(w_0\) is the bias. 
				<!-- \(\newcommand{\xtrain}{\vec{x}_i} \newcommand{\xtest}{\vec{\tilde{x}}_i \newcommand{\yhi}{\hat{y}_i} \) -->
				
			</p>
			<p>
				We can add bias by <i>changing basis</i>: we make a "fake feature", with 1 for all the values:
				\[
					\vec{X} = \begin{bmatrix}
						x_i \\ x_2 \\ \vdots \\ x_n
					\end{bmatrix} \longrightarrow
					\vec{Z} = \begin{bmatrix}
						1 & x_1 \\ 1 & x_2 \\ \vdots &\vdots \\ 1 & x_n
					\end{bmatrix}
				\]
				Thus, we are effectively using \(\vec{Z}\) as our feature set, and have to solve
				\[
				y_i = v_1 z_{i1} + v_2 z_{i2} \equiv w_0 + w_1x_i\]
				Note that because we changed basis, our transformed features are now denoted \(z\) and our transformed wieghts are denoted \(v\).
			</p>
			<p>
				This creates 2D linear regression. 
			</p>
			<p>
				When we learn in 1D, we are fitting a line of best fit. In 2D, then, we are fitting a <i>plane</i> of best fit. And the dimensions of the (hyper)plain just increases. 
			</p>
			<p>
				For \(d\) features (ignoring bias), we have a
				\begin{align}
					\ypred &= w_1 x_{i1} + \cdots + w_d x_{id} \\
					&= \sum_{j=1}^d w_j x_{ij} \\
					&= \vec{w}^\top \xtrain \equiv \vec{w} \cdot \xtrain.
				\end{align}
				The loss is then the MSE:
				\[
				\cal{L} = \frac{1}{2} \sum_{i=1}^n (\wtrans \xtrain - y_i)^2
				\]
			</p>
			<p>
				Whilst finding derivatives for this can be difficult from a partial differentiation perspective, we can actually differentiate matrices fine, which we'll soon see.
			</p>

			<h4>Matrix Magic</h4>

			<p>
				It is probably good now to mention the dimensions of the data.
			</p>
			<ul>
				<li>
					\(\vec{y}: n \times 1 \)
				</li>
				<li>
					\(\xtrain : d \times 1\)
				</li>
				<li>
					\(\vec{X} : n \times d\); can be thought of as rows of \(\xtrain^\top\) (transposed so coln -> row)
				</li>
			</ul>
			<p>
				A single prediction \(\ypred = \vec{w} \cdot \xtrain\). All \(n\) samples can then be gotten from \(\vec{\hat{y}} = \vec{X} \vec{w}\).
			</p>
			<p>
				We say that \(\vec{\hat{y}} - \vec{y} = \vec{r}\), the <b>residual vector</b>, and being the error difference, we can write MSE using it:
				\[
				\cal{L}(\vec{w}) = \frac{1}{2} \lVert \vec{r}^2 \rVert^2 = \frac{1 }{2 }\lVert \vec{X} \vec{w} - \vec{y} \rVert^2
				\]
			</p>
			<p>
				Thus we can find an error using <i>matrix calculus</i>; a symbol will be used here called "nabla" \(\nabla\), which represents multi-dimensional differentiation. For the case of vectors, think of deriving each term element-wise. 
			</p>

			\begin{align}
				\implies \cal{L}(\vec{w}) &= \frac{1 }{2 } \vec{w}^\top \vec{X}^\top \vec{X} \vec{w} - \vec{w}^\top \vec{X}^\top \vec{y} + \frac{1 }{2 }\vec{y}^\top \vec{y} \\

				&= \frac{1 }{2 }\vec{A w^\top w} - \vec{b w^\top} + \frac{1 }{2 }c \pod{\vec{X^\top X, X^\top y, y^\top y} \textrm{ are constant}}  \\

				\nabla \cal{L} (\vec{w}) &= \nabla(\frac{1 }{2 }\vec{A w^\top w}) - \nabla (\vec{bw^\top}) + \nabla(\frac{1 }{2 }c) \\ 

				&= \vec{Aw-b}. \pod{\textrm{if } \vec{A} \textrm{ symmetric}}
			\end{align}

			<p>
				Then, all we need to find is \(\vec{Aw - b} = 0\), or equivalently \(\vec{X^\top X w - x^\top y} = 0\)
			</p>
			\[
			\implies \vec{w = (X^\top X)}^{-1} \vec{X^\top y}.
			\]

			<p>
				MSE has some issues however.
			</p>
			<ul>
				<li>
					MSE is sensitive to outliers, as large differences are empasised more greatly.
				</li>
				<li>
					Finding a model can be memory and time intensive - there are a lot of large matrix multiplications, and large systems of equations. 
				</li>
				<li>
					May <i>extrapolate</i>: predicting values outside of the measured range.
				</li>
				<li>
					Solution may not be unique: <i>colinear features</i>, where 2 features are identical for all samples, which means that both, say, \(y_i = w_1 x_1 + w_2 x_2 \equiv y_i = 0 x_1 + (w_1 + w_2)x_2\), are valid solutions.
				</li>
				<li>
					And finally, linear regression assumes a <b>linear</b> relationship.
				</li>
			</ul>
			
			<p class="blue">
				<b><i>Note.</i></b> The term <b>convex</b> refers to functions with a shape like \(\cup, \cap\), meaning they have only one minima (or maxima). It is easy to optimise these functions (not so for non-convex ones)
			</p>

			<h3>Polynomial Regression</h3>

			<p>
				Often, there is <i>not</i> a linear relationship. 
			</p>

			<p>
				So what do we do? Well, we can very easily "hijack" least squares linear regression to do polynomial regression, in the same method that we use to add a bias. 
			</p>

			<p>
				Suppose again 1D samples \(\vec{X}\), that we suspect has a polynomial relationship to \(\vec{y}\), we can also change the basis to account for squares:
			</p>

			\[
				\vec{X} = \begin{bmatrix}
					x_i \\ x_2 \\ \vdots \\ x_n
				\end{bmatrix} \longrightarrow
				\vec{Z} = \begin{bmatrix}
					1 & x_1 & x_1^2 \\ 1 & x_2 & x_2^2 \\ \vdots &&\vdots \\ 1 & x_n & x_n^2
				\end{bmatrix}
			\]

			<p>
				And would you look at that, we now have the squared features as a separate feature. Now simply fit a line \(\ypred = v_1 z_{i1} + v_2 z_{i2} + v_3 z_{i3}\).
			</p>
			<p>
				We can also naturally extend this to higher polynomials. <i>However</i>, too high polynomials leads to overfitting - whilst training error may go down, test error goes up. 
			</p>
			<p>
				That is, fit new parameters \(\vec{V}\) on transformed featuers \(\vec{Z}\), by solving for \(\vec{ZZ^\top v = Z^\top y} \). 
			</p>
			<p>
				To predict on a validation set \(\vec{\tilde{X}}\), first transform it into the polynomial form \(\vec{\tilde{Z}}\), then do \(\vec{y = \tilde{Z} v }\).
			</p>

			<h4>L2 Regularisation</h4>

			<p>
				Training a regression model may produce excessively large weights.Exessively large weights are very sensitive, thus are more likely to be overfit -> penalise large weights. 
			</p>

			<p class="blue">
				<b>L2 Regularisation</b> is an extra bit we add to our MSE loss:
				\[
				\cal{L}(\vec{w}) = \frac{1}{2} \lVert \vec{Xw - y}\rVert^2 + \frac{\lambda}{2}\lVert \vec{w} \rVert^2 
				\]
				Note the new \(\frac{\lambda}{2}\lVert \vec{w} \rVert^2 \) term. This new lambda parameter controls how much emphasis we want on small weights. A high \(\lambda\) means that we want small weights more than least squares, whereas \(\lambda = 0\) removes the penalty altogether.
			</p>
			<p class="grey">
				\(\frac{\lambda}{2 }\) is done for ease of differentiation.
			</p>
			<p>
				Furthermore, the derivative \(\nabla \cal{L}(\vec{w}) = \vec{X^\top X w - X^\top y + } \lambda \vec{w}\) solves for the equation \((\vec{X^\top X + \lambda I}) \vec{ w = X^\top y}\), and the matrix that multiplies into \(\vec{w}\) is <i>always</i> invertible.
			</p>
		</div>

		<div class="colourband">
			<h2 id="robreg">Robust Regression</h2>
		</div>

		<div class="cbox">
			<h3>Outliers and Loss</h3>

			<p>
				As mentioned, MSE is greatly affected by outliers, as large deviations are emphasised more. Instead, we can use absolute error (instead of squared error) to de-emphasise large deviations. 
			</p>
			<p>
				Absolute (L1) loss is defined as 
				\[
				\cal{L}(\w) = \sum_{i=1}^n |\w\t \x_i - y_i|
				\]
			</p>
			<p>
				Minimising L1 though is much harder, as the absolute function is <i>non-differentiable</i> at \(x=0\), and the gradient is not smooth.
			</p>
			<p>
				This poses problems for our minimisation equations, problems that L2 doesn't. Thus, we want a loss function that gets the best of both worlds.
			</p>
			<p class="blue">
				The <b>Huber loss function</b> is defined
				\[
				\cal{L}(\w) = \sum_{i=1}^n h(\w\t \x_i - y_i) 
				\]
				Where
				\[
				h(r_i) = \begin{cases} \frac{1 }{2 }r_i^2 & |r_i| \leq \epsilon \\ \epsilon(|r_i| - \frac{1 }{2 }\epsilon) & \textrm{otherwise. } \end{cases}
				\]
				For a small number \(\epsilon\). 
			</p>
			<p>
				Huber function is differentiable, becoming
				\[
				h'(r_i) = \begin{cases} r_i & |r_i| \leq \epsilon \\ \epsilon \cdot \textrm{sign}(r_i) & \textrm{otherwise} \end{cases}
				\]
				Where \(\textrm{sign}(x) = 1 \textrm{ if } x > 0 \textrm{ else } -1\).
			</p>

			<h3>Standardising Features</h3>

			<p>
				Standardising features involves rescaling features so that they're all proportional to each other. 
			</p>
			<p>
				In some cases, we don't need to -- for basic models like Bayes and Decision Trees, we don't need to change. For least squares, though weights may be different, fundamentally the model is still the same. 
			</p>
			<p>
				However, standardisation is important for KNN (as large numbers can skew distance unnecessarily) and for regularised least squares (as penalty must be proportional).
			</p>
			<p>
				Data is standardised <b>feature-wise</b>; for each feature \(j\):
			</p>
			<ul>
				<li>
					Compute mean \(\mu_j = \frac{1}{n} \sum_{i=1}^n x_{ij}\)
				</li>
				<li>
					Compute standard deviation \(\sigma_j = \sqrt{ \frac{1 }{n} \sum_{i=1}^n (x_{ij} - \mu_j)^2 }\)
				</li>
				<li>
					Replace every \(x_{ij}\) by \(\frac{x_{ij} - \mu_j}{\sigma_j}\)
				</li>
			</ul>
			<p class="side">
				<b>HOWEVER</b>, when standardising test data, <b>do not use test data mean/stdev</b>s, because technically, we don't know test data, and our assumption is that the training set is a good representation of any new data.
			</p>
			<p>
				Labels/targets/ground truths can also be standardised, in the same way. 
			</p>

			<h3>Feature Selection and Regularisation</h3>

			<p>
				Sometimes, we have a lot of features, but only a few of them matter. 
			</p>
			<p>
				We'd like the model to learn which features of \(\X\) are important. 
			</p>
			<p>
				One way we can do this is to look at the trained weights \(\w\), and take all features \(j : |w_j| >\) a predifined threshold. 
			</p>
			<p>
				Unfortunately, this has some downsides:
			</p>
			<ul>
				<li>
					It struggles with coliniearily and may give wrong results then, as if \(x_1, x_2\) are coliniear, then \(w_1 x_1 + w_2 x_2 \sim (w_1 + w_2) x_1 + 0x_2\).
				</li>
				<li>
					May pick up irrelevant features which happen to cancel: given irrelevant \(x_a, x_b\), then \(0x_a + 0x_b \sim -1000x_a + 1000x_b\).
				</li>
			</ul>

			<p>
				Alternatively we can use a tactic of <b>search and score</b>, by defining (or using a function) \(f\) that computes a "score" of how good this set of features is -- such as the loss function.
			</p>
			<p>
				Of course, this risks overfitting, especially since we're iterating over all \(2^d\) subsets, the optimisation bias can be high. 
			</p>

			<p>
				Instead, we modify the loss function to add a <b>complexity penalty</b>, such as
				\[
				\cal{L}(s) = \frac{1 }{2 }\sum_{i=1}^n (\w_s \cdot \x_{is} - y_i)^2 + \textrm{size}(s)
				\]
				for a <i>subset</i> of features \(s\). The size function counts the number of nonzero weights -- the <b>L<sub>0</sub> norm</b>;
			</p>

			<p>
				Our Loss score becomes 
				\[
				\cal{L}(\w) = \frac{1}{2} \lVert \X\w - \y \rVert^2 + \lambda \lVert \w \rVert_0
				\]
				With a parameter lambda.
			</p>

			<p>
				The problem is, L0 is not convex ... nor differentiable, whereas L2 was very nice indeed, so if we can find a best of both worlds:
			</p>

			<p class="blue">
				Use the convex and (mostly) differentiable <b>L<sub>1</sub> loss</b>:

				\[
				\cal{L}(\w) = \frac{1}{2} \lVert \X\w - \y \rVert^2 + \lambda \lVert \w \rVert_1
				\]

				Also called <b>LASSO</b> regularisation.
			</p>

			<p>
				A <b>sparse</b> matrix has many zeros: this is a good sign of feature selection.
			</p>

			<p>
				Both L0 and L1 (LASSO) prefer sparsity, whilst L2 (Ridge) doesn't.
			</p>
			
		</div>

		<div class="colourband">
			<h2 id="clustering">Clustering</h2>
		</div>

		<div class="cbox">

			<h3>Introduction</h3>

			<p>
				<b>Clustering</b> is a type of <b>unsupervised learning</b>.
			</p>
			<p>
				In unsupervised learning, we are given no labels, and have to infer the underlying "latent" structure. 
			</p>
			<p>
				This is especially useful for outlier detection, grouping, <i>latent factor models</i> (later), and visualisation, among others.
			</p>

			<p>
				For clustering, we are trying to group data into clusters based on their "similarity", and discover the distribution of data. 
			</p>
			<p>
				Often this data is <i>multimodal</i>, where there are more than one "peak" in its distribution (vs, say, a normal distribution, which has one mode).
			</p>

			<p>
				Some assumptions:
			</p>
			<ul>
				<li>The features \(\X = \{\x_1 \cdots \x_n\}\) are in <b>euclidean space</b>.</li>
				<li>\(\X\) can be grouped into \(K\) groups or clusters (this <i>is</i> a hyperparameter);</li>
			</ul>

			<h3>K-Means Clustering</h3>

			<p>
				Assuming that there are K clusters, and that each sample is close to a cluster centre. <span class="grey">In the off chance you're like me and misread K-means, "means" refers to "averages" like of data, and not "methods".</span>
			</p>

			<p>
				We use a <b>heuristic-based</b> improvement: starting with initial random means, and iterate by:
			</p>
			<ul>
				<li><b>Fix means, optimise points.</b> Assigning each \(\x_i\) to its closest mean.</li>
				<li><b>Fix points, optimise means.</b> Recompute means</li>
				<li>Refine until convergence (the means do not move or do not move enough)</li>
			</ul>
			<p>
				And our metric of goodness (i.e. loss function) will be defined as:
			</p>
			<p class="blue">
				For cluster means \(\vec{m} \) and assignments \(\vec{r} \), the <b>mean squared distance</b>:

				\[
				\cal{L}(\{\vec{m}\}, \{\vec{r}\}) = \sum_{i=1}^n \sum_{j=1}^K r_j^i \lVert \vec{m}_j - \x_i \rVert^2 \pod{r^i_j \in \{0,1\}}
				\]

				Where \(r^i_j\) of data \(i\) to cluster \(j\) is 1 if that data is assigned to that cluster, or 0 otherwise. 
			</p>
			<p>
				Note: thus each \(\x_i\) will have a \(\vec{r}\) being a <a href="https://en.wikipedia.org/wiki/One-hot"><i>one-hot-encoding</i></a> of its cluster.
			</p>

			<p>
				The way a sample is assigned to a mean \(k\) is via euclidean distance \(k_i = \argmin\limits_k \lVert \vec{m}_k - \vec{x}_i \rVert^2\)
			</p>
			<p>
				And means are adjusted via \(\vec{m}_k = \dfrac{\sum_i r^i_k \x_i}{\sum_i r^i_k }\)  -- we need to include \(r^i_k\) to ensure we don't include features not assigned to that mean currently.
			</p>

			<p>
				While K-means' loss heuristic is non-convex (i.e. not guaranteed global minimum), we <i>are</i> guaranteed a <i>local</i> minimum, we won't diverge ever. 
			</p>

			<p>
				Some issues though:
			</p>
			<ul>
				<li>
					\(K\) is a hyperparameter - if you don't know how many there should be, welp, sucks for you.
				</li>
				<li>
					Each example can only be assinged to one cluster (fixable)
				</li>
				<li>
					May converge to sub-optimal groups (can use methods such as random restart, non-local split and merge to fix)
				</li>
			</ul>

			<h3>Soft K-means</h3>

			<p>
				Soft K-Means makes <b>partial assigments</b> -- a sample can have partial membership of two clusters. For example, if an \(\x\) is a point that is very close to mean A and further but still close to mean B, it could be given the assignment \(\vec{r} = \{A = 0.84, B = 0.16\}\), like a <i>probability</i>.
			</p>
			<p>
				We also call this <i>fuzzy k-means</i>.
			</p>
			<p class="blue">
				We must set that all assignments must sum to one, using an important function called <b>softMax</b>:

				\[
				r^i_k = \frac{\exp(-\beta \lVert \vec{m}_k -\x_i \rVert^2)}{
					\sum\limits_{j=1}^K \exp(-\beta \lVert \vec{m}_j -\x_i \rVert^2))
				}
				\]

				Where \(\exp(x)\) is the exponential function: \(e^x\). <br>
				\(\beta\) is the <b>stiffness parameter</b>, controlling how "soft" assignments are -- emphasising one main class or allowing a more even spread.
			</p>
			<p>
				If you plot the softMax curve, the stiffness controls how steep the center bit is. 
			</p>
			<p>
				For Soft K-means, we alter the loss function slightly. We say that now \(r^i_j \in [0,1]\), that we can get a range of values. Then, for one mean the loss is \(\newcommand{\m}{\vec{m}} \m_l = \argmin_\m \sum\limits_{i=1}^n r^i_l \lVert \m - \x_i \rVert^2\), and overall
				\[
				\cal{L}(\m) = \sum_{l=1}^n \vec{r}^i \lVert \m - \x_i \rVert^2
				\]
			</p>
			<p>
				This loss function is actually quadratic - there is a unique solution. 
			</p>
			<p>
				Said unique solution is given by solving the first derivative \(\nabla \cal{L} (\m) = \sum_{i=1}^n \vec{r}^i(2\m - 2\x_i).\)
			</p>

			<p>
				What we can do with K-means is actually do a primitive form of data reduction - approximating the data onto a lower space. In this case, by replacing all data points with their closest means, we do <b>vector quantisation</b>, which has this effect.
			</p>

			<p>
				Thus overall, the soft k-means algorithm is as thus:
			</p>

			<ol>
				<li>
					Assing points via \(r_k^i = \textrm{softMax}_\beta (\lVert \m_k - \x_i \rVert^2 \)
				</li>
				<li>
					Update points via \(\m_k = \dfrac{\sum_{i=1}^n r_k^i \x_i}{\sum_{i=1}^n r_k^i }\)
				</li>
			</ol>
		</div>

		<div class="colourband">
			<h2 id="pca">Principle Component Analysis (PCA)</h2>
		</div>

		<div class="cbox">

			<h3>Introduction</h3>
			<p>
				<b>Principal Component Analysis</b> is fundamentally about approximating a feature set in a lower-dimensional <b>"latent space"</b> or <b>subspace</b>. i.e. reduce the dimensions, to extract higher level features, or to store less data overall.
			</p>
			<p>
				One can think about images: images are comprised of possibly hundreds or thousands of pixels, but we can extract larger regions, to act as "components" to reconstruct said image. 
			</p>
			<p>
				<b>Latent Factor Models</b> (LFMs) are ML models that convert data to latent spaces.
			</p>
			<p>
				We call the original dataset \(\X\), and can call the transformed "latent" set \(\Z\).
			</p>

			<p>
				One method of this is <b>vector quantisation</b> - by doing k-means and then replacing each feature with its mean, we have replaced a sample sized \(1 \times d\) (d features) with \(1 \times k\) (k means). Now of course, if \(k > d\), this is hardly helpful.
			</p>
			<p>
				Instead, we want to somehow learn the <i>basic components</i> that make up a sample \(x\), and weight them appropriately to reconstruct the original sample (as well as possible).
			</p>

			<h3>PCA</h3>

			<p>
				The PCA algorithm takes an \(\X\), and produces two vectors, \(\Z, \W\), which are in the following form:
			</p>

			<figure>
				<img src="./assets/pca-matrix.png" alt="PCA matrix" style="max-width: 500px;">
				<figcaption>
					Such that \(\X = \Z\W\), note the dimensions, \((n\times d) = (n \times k) \times (k \times d)\)
				</figcaption>
			</figure>

			<ul>
				<li>
					\(\W\) are our <b>principal components</b>, the components needed to reconstruct the features of \(\X\), ranging from the most fundamental component at the top (the first <b>row</b>), to the least important component at the bottom. We have \(k\) of these. 
				</li>
				<li>
					\(\Z\) are the <b>weights</b> that reconstruct \(\W\) into \(\X\) - each \(\x_i = \W\t \z_i\). One single feature of one sample, \(x_{ij} \) involves multiplying row \(i\) of <b>Z</b> with column \(j\) of <b>W</b>.
				</li>
				<li>
					\(k \leq d\). Note that if \(k \ll d\), we can save a lot of storage space. <span class="grey"><i>The proof is trivial and left as an excersise to the reader.</i></span>
				</li>
			</ul>
			<p>
				PCA is a <b>linear</b> reduction - it can't extract non-linear features on its own. It's essentially just doing a matrix factorisation.
			</p>
			<p>
				PCA is also useful especially to visualise high-dimensional data, as we only live in 3D. 
			</p>

			<h4>The Theory</h4>

			<p>
				[1] To do PCA, we must first <b>center</b> data. Given an \(\X = \{\x_1, \x_2, \dots, \x_d\}\), compute the mean of its <b>features</b>
				\[
				\vecu{\mu} = \fr{1}{n} \sum_{i=1}^n \x_i
				\]
				and then take it away from \(\X\) row-wise; \(X - \vecu{\mu}\). <span class="grey">I'm using underline vector notation for mu, since LaTeX doesn't boldface greek fonts.</span>
			</p>

			<p>
				[2] To solve PCA, we need to learn weights, and adjust based on <b>reconstruction error</b>. We can use MSE for this:
				
				\begin{align}
				\cal{L}(\W, \Z) &= \sum_{i=1}^n \lVert \W\t \z_i - \x_i \rVert^2 \\
				&= \lVert \Z\W - \X \rVert^2_F
				\end{align}

				for two matrices, \(\W, \X\). Note the subscript F is the <b>frobenius norm</b> -- L2 norm but over matrices. 
			</p>

			<p>
				[3] The PCA objective function is non-convex -- it doesn't have a unique global minimum. This makes finding a solution hard.
			</p>
			<p>
				We can enforce a few constraints on each row/component to help us reduce bias and redundancy: 
				<ol>
					<li>Normalisation: enforce \(\lVert \w_c \rVert = 1\)</li>
					<li>Orthogonality: enforce \(w_c \cdot w_{c'} = 0 \forall c \neq c'\)</li>
					<li>Sequentially fit components (fit \(w_0\) first, and \(w_d\) last)</li>
				</ol>
				These constraints are important for our PCA to be effective. Below we can see what this does to a 2D space.
			</p>

			<figure>
				<img src="https://miro.medium.com/max/900/1*_G3fNG6wHJMm9ZpJ7Mw9dg.png" alt="" style="max-width: 600px;" title="">
				<figcaption>
					PCA effectively finds new <i>basis</i> vectors for a set of data. At \(d\) vectors, we can use this to perfectly reconstruct the data -- though of course this no longer is dimension reducing. <a href="https://medium.com/@gaurav_bio/creating-visualizations-to-better-understand-your-data-and-models-part-1-a51e7e5af9c0">[src]</a>
				</figcaption>
			</figure>

			<p>
				It is easiest to find the first, or "main" axis of the data, and then find subsequent ones. We can also see all the data has been centered -- this ensures our components are not skewed one way or another, and originate from 0.
			</p>

			<p>
				[4] Finally, to actually solve this, we use <b>Single Value Decomposition</b> (SVD).
			</p>

			<h4>SVD</h4>

			<div class="blue">
				<p>
					Given an \(\X \in \bb{R}^{n \times d}\), SVD decomposes \(\X\) into the following:
					\[
					\X = \vec{U} \vec{\Sigma} \vec{V}\t
					\]
				</p>
				<ul>
					<li>
						\(\vec{\Sigma} \in \bb{R}^{n \times d} \) is a diagonal matrix of eigenvalues
					</li>
					<li>
						\(\vec{U} \in \bb{R}^{n \times n}\) is an orthonormal matrix (orthogonal* and normal columns)
					</li>
					<li>
						\(\vec{V}\t \in \bb{R}^{d \times d}\) is our orthogonal matrix of components.
					</li>
				</ul>
				<p>
					In linear PCA, \(\vec{V}\t = \W\). We can then take the top \(k\) rows.
				</p>
				<p>
					*a matrix <b>A</b> is orthogonal if \(\vec{A}^{-1} = \vec{A}\t\).
				</p>
			</div>

			<p>
				The eigenvalues in \(\vec{\Sigma}\) are <i>related</i> to \(\nm{\X}\t \nm{\X} \) (where \(\nm{\X}\) is the normalised matrix X). The actual eigenvalues are \(\fr{\vec{\Sigma}.^2}{n-1}\) (\(.^2 \) is element wise squared), though I have not found this to be useful.
			</p>
			<p>
				\(\vec{U}\) is simply not useful. And \(\vec{V}\t \equiv \W\) is <b>very</b> useful.
			</p>

			<div class="side">

				<p>
					To approximate on test data, we need our mean \(\vecu{\mu_j}\) from the <i>training data</i>, and our components \(\W\).
				</p>

				<p>
					Given \(\tilde{\X}\), center it using \(\vecu{\mu_j}\), then compute the new \(\tilde{\Z}\):

					\[
					\tilde{\Z} = \tilde{\X}\W\t
					\]

					And we can use \(\Z\). We can work out the reconstruction error by doing \(\lVert \tilde{\Z} \W - \tilde{\X} \rVert_F^2\)
				</p>
			</div>

		</div>

		<div class="colourband">
			<h2 id="linclass">Linear Classification</h2>
		</div>

		<div class="cbox">

			<h2>Contents</h2>

			<ol>
				<li><a href="#lin-logistic">Linear Classification</a></li>
				<li><a href="#lin-percep">The Perceptron Algorithm</a></li>
				<li><a href="#lin-svm">Support Vector Machines</a></li>
			</ol>

		</div>

		<div class="cbox">
			<h2 id="lin-logistic">Linear Classification</h2>

			<h3>Binary Linear Classification</h3>

			<p>
				Linear Classification is the idea of using regression models to do classification.
			</p>

			<p>
				To do binary classification, we need to convert the class to a numerical value that we can compare to -- for positive and negative values of \(y_i\), <b>replace the positive with +1, and the negative with -1</b>. 
			</p>
			<p>
				From our regression value we then determine a class using the <code>sign</code> function -- <code>sign</code> returns +1 if \(y_i > 0\), -1 if \(y_i < 0\), and 0 if \(y_i = 0\). The 0-result is a decision boundary, much like in KNN. <span class="grey">We'll just pretend these don't exist for now.</span>
			</p>

			<figure>
				<img src="./assets/1d-decision-boundary.png" alt="1d decision boundary for regression" style="max-width: 400px;">
				<figcaption>
					The decision boundary is where the regression model "intersects" with the x axis, having \(y_i = 0\).
				</figcaption>
			</figure>
			<p>
				There are many lines which produce the same decision boundary, and so it may be difficult to immediately solve for an answer.
			</p>

			<p>
				Often, it is better to just draw the decision boundary, and not necessarily the line itself. This also means that we don't need a dimension for y. 
			</p>

			<figure>
				<img src="https://scipython.com/static/media/uploads/blog/logistic_regression/decision-boundary.png" alt="Linear (Logistic) classification decision boundary" style="max-width: 360px;">
				<figcaption>
					Decision boundary of Linear Classification for 2D features. <a href="https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/">[src]</a>
				</figcaption>
			</figure>

			<p>
				This splits the data into two <i>half-spaces</i>, for the two classes. 
			</p>
			<p>
				If we can split the data into two with that straight line, we say it is <b>linearly separable</b>. 
			</p>

			<h4>Losses</h4>

			<p>
				One simple error to use is the <b>0-1 Loss</b> -- count the number of wrong predictions.
				\[
				\cal{L}_{0-1}(\ypred, y_i) = \begin{cases} 0 &\rm{if } \ypred = y_i \\ 1 &\rm{if } \ypred \neq y_i \end{cases} \pod{\textrm{ or simply } [\ypred \neq y_i]}
				\]
				However, the 0-1 loss is a <a href="https://en.wikipedia.org/wiki/Heaviside_step_function">step function</a>, and has a gradient of 0 everywhere -- rather inconvenient when doing the calculus. 
			</p>
			<p>
				Thus what we want instead is a <i>convex approximation</i> that has the same effect as 0-1 loss:
			</p>
			<p>
				If \(\w \cdot \x_i > 0 \land y > 0\), this is a correct prediction. <br>
				If \(\w \cdot \x_i &lt; 0 \land y &lt; 0 \equiv -\w \cdot \x_i > 0 \land y &lt; 0\), this is a correct prediction. <br>
				(Mismatched less than greater thans are incorrect)
			</p>
			<p>
				Thus doing some rearranging a correct prediction is always if \(y_i \w \cdot x_i > 0\). A wrong prediction should always be 0 to approximate 0-1. However, we are trying to <i>minimise</i> errors, so we reverse everything with a minus:
				
				
			</p>
			<p>
				Thus, we can have a loss function of 
				\[
				\cal{L}(\w) = \sum_{i=1}^n \max(0, -y_i\w \cdot \x_i)
				\]
			</p>
			<p>
				However, with this loss there is the chance that \(\cal{L}(\w = \vec{0}) = 0\): i.e. absolutely no weights gets 0 loss. In this case, we are not learning, and the solution is <b>degenerate</b>
			</p>
			<p>
				We change this by adding an offset, s.t. instead of \(y_i \w \cdot \x_i > 0\), do \(y_i \w \cdot \x_i \geq 1\). This is called <b>hinge loss</b>. Whilst this penalises a small section of valid weights, it solves our degeneracy problem.

				\[
				\cal{L}(\w) = \sum_{i=1}^n \max(0, 1-y_i\w \cdot \x_i)
				\]
			</p>
			<p>
				Hinge loss is still not smooth, however (it has a fold), and we prefer smooth graphs, so we will "smooth out" the max by using a <i>log-sum-exp</i> method: \(\max(0, y_i \w \cdot \x_i) \approx \log(e^0+ e^{y_i \w \cdot \x_i} \), giving us the most popular loss function: <b>logistic loss</b>. 
			</p>

			<p class="blue">
				<b>Logistic Loss</b> is given as 
				\[
				\cal{L}(\w) = \sum_{i=1}^n \log(1+ e^{y_i \w \cdot \x_i})
				\]
				and linear classification using this loss has another name: <b>Logicstic Regression [Classifier]</b>.
			</p>

			<figure>
				<img src="./assets/logistic-loss.png" alt="Loss function" style="max-width: 600px;">
				<figcaption>
					<span style="color: darkred;" >Red</span>: Original 0-1 approximation <br>
					<span style="color: darkblue;" >Blue</span>: Hinge loss <br>
					<span style="color: darkgreen;" >Green</span>: Logistic Loss
				</figcaption>
			</figure>

			<h3>Multi-class Classification</h3>

			<p>
				We can extend logistic regression to multiple classes. 
			</p>
			<p>
				<b><i>One vs All.</i></b> for \(k\) possible classes, train \(k\) binary classifiers, based on the two classes \(\in, \not\in\) (in that class or not)
			</p>
			<p>
				Apply all classifiers to a feature, and return the highest scoring (argmax). 
			</p>
			<p>
				We can even do this in one go - put all the weights of the different classifiers as rows in a matrix 
				\[
				\W_{(k \times d)} = \begin{bmatrix}
					\lla \w_1 \lra \\ \vdots \\ \lla \w_k \lra
				\end{bmatrix}
				\]
				Then just predict all classes \(\hat{\y}_i = \W\t \x_i\), or predict the entire set with \(\hat{\vec{Y}} = \X\W\t\).
			</p>

			<p>
				<i>Problem:</i> this assumes all classes are independent (they are not).
			</p>

			<p>
				<b><i>Softmax Loss.</i></b> We want to train the model dependently. It is sufficient to have our loss be that the correct class is predicted highest: 
				\[
				\w_c \cdot \x_i \geq \max_{\forall k} (\w_{k} \cdot \x_i)
				\]
				The value of \(\w \cdot \x_i\) for the <b>correct</b> class \(c\) is the largest value amongst all the possible \(k\) (including \(c\)).

			</p>
			<p>
				Max is not smooth, and so we do the same log-sum-exp step like logistic:
				\begin{align}
				0 \geq &-\w_c \cdot \x_i + \log(\sum_{k=1}^K \exp(\w_k \cdot \x_i)) \\

				\iff &-\log \left(\fr{ \exp{(y_{i}^{correct})} }{ \sum_{k=1}^K \exp(\w_
				k \cdot \x_i )}\right)
				\end{align}
				Where \(y_{i}^{correct}\) is our \(\w_c \cdot \x_i\) from earlier. This is called <b>softmax loss</b>. From this, we assemble a full loss function, which is commonly called categorical cross-entropy:
			</p>

			<p class="grey">
				Again, I'm using dot product notation, slides consistently use matrix notation \(\w_k\t \x_i\). Just bear this in mind.
			</p>

			<p class="blue">
				<b><i>Categorical Cross-Entropy</i></b>, softmax loss with L2-reg, is thus given as
				\[
				\cal{L}(\W) = \sum_{i=1}^n \left( -\w_c \cdot \x_i + \log\left(\sum_{k=1}^K \exp(\w_k \cdot \x_i) \right) \right) + \fr{\lambda}{2} \lVert\W \rVert_F^2 	
				\]
			</p>
		</div>

		<div class="cbox">
			<h2 id="lin-percep">The Perceptron Algorithm</h2>

			<p>
				The Perceptron is an alternate representation of a linear classifier, and also the basis to neural networks. 
			</p>

			<figure>
				<img src="./assets/perceptron-diag.png" alt="Perceptron diagram" style="max-width: 600px;">
			</figure>
			<p>
				It's graphical; individual features are represented as nodes, and links in the graph represent weights. Features &times; weights. In this way, it roughly mimics how (biological) neurons work (hence neural networks)
			</p>

			<p>
				The idea is to find a vector of weights \(\w\) that linearly separate data, same as logistic regression. We iteratively improve the weights over <i>epochs</i> (iterations), with \(\w^0\) being initial wegihts, and \(\w^t\) being the weights at epoch \(t\).
			</p>

			<div class="codediv">\(\w \lla \vec{0}\) <span class="grey">// vector of zeros, doesn't have to be zeros</span>
for epoch \(t\) from 1 .. \(n\): <span class="grey">[*]</span>
	for all \(\x_i\):
		predict \(\ypred \lla \w \cdot \x_i\)
		if \(\ypred \neq \ytest\):
			\(\w \lla \w + \alpha\, y_i \x_i\) <span class="grey">[**]</span>
			</div>

			<p>
				<code>[*]</code> Note: can replace with <code>while there are still errors</code>, but if the data is not linearly separable, this will loop.
				<br>
				<code>[**]</code> Note 2: original material doesn't include learning rate \(\alpha\) -- this means \(\alpha = 1\).
			</p>
			<p>
				The use of \(y_i \x_i\) has the same idea as logistic loss - because y is -1 or +1. this encourages \(\w\) to tend towards the right prediction for those features \(x_i\) (and discourages tending to the wrong prediction), repeated enough, all the features should "smooth out" and get you a good \(\w\).
			</p>
			<p>
				Adding a bias is the same as adding an extra input node with a constant value 1.
			</p>
			<p>
				For multi-class perceptron, because there are multiple classes, you may have to do the encouraging (+) and discouraging (-) separately, because classes will be one-hot encoded.
			</p>
		</div>

		<div class="cbox">
			<h2 id="lin-svm">Support Vector Machines</h2>

			<div style="display: flex; align-content: center; align-items: center;">

				<figure>
					<img src="https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm.png" alt="SVMs" style="max-width: 300px; width: 300px;">
					<figcaption>
						Support Vector Machines <a href="https://www.javatpoint.com/machine-learning-support-vector-machine-algorithm">[img]</a>
					</figcaption>
				</figure>
				<figure>
					<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/300px-SVM_margin.png" alt="SVM detail" style="max-width: 300px; width: 300px;">
					<figcaption>
						Support Vector Machine - optimisation diagram [wikipedia]
					</figcaption>
				</figure>
			</div>

			
			
			<p>
				Decision boundary of linear classifier is perpendicular to the line itself. There may be multiple permissible boundaries. 
			</p>
			<p>
				The "best" boundary is one that maximises the difference between nearest points (vectors) -- maximises the margin between them. 
			</p>
			<p>
				The points (vectors) closest to the line, that define the margin, are thus the <b>support vectors</b>. 
			</p>

			<p>
				<b><i>Optimising.</i></b> Set the weights such that \(\w \cdot \x_i = +1,\; \w\cdot\x_j = -1\), for the two support vectors \(\x_i, \x_j\) (anything beyond that is more negative, and more positive). 
			</p>
			<p>
				Thus given the distance between the two support vectors is \(d\), we know \(\w \cdot \x_i = 1\) is the same as \(\w  \cdot(\x_j + d \fr{\w}{|\w|}) = 1\). Rearranging this we get 
				\[d |\w| = 2 \iff d = \fr{2}{\w}\]
			</p>
			<p>
				Thus maximise \(d = \fr{2}{\w}\), which is the same as minimising \(\fr{1}{2}|\w|\), or \(\fr{1}{2}|\w|^2\).
			</p>
			<div class="side">

				<p>
					<b>Minimise</b> \(\fr{1}{2}\lVert\w\rVert^2\), subject to
				</p>
				\[
				\begin{matrix}
					\w \cdot \x_i \geq 1 & \rm{for } y_i = 1 \\ 
					\w \cdot \x_i \leq -1 & \rm{for } y_i = -1
				\end{matrix}
				\]
			</div>
			<p>
				The violation is how much "off" they are: for \(y_i = 1, \w \cdot \x_i = -0.5\), the violation is 1.5.
			</p>
			<p>
				This is just <b>hinge loss</b> - <b>SVMs are L2 regularised hinge loss</b>.
			</p>

			<h3>Multiclass SVMs</h3>

			<p>
				We want to find a loss that encourages the largest \(\vec{w}_c \cdot \vec{x}_i\) to be the correct prediction, denoted \(\vec{w}_{y_i} \cdot \vec{x}_i\). Definable as 

				$$
				\vec{w}_{y_i} \cdot \vec{x}_i > \vec{w}_{c} \cdot \vec{x}_i \pod{\forall c \neq y_i}
				$$

				But penalising violations is "degenerate" (vecs equal to 0), thus can redefine as

				\begin{align}
				\vec{w}_{y_i} \cdot \vec{x}_i &\geq \vec{w}_{c} \cdot \vec{x}_i  +1\pod{\forall c \neq y_i}\\
				\implies 0 &\geq 1 - \vec{w}_{y_i} \cdot \vec{x}_i +\vec{w}_{c}\cdot \vec{x}_i
				\end{align}
				
				
			</p>
			<p>
				Two ways to measure violation:
			</p>
			<ul>
				<li>
					The sum of all violations: $$
\sum\limits_{c \neq y_i} \max(0, 1 - \vec{w}_{y_i} \cdot \vec{x}_i +\vec{w}_{c}\cdot \vec{x}_i) \longrightarrow SUM
$$
				</li>
				<li>

					The largest violation: $$
\max_{c \neq y_i}(\max(0, 1 - \vec{w}_{y_i} \cdot \vec{x}_i +\vec{w}_{c}\cdot \vec{x}_i)) \longrightarrow MAX
$$
				</li>
			</ul>
			<p class="blue">
				This gives us a loss function of 
				$$

\begin{matrix}\mathcal{L}(\vec{W}) = \sum\limits_{i=1}^n SUM + \frac{\lambda}{2}||\vec{W}||^2_F&
\textrm{or}&
\mathcal{L}(\vec{W}) = \sum\limits_{i=1}^n MAX\end{matrix} + \frac{\lambda}{2}||\vec{W}||_F^2
$$
			</p>
			<p>
				For \(k\) classes,
			</p>
			<ul>
				<li>
					SUM gives a penalty of \(k-1\) for \(\vec{w}_c = 0\)
				</li>
				<li>
					MAX gives a penalty of 1 instead.
				</li>
			</ul>
		</div>

		<div class="colourband">
			<h2 id="kernel">Kernel Trick</h2>
		</div>

		<div class="cbox">
			<p>
				Sometimes, your data is not linearly separable. But it is separable if you re-basis it, such as polynomial regression:
			</p>
			<ul>
				<li>
					By converting features from \(\X = \begin{bmatrix}
					\x_1& \x_2
					\end{bmatrix}\) to \(\Z = \begin{bmatrix}
					1 & \x_1 & \x_2 & (\x_1)^2 & (\x_2)^2 & \x_1 \x_2 
				\end{bmatrix}\)
					we are separating with polynomials.
				</li>
				<li>
					But note that we go from 2d to 6d, if we want to correctly do it, and this becomes a problem with high dimensions.
				</li>
			</ul>
			<p>
				We can merge L2 regularisation into our prediction formula, and get an equation 
				\[
				\vec{v} = \Z\t (\Z \Z\t + \lambda \vec{I})^{-1} \y
				\]
				for a new set of weights \(\vec{v}\) based on our new basis. 
			</p>

			<button class="collapsible">Derivation... </button>
			<div class="ccontent">
				<ul>
					<li>
						L2 Reg: \[\cal{L}(\vec{v}) = \fr{1}{2} \lVert \Z\vec{v} - \y \rVert^2 + \fr{\lambda}{2} \lVert \vec{v} \rVert\]
					</li>
					<li>
						\[
						\nabla \cal{L}(\vec{v}) = \Z\t\Z \vec{v} - \Z\t \y + \lambda\vec{v}
						\]
					</li>
					<li>
						\[
						\implies \vec{v} = (\Z\t \Z + \lambda \vec{I})^{-1} (\Z\t \y)
						\]
						where \(Z_{(k \times k)}\) is always invertible
					</li>
					<li>
						\[
						\implies \vec{v} = \left[ \Z\t (\Z \Z\t + \lambda \vec{I})^{-1} \right]_{(n \times n)} \y
						\]
					</li>
				</ul>
			</div>

			<p>
				Now for test data \(\tl{\X}\) predict \(\hat{\y}\) by computing \(\tl{\Z}\):

				\begin{align}
					\hat{\y} &= \tl{\Z} \vec{v} \\
					&= \tl{\Z} \Z\t (\Z \Z\t + \lambda \vec{I})^{-1} \y
				\end{align}
			</p>
			<p>
				This is however computationaly expensive, but we have the blocks \(\tl{\Z} \Z\t,\; \Z\Z\t\), and so let us call them \(\tl{\vec{K}}, \vec{K}\).
			</p>

			<p>
				The \(\vec{K}\) is now the <b>kernel matrix</b>, and is an \(n \times n\) matrix <i>computed from</i> \(\Z\). Each cell \(k_{ij}\) is just \(\z_i \cdot \z_j\).
			</p>
			<p class="blue">
				The <b>kernel trick</b> is to simply not: don't calculate \(\Z\),  calculate \(\vec{K}\) directly
			</p>

			<button class="collapsible">Example for quadratic regression... </button>
			<div class="ccontent">
				<p>
					<b><i>Example.</i></b> Let \(\x_i = (a, b), \; \x_j = (c, d)\). 
				</p>
				<p>
					To do quadratics, we can: get \(\z_i = (a^2, ab\sqrt{2}, b^2)\) and \(\z_j = (c^2, cd\sqrt{2}, d^2)\)
				</p>
				<p>
					The dot product \(\z_i \cdot \z_j =\)
					\begin{align}
						&= a^2 c^2 + (ab\sqrt{2} )(cd\sqrt{2}) + b^2 d^2 \\
						&= (ac + bd)^2 \\
						&= (\x_i \cdot \x_j) ^2
					\end{align}
				</p>
				<p>
					i.e. there's no point to \(\z\), just have a <b>kernel function</b> \(\kappa(\x_i, \x_j) = (\x_i \cdot x_j)^2\) and skip the \(\z\).
				</p>
			</div>

			<p class="blue">
				We have a <b>kernel function</b> \(\kappa(\x_i, \x_j)\) that computes cell \(k_{ij}\) of the kernel, skipping the middleman. 
			</p>
			<p>
				They're usually given to us. The kernel function must obey Mercer's Theorem. 
			</p>
			
			<button class="collapsible">Mercer's Theorem... </button>
			<div class="ccontent">
				<p>
					<b><i>Theorem.</i></b> Rather than first mapping from X to Z with a function \(\phi : \X \lra \Z\), we instead compute a <i>modified inner product</i> \(\kappa(\cdot, \cdot)\), which is a function that fits a bunch of formal stuff, but most importantly is <b>symmetric</b>: \(\kappa(a, b) = \kappa(b,a)\)
				</p>
			</div>

			<p>
				Now, the process becomes
			</p>
			<ul>
				<li>
					Form \(\vec{K}\) from \(\X\), compute \(\vec{u} = (\vec{K} + \lambda \vec{I}) ^{-1} \y\) (u are weights)
				</li>
				<li>
					Form \(\tl{\vec{K}}\) from \(\tl{\X}\), and compute \(\hat{\y} = \tl{\vec{K}} \vec{u}\).
				</li>
			</ul>
			<p>
				Kernel trick can be extended to non vector data (like text, images), and can be used for multiple things, <b>including PCA</b>. 
			</p>
		</div>
		
		<div class="colourband">
			<h2 id="mle">MLE and MAP estimation</h2>
			<p class="subheading">Maximum likelihood / Maximum <i>a-posteriori</i> estimation</p>
		</div>

		<div class="cbox">

			<p class="grey md-conv">
				Okay, look, I don't *actually* have much of a clue what's going on here.
			</p>

			<p>
				To clarify notation:
			</p>
			<ul>
				<li>
					\(\max(\cdot)\) and \(\min(\cdot)\) return the maximum and minimum <i>value</i> of a function, w.r.t a parameter.
				</li><li>
					\(\argmax(\cdot)\) returns the *parameter* that achieves the maximum (or minimum) value
				</li>
				<li>
					In some case, argmin, argmax return sets:
$$
\begin{align*}
\argmin_w((w-1)^2) &\equiv \{1\}\\
\argmax_w(\cos w) &\equiv \{\dots, -2\pi, 0, 2\pi, \dots\}
\end{align*}
$$
				</li>
			</ul>

			<h3>MLE</h3>
			<p>
				Assume a dataset D with parameter \(w\). Ex. flip a coin 3 times and get HHT, parameter \(w\) is the likelihood that the probability lands on heads. 
			</p>

			<p>
				The <b>likelihood</b> of D is a <i>probability mass function</i> \(p(D|w)\): prob of observing D given parameter \(w\)
			</p>
			<ul>
				<li>
					Given \(D = \{H,T,T\}\) and \(w=0.5\), then \(p(D|w) = 0.5 \times 0.5 \times 0.5 = 0.125\)
				</li>
				<li>
					Given \(w=0\) then \(p(D|w) = 0\).
				</li>
			</ul>
			<p>
				You can plot \(p(D=\{\cdots\}|w)\) w.r.t. D and \(w\). If this is continous, you have a probability distribution. Since it's w.r.t D, the area under the dist does not sum to 1.
			</p>

			<p class="blue">
				We get the <b>maximum likelihood estimation</b>:
\[
MLE  = \hat{w} \in \argmax_{w}(p(D|w))
\]
			</p>

			<p>
				To compute MLE, we can use argmin, and minimise the negative-log-likelihood:
\[
\argmax_\vec{w} (p(D|\vec{w})) \equiv \argmin_\vec{w}(-\log(p(D|\vec{w})))
\]
We want to switch it to minimise so we can integrate it as a normal log function, also log removes multiplications which makes things more efficient:
\[
\log(\alpha\beta) = \log\alpha + \log\beta
\]
			</p>

			<p>
				If our dataset D has \(n\) examples:
\[
p(D|\vec{w}) = \prod_{i=1}^n p(\vec{x}_i | \vec{w})
\]
Then MLE would be
\[
\vec{\hat{w}} \in \argmax_\vec{w} (p(D|\vec{w})) \equiv \argmin_\vec{w}(\sum\limits_{i=1}^n p(\vec{x_i}|\vec{w}))
\]
			</p>

			<p class="blue">
				<b><i>Least Squares is Gaussian MLE.</b></i> Recall least squares: minimise errors squared. If we grab the errors \(\epsilon_i\) and plot a histogram of error occurences, it forms a gaussian (normal) dist.
			</p>

			<p>
				More generally, have \(y_i = \vec{w}\cdot \vec{x_i} + \epsilon_i\) where \(\epsilon_i\) follows a normal dist,
\[
p(\epsilon_i) = \frac{1}{\sqrt{2\pi}}\exp(-\frac{\epsilon_i^2}{2})
\]
we have a <b>gaussian likelihood</b> for \(y_i\): 
\[
p(y_i|\vec{x_i},\vec{w}) =  \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{(\vec{w}\cdot\vec{x_i}-y_i)^2}{2}\right)\longrightarrow G(\vec{w, \vec{x_i}})
\]
Then we just find \(\argmin_\vec{w}(-\sum\limits_{i=1}^n \log(G(\vec{x_i,w}))\).
			</p>

			<p>
				After simplifying, the inner function of the argmin leads to \(-\sum\limits \left[\log\frac{1}{\sqrt{2\pi}} - \frac{1}{2}(\vec{w} \cdot \vec{x_i} - y_i)^2 \right]\) (\(\log\exp\) cancels.)
			</p>

			<p>
				Thus finally, we get a \(k + \frac{1}{2}\sum\limits_{i=1}^n (\vec{w}\cdot \vec{x_i}-y_i)^2\) for some constant \(k\), and to minimise this, is just to minimise least squares.
			</p>

			<button class="collapsible">Example... </button>
			<div class="ccontent">
				<p>
					<b><i>Example.</b></i> MLE for a normal dist. Assume that samples \(\vec{X} = \{\vec{x_i}\dots \vec{x_n}\}\) and their conditional probability is normal dist:
\[
p(\vec{X}|\mu, \sigma) = \left(\frac{1}{\sigma \sqrt{2\pi}}\right)^n \exp\left( -\frac{1}{2\sigma^2}  \sum\limits_{i=1}^n(\vec{x_i} - \mu^2)  \right)
\]
We want to find the MLE
\[
\hat{\mu}, \hat{\sigma} \in \argmin(-\log(p))
\]
Which is a quadratic function. To find the minima, we can compute partial derivatives w.r.t \(\mu, \sigma\).
				</p>
				<p>
					Solving for \(\mu,\sigma\), we can get
\[
\begin{align*}
\hat{\mu} &= \frac{1}{n}\sum\limits_{i=1}^n \vec{x_i} \\
\hat{\sigma^2} &= \frac{1}{n}\sum\limits_{i=1}^n (\vec{\hat{x}_i} - \hat{\mu})^2
\end{align*}
\]
				</p>
			</div>
		</div>

		<div class="cbox md-conv">
			#### MAP

			MLE can easily lead to overfitting with little observations -- a coin tossed three times gets {HHH}, the MLE would find $p_{heads}$ = 1.

			Rather than finding a $\hat{w} \in \argmax_w(p(D|w))$, we want to find $\argmax_w(p(w|D))$.

			<div class="blue">
				The <b>maximum a-posteriori estimation</b> is finding
				$$ \hat{\w} \in \argmax_\w (p(\w|D)) $$
			</div>

			MAP can be gotten by MLE using the *bayes rule*:

			$$ 
			p(\w | D) = \fr{p(D|\w)p(\w)}{p(D)} \propto p(D|\w)p(\w) 
			$$

			Thus MAP maximises MLE, but multiplies it with the *prior* of $\w$: the belief that $\w$ is correct before seeing D.

			We can simplify MAP by using the **conditional independence assumption**:

			$$
			\argmax_\w (p(\w | \X)) = \argmax_\w \left( p(\w)\prod_{i=1}^n [p(\x_i|\w)]  \right) 
			$$

			And since machine learning algorithms want to minimise, we can take the *negative log*, and add a regulariser. The reason why we take the log is to convert a product into a sum (because of log laws).

			<div class="blue">
			$$
			\hat{\w} \in \argmin_\w \left\{ -\sum_{i=1}^n [\log (p(\x_i | \w))] - \log(p(\w)) \right\}
			$$
		</div>

			One can consider the minus log of the prior to be a regulariser for the weights. 

			
		</div>

		<div class="cbox">
			<button class="collapsible">MAP is just L2 Loss</button><div class="ccontent md-conv">
				***MAP is just L2-loss***. Since each dimension of $\w$ comes from the gaussian curve with $\mu = 0$, and is independent, we know the following:

				$$ 
				p(\w) = \prod_{j=1}^d p(w_j) \propto \prod_{j=1}^d \exp(-\fr{\lambda}{2} w_j^2) \propto \exp(-\fr{\lambda}{2} \sum_{j=1}^d w_j^2).
				$$

				Take the negative log like before, for minimisation, and we get
				$$
				\fr{\lambda}{2} \lVert  \w \rVert^2.
				$$

				The MAP esitmate **for all y** of the training examples is given as 
				\begin{align}
				\hat{\w} &\in \argmin_\w \{ -\log(p(\y | \X, \w)) - \log(p(\w)) \} \\ 
				&\equiv \argmin_\w \left\{ -\sum_{i=1}^n [\log (p(y_i | \x_i, \w))] - \fr{\lambda}{2} \lVert  \w \rVert^2 \right\}
				\end{align}

				Given that $p(y_i | \x_i, \w) \propto \exp(-\fr{(\w \cdot \x_i - y_i)^2}{2}$, and $p(w_j) \propto \exp(-\fr{\lambda}{2}) w_j^2)$, putting this all together MAP is equivalent to minimising
				$$
				\cal{L}(\w) = \frac{1}{2} \lVert \X\w - \y \rVert^2 + \fr{\lambda}{2} \lVert \w \rVert^2 
				$$ 
				Where the first part is MLE, and the second part is the prior. Thus MLE is just non-regularised L2 loss. 

				This assumes a **gaussian** distribution ($\mu = 0, \sigma = 1$).
			</div>
			<p></p>
		</div>

		<div class="colourband">
			<h2 id="recom">Recommender Systems</h2>
		</div>

		<div class="cbox">

			<div class="md-conv">

				**Recommender Systems** are systems that suggest things to users that are liely to interest them. User's interests are predicted as *ratings*, based on their previous interactions.

				There are two main approaches:

				* Content-based learning (supervised)
				* Collaborative Filtering (unsupervised)
				
				### Collaborative Filtering

				We have an item/user matrix \(\Y\), with n rows of *users* and d columns of *items* (let's call them movies). Each entry is then a specific user's rating for a specific movie.

				This matrix is **sparse**: it has many blank entries. Thus, a row of \(\Y\) may look like `[? 2 3 ? ? 1 ?]` (numbers are ratings).

				The idea is to complete the matrix, based on similarities to other users. 

				We can use a *latent space*, $\Y = \Z\W$. Each **row** of $\Z, \z_i$ are the latent features of user \(i\). Each **column** of $\W, \w^j$ are the latent features of a movie \(j\). Thus, a single entry \(y_{ij} = \w^j \cdot \z_i\). 
				
				By encoding data into a latent space, we can then sample values from the space that we never had in the original dataset, and *decode* this into a predicted rating.
				
			</div>

			<div class="blue md-conv">
				We can use L2-loss PCA, for **all available** ratings R: 

				$$ 
				\cal{L}(\W, \Z) = \sum_{\forall i, j \in R}\left[ \w^j \cdot \z_i - y_{ij} \right] + \fr{\lambda_1}{2} \lVert \Z \rVert_F^2 + \fr{\lambda_2}{2}\lVert \W\rVert^2_F
				$$

				Note we now need two lambdas. (These are Hyperparameters)
			</div>

			<div class="md-conv">
				This model only uses the information we currently have, skipping over blanks, but the two matrices it constructs can be reconstructed into a full matrix, thus naturally filling in the blanks. 

				An individual cell is given as $\hat{y}_{ij} = \w^j \cdot \z_i$.

				However, we must consider that we must consider that some users rate lower or higher on average, and so we need to add all appropriate biases to our prediction:

				$$ 
				\hat{y}_{ij} = \w^j \cdot \z_i + \beta + \beta_i + \beta_j
				$$ 

				Where:
				* $\beta$ is the **global** bias 
				* $\beta_i$ is the **user-specific** bias
				* $\beta_j$ is the **item-specific** bias

				Collaborative filtering predicts the rating of each item for each user in the system -- it *cannot* predict new users.

				### Content-Based Learning

				Content-based learning uses a simple linear model, with the two matrices \(\X, \Y\). 

				$\X$ contains **rows** $\x_ij$, which are the <b>joint features</b> of user i and item j. \(y_{ij}\) is of course the rating. 

				Simply predict \(\hat{y}_{ij} = \w \cdot \x_{ij}\) with a (global) set of weights.

				This is useful for predicting ratings for *new users/items*, but cannot learn features. 

				### Hybrid System 

				Both systems have their uses and their lack of uses, and so would it be not good to combine them? 

				Well, the good thing is we can:
			</div>

			<div class="blue md-conv">
				Use the **hybrid prediction**

				$$ 
				\hat{y}_{ij} = \w^j \cdot \z_i + \beta + \beta_i + \beta_j + \w \cdot \x_{ij}
				$$

				Where 
				* $\w^j \cdot \z_i + \beta + \beta_i + \beta_j$ is the collaborative filtering
				* $\w \cdot \x_{ij}$ is the linear regression, the same $\beta$ as before can be used as this bias
				* $\beta_i + \beta_j$ are the specific item biases
				* $\w, \w^j$ are **different biases**. The \(\w^j\) is part of the bigger matrix \(\W\) for collaborative filtering, whilst small \(\w\) is the linear regression.
			</div>

			<div class="md-conv">
				This is solved using **stochastic gradient descent**.

				### Gradient Descent

				Given a loss function, want to find a *minimiser* $\vec{w}^*$. E.g. for least squares

				$$
				\argmin_{\vec{w} \in \mathbb{R}^d} \frac{1}{2} \sum\limits_{i=1}^n (\vec{w \cdot x}_i - y_i)
				$$

				Least squares is convex, but often functions are non-convex -- has multiple local minima. In this case, it suffices to find a local minimum (since it's hard to guarantee a global minimum).
				
				Gradient descent: iterative optimisation
				* Start with random solution $\vec{w}^0$
				* Use gradient $\nabla f(\vec{w}^0)$ to refine the solution into $\vec{w}^1$
				* ...
				* As $t \longrightarrow \infty$, $\nabla f(\vec{w}^t) \longrightarrow 0$

				Gradient descent converges to global minimum in case of convex, and some local minimum stationary point in non-convex. 

				Given a $\vec{w}^0, \nabla f(\vec{w}^0)$, we get more information about how to get closer to the minium value. The gradient "points up" away from minimiser, so we make a step in the direction of the negative gradient, thus we refine by doing $\newcommand{\w}{\vec{w}}$
				$$
				\w^1 = \w^0 - \nabla f (\w^0)
				$$
				This performs a "step". As we get nearer to the minimum, the gradient gets shallower, so our steps naturally get smaller.$\renewcommand{\epsilon}{\varepsilon}$
				
				We can alter our step size:
				* Start with solution $\w^0$
				* Use gradient $\nabla f(\w^0)$ to find $\w^1$, stepping with a (possibly decaying) *learning rate* $\alpha$:
				$$
				\w^{t+1} = \w^t - \alpha^t \nabla f(\w^t)
				$$
				* Repeat, and stop iff no progress is made, or if $|\nabla f(\w^t)| \leq \epsilon$ for some small value $\epsilon$.
				
				For least squares, using grad-desc is less complex than solving it straight up.
				<!-- ($\cal{O}(nd^2 + d^3$), the gradient descent iteration complexity is $\cal{O}(nd)$. -->

				### Stochastic Gradient Descent

				Grad Descent is complex and still long for large datasets, so we use SGD, which uses the gradient of a **random (selection of) sample(s)**:
				$$
				\w^{t+1} = \w^t - \alpha^t \nabla f_i(\w^t)
				$$
				Where
				$$
				\nabla f_i(\w^t) = \vec{x}_i (\w \cdot \vec{x}_i - y_i)
				$$
				A *single* sample. 
				
				Complexity is now independent of $n$, but suited to loss funcs which require summing over all $n$. 
				
				* Start with random soln.
				* Use gradient of a random sample $i$ to refine the solution, with a learning rate.

				**Disadvantaages:**
				* We use the gradient of a random sample - that sample's function might not be equal to objective function and even may point in the *wrong* way. 
				* But *on average* SGD is expected to move $\w$ in the right direction, over a large set of samples.
				$\newcommand{\x}{\vec{x}} \newcommand{\y}{\vec{y}} \newcommand{\X}{\vec{X}} \newcommand{\tr}{^\top}$
				Whilst each sample will still define a function of the same *shape* (naturally, since they're all MSE), they will be shifted in position from the actual objective function. Thus you get a random path that's not always going the right way (stochastic).
				
				But because of the shifting, far away from $\w^\ast$ every graident points towards the right direction. However in the middle there's a no-man's land (*challenging region*) where we can't guarantee movement in the right direction. 
				<figure>
					<img src="https://blog.paperspace.com/content/images/2018/05/fastlr.png" alt="" style="max-width: 400px;">
					<figcaption>
						Challenigng Region: Where stochastic gradient descent starts flip flopping around. <a href="https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/">[src]</a>
					</figcaption>
				</figure>
				
				We want to minimise the challenging region.

				The challenging region very related to **variance** of gradients, it can be detected by
				$$
				\frac{1}{n}\sum\limits_{i=1}^n \lVert \nabla f_i (\w) - \nabla f (\w^t) \rVert^2
				$$
				* If the variance is 0, we are always going in the right direction
				* If variance $\approx$ 0, we are just in the challenging boundary
				* If variance $\gg$ 0, then we're in the challenging region, and probably close to the minimum.

				The *learning rate* is associated to size of challenging region, that is, $\alpha \propto$ challenging region. Large $\alpha$ makes fast convergence, but a larger stochastic challenging region. The opposite is true. 

				Thus use **learning rate decay**. Decreasing too quickly -> not fast enough to reach ball, decreasing too slowly -> more stochasticity. 
				
				SGD converges to local minimum if
				$$
				\frac{\sum\limits_{t=1}^\infty (\alpha^t)^2}{\sum\limits_{t=1}^\infty \alpha^t} \approx 0
				$$
				Calculating the ratio of alpha squared to alpha "effect of variance to how much we can move"
				
				Commonly we define $\alpha = \cal{O}(\frac{1}{t})$. However in practice this tends to make alpha decay too quickly, so better $\alpha^t = \frac{a}{\sqrt{t}} : \alpha \ll 1$. But this is all trial and error.

				**For recomender systems**, use gradient descent with objective function, update based on random $i,j$ and LR $\alpha$, where $r_{ij}$ is the *residual error* $\hat{y}_{ij} - y_{ij}$.
			</div>
		</div>

		<div class="colourband">
			<h2 id="nns">Neural Networks</h2>
		</div>

		<div class="cbox">
			<h2>Contents</h2>

			<ol>
				<li><a href="#nn1">Neural Networks</a></li>
				<li><a href="#nn2">Autoencoders</a></li>
			</ol>

			<h2 id="nn1">Neural Networks</h2>

			<div class="md-conv">

				Neural Networks (NNs) can learn important features as well as labels. 

				They are based on linear regression -- specifically, the *perceptron* can be thought of as the most basic neural network -- NNs are just stacked perceptrons, fundamentally. 

				They also take concepts from PCAs, because we need to extract features and then use the extracted features.

				<figure>
					<img src="http://nicolamanzini.com/wp-content/uploads/2017/11/single_hidden_layer.jpg" alt="Neural Network" style="max-width: 400px;">
					<figcaption>A neural network with one "hidden layer"</figcaption>
				</figure>

				The input layer is our $\x_i$, our set of input data. For the case of images, these might just be the pixel values, flattened into 1D.

				The middle *(hidden)* layer is our extracted features $\z_i$, and we get from $\x_i \longrightarrow \z_i$ via a set of weights \(\W\): $$\underset{(a \times 1)}{\z_i} = \underset{(a \times d)}{\W} \underset{(d \times 1)}{\x_i}$$

				The dimensions are shown <span class="grey">even if the W is now wonky</span>.

				We can get our \(y_i\) (output) from the middle layer by doing a linear regression \(y_i = \vec{v}\t \z_i\), with a new set of weights $\vec{v}$.

				These basic neural netowrks are called **multi-layer perceptrons** (MLPs)

				The fundamental idea is, a NN can approximate **any** function, given enough complexity.

				#### Linearity

				Note that the x to z part, the "PCA" part, are really just several overlapping perceptrons. Thus, it is a linear PCA. 

				However, a linear function into a linear function makes a linear function: $y_i = \vec{v}\t \W \x_i \equiv y_i = \vec{A} \x_i$, and so what we've done is just a linear regression with extra steps.

				Thus, the crux of a neural netowrk is to **introduce non-linearity**:

				* At the end of every layer, include a non-linear **activation function** $h(\cdot)$, which transforms the data such that you cannot reduce the NN to a linear regression.
			</div>

			<p class="side md-conv">
				One of these is **sigmoid**:

				$$ h(x) = \fr{1}{1 + e^{-x}} $$

				Which maps $x \longrightarrow [0, 1]$
			</p>

			<div class="md-conv">
				So now, after every layer (except the input) we add an activation function layer, so now $\x_i \longrightarrow \z_i \longrightarrow h(\z_i) \longrightarrow \hat{y}_i \longrightarrow h(\hat{y}_i)$. In diagrams, these activation functions are often ommitted for clarity. 
			</div>

			<p class="side md-conv">
				An activation function that may be used for \(y_i\), for classification could be the **step** function:

				$$ h(x) = \begin{cases} 1 & x > 0 \\ -1 & x  < 0 \end{cases} $$ 
			</p>

			<div class="md-conv">
				One more thing is that PCAs will enforce orthogonal and normalised features -- we do no such thing for NNs. 

				The idea is to let them work it out on their own. 

				NNs use the same losses as everything else -- MSE, as an example.
			</div>

			<div class="md-conv">
				### Classification

				NNs can be thought of like multi-class logistic regression classifiers. 
			</div>

			<div class="side md-conv">
				A commonly used classification loss is **categorical cross-entropy**:

				$$ \cal{L} = \fr{1}{n} \sum_{i=1}^n \sum_{c=1}^C (y_{ic} \log (y_{ic})) $$ 

				Where big C is the number of classes (one hot encoding). This is also called log-loss. 
			</div>

			<div class="md-conv">
				Categorical Cross-Entropy emphasises the loss of the correct class only.
				
				Usually, outputs use the activation function **softmax**, to convert its preditions to probabilities. 
			</div>
			
			<div class="md-conv">
				### Training

				The full equation of a two hidden-layer network is like follows: $\yhat_i = \v\t h_2 ( \W^{(2)} h_1 ( \W^{(1)} \x_i))$. $h_1, h_2$ are activation functions, and can be the same, or different. We will treat them as the same. 

				There are now two sets of hidden layer weights, and one final regression weight. 

				A **bias** is usually implicit, and can be added to only the output, or often to every single layer.

				Graphically, we can represent this bias as being the weight of a new node on that layer which is always 1, just like a perceptron.

				<hr>

				NNs are trained with **stochastic gradient descent**. 

				For one hidden layer, an MSE loss is given as 
				$$ \cal{L}(\W, \v) = \fr{1}{n} \sum_{i=1}^n (\v\t h(\W\t \x_i) - y_i)^2 $$

				The gradient of a random sample is calculated, and is propagated back through the network and incrementally update weights -- **backpropagation**.

				Rather than using one sample, we can take a random subset of samples, a **minibatch**.

				$$ \w^{t+1} = \w^t - \alpha^t \fr{1}{|B^t|} \sum_{i \in B^t} \left(\nabla \cal{L}_i (\w^t) \right) $$

				The challenging region of SGD is *inversely proportional* to batch size. 

				Note on terminology:
				* **Epoch**: a full pass through the training set.
				* **Iteration**: a single SGD update (i.e. batch number)

			</div>

			<button class="collapsible">Backpropogation Calculus (In Detail)...</button>
			<div class="ccontent">
				<div class="md-conv">
					I'll get to it sometime.
				</div>
			</div>

			<div class="md-conv">
				Thus,
				* **Forward propagate** features, to get predictions and loss
				* **Backward propagagte** losses, to update weights

				Parameter initialisation is also important:
				* If the starting values are too large, learning takes a long time
				* Thus commonly use random, small weights
				* The bias is often initialised to 0

				In SGD, the step size (learning rate) is also very sensitive. 
				* Decreasing step size decreases stochastic effect 
				* LR can be manually monitored and decreased, or put on a scheduler to slowly decay

				***Momentum.*** In better optimisers like Adam, something called momentum is used. 
				* Think of a heavy ball rolling down a hill. If it encounters a small bump, it has enough momentum to roll over. 
				* This is what momentum does in SGD, adding "intertia" to the descent.
				* The "ball" "accelerates" and "rolls" around.

				We add a beta \(\beta_t \in [0, 1]\) for momentum, making 
				$$ \w^{t+1} - \alpha^t \nabla\cal{L}_i (\w^t) + \beta^t (\w^t - \w^{t-1}) $$

				***Vanishing Gradient Problem.*** 
				* Given that we are using sigmoid as our activation function, more layers of network stacks on more sigmoid functions
				* Since sigmoids have flat gradients far away, we can get an issue where the gradient "vanishes" to zero. 

				This issue is also true with other activation functions for large enough networks, but for our case, really, no one ever uses sigmoid. 

				
			</div>

			<div class="side md-conv">
				Rather, the most common activation function used is **ReLU: Rectified Linear Unit**. 

				$$ h(x) = \max(0, x) $$
			</div>

			<div class="md-conv">
				ReLU provides sparser activations, and also happens to be differentiable (which is kinda needed)

				**tanh** is also a function, though it is rarely used.

				NNs also have the fundamental tradeoff of more complex -> more accuracy -> more chance to overfit. 

				We can also still use regularisations for neural networks, for all the various weights.
			</div>

			<h2 id="nn2">Autoencoders</h2>

			<div class="md-conv">
				Autoencoders are machine learning models that learns a mapping to and reconstruction from a linear space, much like a PCA. 

				It consists of an **encoder**, a **decoder**, and a hidden **latent space** layer between them. Often, this forms a funnel shape.

				<figure>
					<img src="https://miro.medium.com/max/1200/1*oUbsOnYKX5DEpMOK3pH_lg.png" alt="" style="max-width: 400px;">
					<figcaption>
						A diagram of an autoencoder, where the encoder and decoder parts have 
					</figcaption>
				</figure>

				
			</div>
		</div>

		<footer>
			<div class="cbox">
                <div class="columncontainer ctwo" id="fc2">
                </div>
                <script type="text/javascript" src="../../js/footerGen.js"></script>
            </div>
		</footer>

	</div>

	<script type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script type="text/javascript" src="../../js/prism.js"></script>
	<script type="text/javascript" src="../../js/markdown.js"></script>
</body>
</html>