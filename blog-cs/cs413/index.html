<!DOCTYPE html>
<html>
<head>
	<title>CS413</title>
	<meta charset="utf-8">
	<link rel="stylesheet" type="text/css" href="../../style/style.css" media="all">  <!--TODO: CHANGE HREF-->
	<link rel="stylesheet" type="text/css" href="../../style/prism.css" media="all">
	<meta name="viewport" content="width=device-width" initial-scale=1.0>  <!--TODO: CHANGE LINKS ON BOTTOM OF SHEET FOR COLLAPSIBLE-->
	<link rel="icon" type="image/png" href="../../style/images/DragonIcon.png">
	
	
	<script async id="MathJax-script" type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
	<script defer type="text/javascript" src="../../js/toggle-darklight.js"></script>
	<script defer type="text/javascript" src="../../js/collapsible.js"></script>  <!--This stays at the end-->
	<script defer type="text/javascript" src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
	<script defer type="text/javascript" src="../../js/arrange.js"></script>
	<script defer type="text/javascript" src="../../js/prism.js"></script>
</head>
<body>

<div class="hidden">
	<header>
		<div class="parallax parsmaller">
			<div  style="display: grid; grid-template-columns: 1fr 1fr 1fr 7fr 1fr 1fr; grid-column-gap: 10px; padding: 5px; ">
				<div class="column tinycolumn">
					<a href="../../" class="nav">Home</a>
				</div>
				<div class="column tinycolumn">
					<a href="../../blog.html" class="nav">Blog</a>
				</div>
				<div class="column tinycolumn">
					
					<a href="../../about.html" class="nav">About</a>
				</div>
				<div></div>
				<div class="column tinycolumn">
					<a href="https://ko-fi.com/yijunhu" class="nav">Tip me</a>
				</div>
				<div class="column">
					<button class="nav dark-light">Dark Mode</button>
				</div>
			</div>
			<div class="cbox"> 		
				<h1>CS413</h1>
				<p class="subheading">Image and Video Analysis</p>
			</div>
		</div>
	</header>

<header>
	<div class="cbox">
		<h1>Contents</h1>
	</div>
</header>
<!-- REMEMBER TO DO! -->

<div class="cbox">
<div class="md-conv">

1. [Perception](#perce)
2. [Sampling and Quantisation](#sampling)
1. [Computation Over Images](#comp)
1. [Transforms](#transform)
1. [Feature Detection and Matching](#fd)
1. [View Geometry](#view)
1. [Machine Learning for Computer Vision](#ml)

$$
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\vec}{\mathbf}
\renewcommand{\lra}{\longrightarrow}
\newcommand{\x}{\vec{x}}
\newcommand{\y}{\vec{y}}
\newcommand{\p}{\vec{p}}
\newcommand{\q}{\vec{q}}
\newcommand{\dd}{\rm{d}}
\newcommand{\t}{^\top}
\newcommand{\rm}{\textrm}
\newcommand{\partd}[2]{\frac{\partial #1 }{\partial #2 }}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\argmax}{argmax}
$$

</div>
</div>


<div class="colourband">
	<h2 id="perce">Perception</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Introduction 

Perception processes are constant and unconscious. Perception informs action, and action gives stimulus to perception.

- Receptors *transduce stimuli* to electric signals 
- Nerves carry signals to the brain 
- Signals are processed and action given 

This module again goes over neural structure. 

- **Afferent** nerves carry signal from sensory neurons to the CNS (central nervous system)
- **Efferent** nerves .. from CNS to motor neurons.

Neurons can be exhibitory and inhibitory, strength given by *firing rate*, which is limited by refractory period.

The grey matter in the brain does all the processing.
- **occipital** -&gt; vision 
- **parietal** -&gt; touch
- **temporal** -&gt; hearing, taste, smell 

### Optics 

Light goes through the pupil, refracted onto the **fovea** of the retina. 
 
Photo receptors are at the *back* of the retina, so light goes through like this:

> Cones and rods &lt;- &lt;- bipolar cells &lt;- &lt;- ganglian cells &lt;- &lt;- *light* 

The **ganglian cells** transfer data through optical nerve.

### Light 

Comes in photons. Visible light is around 300 - 800 nm.

Light can be transmitted, absorbed, or reflected.

**Ambient optical array** is the pattern of light reaching the eyes. Viewpoint changes define optical flow (motion) which is used to determine distance, etc.

Objects are *projected* onto a 2D surface to form an image. Its projected size is proportional to distance. Depth information is *lost* with one sensor (can only use contextual clues) -- hence we have 2 eyes. 

A stereogram camera works the same way.

### Visual Pathway

Data is transmitted from the optic nerves through the **lateral geniculate nucleus (LGN)** in the thalamus, and then gets sent to the **primary visual cortex (PVC)** on the **opposite** half of the brain (and upside down)

- LGN performs initial mapping -- retains a *retinoscopic map* (the position data)
- PVC has *hypercolumns* -- each column deals with some part of the map.

In the eye, the *fixed* cornea does 80% of the focusing, whilst the *flexible* lens does 20%. The iris changes size to allow different amounts of light in.

The *fovea* being on the optical axis has the best colour and spatial *acuity* (&asymp; resolution).

The *periphery* is less dense, and mostly rods (light) rather than cones (colour). This makes it monochrome. There are *no* cones in the fovea itself, and simply nothing in the blind spot. Your brain inpaints this.

<figure style="text-align: center;">
	<img src="https://upload.wikimedia.org/wikipedia/commons/1/1f/Distribution_of_Cones_and_Rods_on_Human_Retina.png" alt="File:Distribution of Cones and Rods on Human Retina.png - Wikipedia" style="max-width: 400px;">
	<figcaption>Density of cones and rods (Wikipedia)</figcaption>
</figure>
<figure style="text-align: center;">
	<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/1416_Color_Sensitivity.svg/300px-1416_Color_Sensitivity.svg.png" alt="Photoreceptor cell - Wikipedia">
	<figcaption>Cone/Rod wavelength sensitivity (Wikipedia)</figcaption>
</figure>

Cones have less sensitivity to light but reach max sensitivity faster than rods. 

<img src="https://www.cns.nyu.edu/~david/courses/perception/lecturenotes/ganglion/rgc-slides/Slide4.jpg" alt="Perception Lecture Notes: Retinal Ganglion Cells" style="float: right; max-width: 300px;">

The **receptive field** is the area which influences the firing of a ganglian cell. 

It comes in a centre-surround shape, being either on centre off surround, or the opposite (right)

Different ganglians have overlapping receptive fields -- this effectively is an **edge detection** convolution, but **rotation invariant**.

They fire more strongly to lines than to intersections -- hence the black in the corners of a grid of squares illusion. 

Ganglian -> Optic Nerve -> LGN -- visual fields put together again. LGN has a *bilateral layer structure*, which overlaps images from two eyes for stereoscopic projection. If you think about it very hard, it's literally like a CNN. Granted, CNNs didn't get invented out of thin air.

### Visual Cortex 

Cells in the VC respond **selectively** to features 
- **simple** cells respond to features at different orientations 
- **complex cells** non-linearly respond to orientation and motion 
- **end-stopped** cells respond to moving lines and corners

This is a functional separation -- splitting into form, colour, and motion.

</div>
</div>

<div class="colourband">
	<h2 id="sampling">Sampling and Quantisation</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Introduction 

Sensors acquire data from either visible or non-visible "modalities", usually something spatially from the world. 

Image data can be different dimensions: 2D, 3D, or 2D + t (time), 2D + d (depth), etc.

***Pixels.*** represent real world dimensions. We require calibration to use imaging devices to measure the real world. This is often done with a known reference, like a checkerboard. 

***Data Rates.*** are proportional to data size. Measured in bits per second (bps). Compression is done to data that is too big.

***Sampling Challenges.*** 
- Analogue data is acquired via sensor and **quantised** to binary. 
- Data can be very big and need compression; very noisy and need denoising.
- Must be calibrated to be used in image processing.

### Images

Images are matrices of pixels or voxels. Mathematically images can be thought of as functions:
- The idealised, **continuous** image $f : \R^2 \lra \R$ where $f(x,y)$ gets the value
- The actual discrete image $f: \N^2 \lra \N$ where $f[i,j]$ 
- Multi channel images, e.g. $f: \N^2 \lra \N^3$ etc.

The domain is sampled (gets resolution), the range is quantised (gets colour depth)

***Spatial frequency.*** in Hz, is "cycles per pixel", since image data has changes in amplitude. It's like, noise.

Subsampling is reducing the resolution -- sampling at below 2&times; the max frequency loses information (Nyquist's theorem) -- *subsampling artefacts* 

**Anti-aliasing** is applying a low pass filter to remove noise (high frequencies).

**Uniform quantisation** is truncating the least significant bits. This can produce "false contours" in low frequency areas.

***Colour Spaces.***
- Perception uses RGB -- so RGB colour spaces. 
- Can also linear transform colour spaces, e.g. into **YUV** or **YCbCr** spaces.
- We are more sensitive to value/luminance than colour, so downsampling CbCr leads to less perceptual loss.


### Interpolation

**Interpolation** is the opposite of subsampling. It makes data from nowhere. 

**Nearest neigbhour** interpolation is basically: if we upscale by factor $n$ repeat each pixel $n$ times. Leads to pixelisation which may or may not be desirable.

**Linear interpolation** models distance between pixel values as lines. Suppose pixel $x_1$ has value $a$ and $x_2$ has value $b$, for an unknown point $x_1 \lt x' \lt x_2$ fraction $d$ between the points, we get 
$$
f(x') = a + d(b-a) = db + (1-d)a
$$

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/9/91/Bilinear_interpolation_visualisation.svg/1200px-Bilinear_interpolation_visualisation.svg.png" alt="File:Bilinear interpolation visualisation.svg - Wikipedia" style="float: right; max-width: 240px;">

**Bilinear interpolation** 

For some continuous $x, y$ between four points, interpolate horizontally then interpolate vertically. Expressed in matrix form this is
$$
f(x,y) = \begin{bmatrix}
	1 - \Delta x & \Delta x 
\end{bmatrix} \begin{bmatrix}
	f[i, j] & f[i, j+1] \\\\ f[i+1, j]& f[i+1, j+1] 
\end{bmatrix} \begin{bmatrix}
	1 - \Delta y \\\\ \Delta y
\end{bmatrix}
$$
Where 
\begin{align}
	\Delta x &= x - i \\\\ 
	\Delta y &= y - j
\end{align}

Everything is weighted combinations. There's also (bi)quadratic, bicubic, etc. 


**Sinc interpolation** takes a weighted sum of infinite set of neighbours based on the $\frac{ \sin x }{ x }$ function. This is the "idealised" interpolation.

Naturally infinity is kinda difficult for computers, so *Lanczos* since modulates it so it goes to 0 after $n$ things. $n$ is basically the radius, so samples taken is $O(n^2)$

**AI Inpainting** is *technically* just a form of interpolation, except you smash a whole heap of carefully adjusted matrices at it so that it comes up with something convincing.

### Domain Measurements 

Many different distance metrics, like euclidean $d_e (\p, \q) = \lVert \p - \q \rVert$, manhattan $d_4(\p, \q) = |p_x - q_x| + |p_y - q_y|$, chess board/L0 $d_8 = \max|\p - \q|$. 

***Connected Components.***
-  4 neighbours are the orthogonal pixels, 8 includes the diagonals. 
- Adjacent pixels with the same colour are the same connected component 
- Partition pixels by binarisation -- label colours into classes. 
- Find connected components by visiting pixels -- after one pixel is visited, recursively add its like-neighbours to the same component, and mark all of them as visited. Repeat until no unvisited pixels.

The **centroid** of a CC is a vector of element-wise **average** of all cell coords. The centroid position is:
$$
\vec m = \frac1N \sum_{k=1}^N \p_k
$$
Covariance is:
\begin{align}
	\vec I  &= \sum_{k=1}^N (\p_k - \vec m) (\p_k - \vec m)\t \\\\
	&= \begin{bmatrix}
		a & b \\\\ b & c
	\end{bmatrix} : \theta = \tan^{-1} \left( \frac{ b }{ a-c } \right)
\end{align}

***Distance transforms.*** Take any undivided component, we can calculate the distance of every pixel to its nearest boundary. 

We can use this to value the pixels and **skeletonise** components. 

***Chain codes.*** Compact encoding of edge direction. Walk around the boundary, and record the direction to the next pixel (0 to 7). This can be stored very compactly, and it is easy to do rotations (add 1 mod 8).

***Run Length Encoding.*** For only 2 states, RLE only needs to say the starting state.

> These are both used in H.264 compression 

### Range Measurements 

Model the image as a random variable sampled from an unknown distribution $\Phi$. 
$$
F = \\{f_0, f_1, .. f_{N-1}\\} : f_i \sim \Phi_f 
$$
The mean is the expected value 
$$
\E[f] = \frac1N \sum_i f_i = \mu
$$
The variance is the average square difference 
$$
\var[f] = \frac1N \sum_i (f_i - \mu)^2 = \sigma^2
$$


***Calculating the distribution.*** Can get a histogram of pixel values $\sim$ estimate of distribution. The posterior probability $h[k]$ (number of pixels of colour $k$) where 
$$
p(k|f) = \frac{ h[k] }{ N } \approx \Phi_f
$$
This is location invariant -- sometimes helpful, sometimes not.


***Optimal Thresholding via MAP decision value.*** Noise changes histogram, can inspect histogram in thresholding to minimise mis-classification. Let's suppose noise is gaussian.

$$
\rm{label} = \begin{cases} 1 & p(1 | f_i) \gt p(0 | f_i) \\\\ 0 & \rm{otherwise} \end{cases} 
$$

The posterior probability ($c$ classes) is likelihood &times; prior
$$
p(c | f) = \frac{ p(f|c) p(c) }{ p(f) } \propto p(f | c) p(c)
$$

We can set $p(1)$ to be the proportion of foreground and $p(0)$ to the proportion of background. Since we have gaussian noise, $p(f|1) = N(192, \sigma)$ (192 foreground) and $p(f| 0) = N(64, \sigma)$ (64 background). Then simply solve for 
$$
p(f|1)p(1) = p(f|0)p(0)
$$
to get the threshold.



</div>
<button class="collapsible">Example... </button>
<div class="ccontent md-conv">

***Example.*** A 512 by 512 im has 256 colours. The foreground is at 150, the background is at 80. The foreground takes up $\frac14$, the background takes up $\frac34$. Find the threshold.

Let background be B and foreground F, a pixel $x$.

$$
\begin{matrix}
p(B|x) \propto p(x|B) p(B) & p(F|x) \propto p(x|F)p(F)
\end{matrix}
$$

The original gaussian is $e^{-x^2}$. Adding variance: $e^{-\frac{ x^2 }{ 2\sigma^2 }}$ and taking away the mean: $e^{-\frac{ (x-\mu)^2 }{ 2\sigma^2 }}$, then normalising to 1:
$$
\frac{ 1 }{ \sigma \sqrt{2\pi} } e^{-\frac{ (x - \mu)^2 }{ 2 \sigma^2 }}
$$
We get given prior probs, assume noise is normally distributed with scaling factor $k$ and $\sigma = 20$:
$$
p(B) = \frac 1K e^{-\frac{ (x - 80)^2 }{ 2 \times 20^2}}
$$
Solve for the thing (ratio $\frac34$ to $\frac14$):
\begin{align}
	3 \exp \left( -\frac{ (x-80)^2 }{ 2 \times 400 } \right) &= \exp \left(-\frac{ (x - 150)^2 }{ 2 \times 400} \right ) \\\\
	-\frac{ (x - 80)^2 }{ 800 } + \ln 3 &= -\frac{ (x - 150)^2 }{ 800 } \\\\
	x &\approx 121.278
\end{align}


</div>

<div class="md-conv">

We still get some mis-classification since pixels are treated independently. Post-process to clean up. 

This model however supposes 2 peak additive gaussian noise -- not usually the case. 

***Entropy.*** Information (shannon) entropy -- the information content of some image:
$$
H = -\sum_k p(k) \log_2 p(k)
$$
For the probability of each colour $k$. This is estimable from the histogram, and is measured in **bits**.

**Redundancy** is the quantisation bits (bits left over) -- so 8 bit image with 6.2 bits of entropy makes 1.8 bits redundancy. The entropy is the lower bound for the number of bits required to store an image.

***Distortion.*** Can use SSD of original image vs new image to get distortion differences. (Also called RSS)
$$
SSD(F, G) = \frac 1 N \sum_i (f_i - g_i)^2
$$

The signal to noise ratio **SNR** is
$$
SNR = \frac{ \var(F) }{ SSD(F, G) }
$$
And is often measured in *decibels* in $\log_{10}$ form: $SNR\\; \rm{dB} = 10\log_{10}(SNR)$ 

</div>

</div>

<div class="colourband">
	<h2 id="comp">Computation Over Images</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Point/Pixel Level

A curve/transformation between input and output, via some mapping func, like 
- $T(x) = \log(1+x)$ 
- $T(x) = x^\gamma$ -- **gamma correction**
- $T(x) =$ histogram equalisation.

Histeq is done with the cumulative histogram $H_C(k) = \sum_{j=1}^k H(j)$. Let $H_C(I)$ be the ideal (diagonal) histogram.

If 255 is the max colour, the transform function $T_{HE}(x) = \frac{ 255 }{ N } H_C(x)$ where $N$ the number of pixels is the scaling factor. This can be generalised to any function.

### Local Level 

**Convolution** over a small area. We denote it as $\circledast$ here. 
$$ g(x) = \sum_{p = -M}^M h(p) f(x-p)$$

Padding on the boundary (or not).

The **gaussian smoothing function** is 
$$
g(x, y; \sigma) = \exp(-\frac{ x^2 + y^2 }{ 2\sigma^2}) \pod{\equiv g(x;\sigma) g(y;\sigma)}
$$
- Kernels are normalised after, and higher $\sigma$ makes more smoothing.

The **gradient operator** approximates derivates on certain directions 
$$
f'(x) = \lim_{\Delta \lra 0} \frac{ f(x + \Delta) - f(x) }{ \Delta} \approx f(x+1) - f(x) =: \Delta f(x)
$$
Alternatively, the **central difference** gradient (which is centred)
$$
\Delta_c f(x) = \frac{ f(x+1) - f(x-1) }{ 2}
$$

Convolution has a nice property:
$$
\frac{ \dd (f(x) \circledast h(x))}{ \dd x } = f(x) \circledast \frac{ \dd h(x) }{ \dd x}
$$

So we can estimate gradient of a convolved function by using the derivative of its kernel:
\begin{align}
	g(x; \sigma) &= \exp(- \frac{ x^2 }{ 2\sigma^2 })\\\\
	g'(x; \sigma) &= - \frac{ x }{ \sigma^2 }\exp(- \frac{ x^2 }{ 2\sigma^2 }) = \frac{ -x }{ \sigma^2 }g(x; \sigma) \\\\
	g''(x; \sigma) &= (\frac{ x^2 }{ \sigma^4 } - \frac{ 1 }{ \sigma^2 })g(x; \sigma).
\end{align}

The **laplacian** of a gaussian is the sum of its second derivatives:
$$
\nabla^2 g(x,y ; \sigma) = \partd{g(x,y; \sigma)}{x^2} + \partd{g(x,y; \sigma)}{y^2}
$$
But really we just approximate this using a **discrete kernel**. 

Both first and second derivative essentially do a form of edge detection. The laplacian is usually approximated using a difference of gaussians.


### Multiresolution and Block-Based Representation 

***Pyramids*** 

To an image, we can repeatedly apply blur -> subsample to form a "pyramid" of smaller and smaller images, ending with one pixel. A pixel at one level represents a block on a level below.
- Gaussian smooth 
- Subsample $n$ -> remove every $n$th pixel 

To get a $P(F, l)$ for image $F$ and level $l$ 

**<span class="sc">Reduce</span>:**
$$
P(f, l) \lra (\circledast g(x, y; \sigma)) \lra (\downarrow 2) \lra P(f, l+1)
$$

**<span class="sc">Expand</span>:** Use NN upsampling and **same smoothing function**
$$
\hat P(f, l) \longleftarrow (\circledast g(x, y; \sigma)) \longleftarrow (\uparrow 2) \longleftarrow P(f, l+1)
$$

Laplacian is approximated by differencing $P(f, l) - \hat P(f, l) =: DoG(l+1, l)$ (i.e. upsample the smaller layer)

Can do this to the entire pyramid to get a pyramid of differences. 

***Laplace pyramid based compression.*** laplacian image code:
- Build laplace pyramid 
- Keep top level gaussian 
- *threshold* laplacian 
- *quantise* laplacians -- more bits for lower frequencies (higher on pyramid) and less for higher frequencies.
- Use RLE

Can customise the compression ratio for compressions -- studying the distortion (using MSE) vs compression ratio is good for comparisons.


</div>
</div>

<div class="colourband">
	<h2 id="transform">Transforms</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Introduction 

Using DCT and DFT bases.

**Basis projection** is a way to analyse a signal/vector. Multiply input by **basis vector** (discrete projection) := 
$$
\langle f, g \rangle = f \cdot g = \sum_{i = 0}^{N-1} f(i)g(i)
$$

### Cosine Transform

On cosine funcs at different frequencies $u = 0,1,2..$
$$
g_1(x, u) = \alpha(u) \cos\left(\frac{ (2x+1)u\pi }{ 2N }\right)
$$
Where $\alpha(u)$ is a normalising factor. 

1D bases can be used to make 2D basis simply:
\begin{align}
	g(x, y ; u,v) &= g_1(x; u) g_1(y; v) \\\\
	F(u, v) &= \sum_{x=0}^{N-1} \sum_{y=0}^{N-1} f(x,y) g(x,y ; u,v)
\end{align}
And converted back by:
$$
f(x,y) = \sum_{u=0}^{N-1} \sum_{v=0}^{N-1} F(u,v) g(x,y; u,v)
$$

### Fourier Transform

Complex! (literally, like $\mathbb{C}$ complex). Fourier is done with complex numbers:

$$
f(t) = \frac{ 1 }{ 2\pi } \int_{-\infty}^{\infty} F(\omega) e^{i\omega t } \dd \omega 
\pod{re^{i\theta} = r\cos\theta + ri\sin\theta}
$$

Uses a frequency $\theta$ (Hz), which is angular frequency. 

The **Discrete Fourier Transform** is given in 1D (note the $-i$):
$$
F(u) = \frac 1N \sum_{x=0}^{N-1} f(x) e^{-i \frac{ 2\pi u x }{ N }}
$$
- For data samples $f(x) : 0 \leq x \leq N$ 
- Frequency $0 \leq u \leq N$ 
- exponent in radians $0 \leq \frac{ 2 \pi u x}{ N } \lt 2\pi$ 

The inverse FT
$$
f(x) = \frac 1N \sum_{u=0}^{N-1} F(u) e^{i \frac{ 2\pi u x }{ N }}
$$

- DCT is **not** shift invariant -- phase shift leads to leakiness (JPEG artefacts). 
- Since DFT also does the sine decomposition it gets the phase. 
	- FFT can misplace very small frequencies, so often one ignores them.

**2D DFT** is done separately per dimension. The complex domain is 
$$
F(u, v) = R(u, v) + iI(u, v) = |F(u,v)| e^{i \phi(u,v)} \pod{\phi(u,v) = \arctan\left(\frac{ I(u,v) }{ R(u,v) }\right)}
$$
And the power/magnitude spectrum is 
$$ P(u, v) = |F(u, v)|^2 $$ 


Generally filtering is done over magnitude.

<b-blue>

***Thm.*** Spatial convolution in the pixel domain &equiv; element-wise multiplication in fourier domain.

$$
h(x,y) \circledast f(x,y) = H(u,v).F(u,v)
$$

</b-blue>

Therefore operating over fourier space is quicker -- and we can even skip FFTing $h$ into $H$ and immediately make a filter for fourier space.

Certain operations -- like ideal low pass, can lead to artefacts. Thus for low-pass gaussian is used instead. 

FFT is efficient at $O(n \log n)$. 



</div>

</div>

<div class="colourband">
	<h2 id="video">Video</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Background Subtraction 

A video can be modelled as <strike>literally is</strike> a series of images: $F(t)$ is frame $t$.

One of the first steps in video analysis is to separate the foreground from the background.

This has several challenges due to illumination changes, shadows, camera shake, etc.

Methods include:
- Frame differencing 
- Running averages (learning the "steady state" of the background)
- Statistic model of colour distribution 
	- Gaussian model by multi-modal gaussian 
	- Likelihood test for temporal changes 

***Frame differencing.*** Threshold the difference of adjacent frames.
$$
D(t) = F(t) - F(t-1)
$$
$$
FG(x, y, t) = \begin{cases} 1 & \rm{if } |D(x,y,t)| \gt T \\\\ 0 & \rm{otherwise} \end{cases} 
$$

This is functionally a gradient over time -- gives **edges** of moving objects only.

***Running average.*** Via frame window or learning rule. This finds the background:
- Sum of all frames up to frame $k$
$$
B = \frac 1 K \sum_{t=1}^k F(k) 
$$
- Sum of all frames over a window $w$ 
$$
B_w = \frac 1 w \sum_{t=k-w}^k F(t) 
$$
- Using a learning rate $\alpha$:
$$
B_\alpha (t_1) = \alpha F(t) + (1-\alpha) B_\alpha(t)
$$

The RA needs time to stabilise, so that moving objects are erased. 

### Gaussian Modelling

***Gaussian model of background.*** Model:
$$
B(x,y) \sim N(\mu_{x,y}, \sigma_{x,y}) = \frac{ 1 }{ \sigma \sqrt{2\pi}} \exp\left(- \frac{ (B - \mu)^2 }{ 2 \sigma^2 }\right)
$$

With a mean and variance for each pixel $(x,y)$. 

To model each pixel over time : drawn from normal distribution, parametric with mean and variance. Learn by learning rule:
\begin{align}
	\mu(t+1) &= \alpha F(t) + (1-\alpha) \mu(t) \\\\
	\sigma^2 (t+1) &= \alpha(F(t) - \mu(t))^2 + (1-\alpha) \sigma^2(t) 
\end{align}
If variance is greater than some factor $|F(x,y,t) - \mu(x,y,t)| \gt K\sigma(x,y,t)$ then this region is significant. 

***Multiple Gaussians.*** If the image background has repetitive actions, it has multiple modes. We can use a mixure of gaussians (weighted sum) to model this:

$$
GMM(f, M) = \sum_{k=1}^M w_k N(\mu_k \sigma_k)
$$

For $M$ gaussians, which tends to be small, and $\sum_{\forall k} w_k = 1$.

Optimise this using *expectation maximisation*. We:
- Start off with some model parameters 
- Update parameters for the background
- Test if data is likely to be drawn from the PDF 

By, for $k$ modes:
- Decide which of $k$ modes a pixel belongs to via $(f(t) - \mu_k)^2 \lt 2.5\sigma_k^2$ (finding standard deviations away)
- Update if it is significant 
- Reduce the parameters of non-member parameters
- Normalise WeightedSetPacking

Its a bit wishy washy written like this but basically it's iterative adjustment of the multi-gaussian model, based on learning rules. We usually start with evenly spaced gaussians with large variances, then update like the following:
\begin{align}
	\mu(t+1) &= \alpha f(t) + (1-\alpha) \mu(t) \\\\
	\sigma^2(t+1) &= \alpha (f(t) - \mu(t))^2 + (1-\alpha(t)) \sigma^2(t) \\\\ 
	w(t+1) &= \alpha + (1-\alpha) w(t) \rm{ only if ---}
\end{align}
only if the component is in this mode, else $w(t+1) = (1-\alpha)w(t)$ only.

Classify a pixel into foreground and background by thresholding over all modes:
$$
B = \argmin_b (\sum_{k=1}^b w'_k \gt T)
$$
Where $B$ is the smallest subset with weighted weights $w'$ weighted by variance. The likelihood of foreground is 
$$
FG(x,y,t) = \begin{cases} 1 & GMM(f(x,y,t); B) \gt P \\\\
0 & \rm{otherwise} \end{cases} 
$$

$w'_k$ is taken in decreasing order, $w_k' := \frac{ w_k}{ \sigma }$.


### Optical Flow 

Estimating **local** motion in image sequence by looking for motion of features of interest.

We want to estimate motion featuers -- images themselves are not exact. Motion vectors enable object tracking.

The output to this is a vector field with $(u,v)$ vectors describing motion in each axis.

***LK Taylor Approximation Method.*** Assumes flow (motion) is *local* and *constant* over that region.

Uses consistency of colour
- Brightness consistency:
$$
\partd{I(x,y,t)}{t} = \partd{I}{x} \partd{x}{t} + \partd{I}{y} \partd{y}{t} + \partd{I}{t}
$$
Where $\partd{x}{t}, \partd{y}{t}$ are $u,v$ motion respectively.
- This is done with 1st order taylor approx:
$$
I(x + \Delta x, y + \Delta y, t + \Delta t) = I(x,y,t) + \partd{I}{x} \Delta x + \partd{I}{y} \Delta y + \partd{I}{t} \Delta t + \cdots 
$$
- Colours are constant, and time is 1, so we can say $I(x + \Delta x, y + \Delta y, t + \Delta t) = I(x,y,t)$
- To get the partials, we simply use convolution. For $\partd{I}{t}$ we can actually filter on the time dimension to get the ... temporal gradient?

Note that for $\Delta t = 1$ and 
$$
\partd{I}{x} \Delta x + \partd{I}{y} \Delta y + \partd{I}{t} \Delta t = 0
$$
We get 
$$
\partd{I}{x} \Delta x + \partd{I}{y} \Delta y = - \partd{I}{t} = \begin{bmatrix}
	\partd{I}{x} & \partd{I}{y}
\end{bmatrix} \begin{bmatrix}
	\partd{x}{t} \\\\ \partd{y}{t}
\end{bmatrix}
$$
We can estimate this using least squares estimation, since this resolves to a system of linear equations 
\begin{align}
	I_{x_1} u + I_{y_1} v &= -I_t \\\\
	&\vdots \\\\
	I_{x_n} u + I_{y_n} v &= -I_t 
\end{align}
$$
\implies A \begin{bmatrix}
	u \\\\ v 
\end{bmatrix} = \y \lra \begin{bmatrix}
	u \\\\ v 
\end{bmatrix} \approx (A\t A)^{-1}A\t y
$$

Where $(A\t A)^{-1}A\t$ is the **pseudo-inverse** of a non-square $A$.

This works since $A\t A$ is the hessian (below). The eigenvalues tell us of its properties.

If the region patch is too small, we may miss motion, but itf it's too large, can only get a general global motion.

The hessian gives us an indication of validity of motion -- same as with harris, we like to work with corners. 

Note that LK fails if 
- the preconditions don't hold, e.g. the colours change 
- motion is bigger than region size 
- object is deforming non-linearly 
- estimation window size is insufficient -> do a pyramid calculation.

</div>
</div>

<div class="colourband">
	<h2 id="fd">Feature Detection and Matching</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Features 

A way to recognise/distinguish objects. Traditional CV techniques involve engineering features.

***Low level features.*** Include simple characteristics like 
- edges and corners 
- high curvature ~ second derivative values 
- local features which are invariant (to rotation, scaling, etc...)

Good visual features are 
- distinctive
- pose invariant -- invariant as above  
- local 
- repeatable -- resistant to lighting, skew, etc.

Features have two parts: **location** or key point: $k = (x,y)$, and a **descriptor** of feature coefficients $f(x,y) = [f_1 .. f_m]$ 

### Harris Stephens Corner Detection 

- Key points at points of high curvature 
- Is local and pose invariant, robust to illumination
- Distinctive descriptors.

For image patch size $w$:
$$
E(u,v) = \sum_{(x,y) \in W} w(x,y) (I(x+u, y+v) - I(x,y))^2 
$$
The window function $w(\cdot, \cdot)$ times the SSD of the image patch shifted $(u,v)$ against itself.

Generates first and second derivatives. It has those properties since 
- on a constant region, there are no appreciable changes when patch is shifted 
- On an edge, we detect changes when patches are shifted orthogonal to the edge, but not along the edge 
- on a corner, any shift causes expression, so it picks these out. This also does not change if the image is rotated. 

Expanding image using taylor series:
$$
f(x+u, y+v) = f(x, y) + u \partd{f}{x} + v \partd{f}{y} + \cdots = f(x+y) + uF_x + vF_y
$$
We know how to work out these partial derivatives. Hence 
\begin{align}
	I(x+u, y+v) &= I(x,y) + uI_x + vI_y \\\\
	E(u,v) &\approx \sum_{(x,y) \in W} (I(x,y) + uI_x + vI_y - I(x,y))^2 \\\\
	&\cdots \rm{ omitted } \cdots \\\\
	&= \begin{bmatrix}
		u & v 
	\end{bmatrix}\left( \sum_{(x,y) \in w} \begin{bmatrix}
		I^2_x & I_x I_y \\\\ I_y I_x & I_y^2 
	\end{bmatrix} \right) \begin{bmatrix}
		u \\\\ v
	\end{bmatrix}
\end{align}
Which is the *hessian*. 

The **eigenvalues $\alpha, \beta$ of the hessian** captures the local region variation. 
- If they are small on both axes, then the patch is flat 
- If they are spread out on one axis, then the patch is an edge 
- If they are spread out on both axes, then the patch is a corner 
Hence the corner determiner is:
$$
R = \alpha \beta - k(\alpha + \beta)^2
$$
For some threshold $k$, changing the number of KPs detected.

### SIFT

Remember robotics? [I will just leave this here](../cs313/#v3). Gaussian filtered images with scale $k_i$ are denoted as $I_G(k_i \sigma)$.


### Feature Matching

Suppose two ims I has feature $i$ and J has feature $j$

***Nearest Neighbour.*** Finding the closest $i, j$s (euclidean distance):
$$d(i,j) = \lVert f_i - f_j \rVert^2$$
For high dims euclidean distance gets less useful -> use smth like cosine distance. Then get 
$$
\argmin_j d(i,j)
$$

We can require a maximum accept distance to remove outliers. 

Over a large number of images, the complexity can tend to $O(\rm{horrible})$.

***Scoring / Correspondence.***

- Learn matching using supervised learning. 
- We learn feature sets from a set of objects
- For an unknown image, we want to find the best set of inliers. 
- We score each set of inliers/matches by:
	- highest number of inliers 
	- sum of distances of first N inliers 
- An optimisation is to apply a set of geometric constraints 
	- e.g. between features sets, examining them relative to each other to deduce pose change 
	- trying to calculate pose from a subset of inliers. 
	- score this against all other inliers and minimise.

***Bag of words.*** 

- For all features, cluster and identify a subset of most likely features. These will be your feature prototypes -- the *words* 
- Images are described with these feature words.
- Going from high dimensional features to low dimensional bins. 

### SURF 

Improvement in SIFT, with SURF features matching rotated and transformed objects.

**Rectification** is finding a **projective transformation** between an image pair. 

This is done via **RANSAC** (random sample and correctness). Suppose there exists a transform $T : X \lra Y : T(\x) = \y$.
- Take a subset of points, and solve for $T$ based on them. We get an estimate $\hat T(\x) = \hat \y$. 
- Calculate a score $S_{\hat T} (\hat \y , \y)$
- Repeat with other subsets
- Plt $S$ against estimated $\hat T_n$ for every $n$, and look for a maximised score. That would be our best transformation.

We can use this to measure depth through disparity, or mosaicing images.

</div>
</div>

<div class="colourband">
	<h2 id="view">View Geometry</h2>
</div>

<div class="cbox">
<div class="md-conv">

### Cameras 

Camera geometry can be modelled as ain image plane. From a 3D world coordinate $P$ to a 2D projected coordinate $p$ -- **projection**.

A point $P(X, Y, Z) \lra p(x,y,f)$ for a focal length $f$:
$$
\begin{matrix}
	\frac{ x }{ f } = \frac{ X }{ Z } & \frac{ y }{ f } = \frac{ Y }{ Z }
\end{matrix}
$$
Usually we get homogenous coordinates (homogenise s.t. $f=1$):
$$
\begin{bmatrix}
	x \\\\ y \\\\ 1 
\end{bmatrix} = \frac{ f }{ Z } \begin{bmatrix}
	X \\\\ Y \\\\ \frac{ Z }{ f}
\end{bmatrix} \approx \begin{bmatrix}
	X \\\\ Y \\\\ \frac{ Z }{ f }
\end{bmatrix}
$$
Essentially a rotation-movement as a 3D space over $(x,y,1)$. We don't need the $\frac{ f }{ Z }$ multiplier since in homo-coords scaled versions work the same. There is no origin $(0,0,0)$ in this system.

Indeed we can represent this in matrix form with the perspective transform as 
$$
\begin{bmatrix}
	x \\\\ y \\\\ 1
\end{bmatrix} = \begin{bmatrix}
	f & 0 & 0 & 0 \\\\
	0 & f & 0 & 0 \\\\
	0 & 0 & 1 & 0
\end{bmatrix} \begin{bmatrix}
	X \\\\ Y \\\\ Z \\\\ 1
\end{bmatrix} \longleftrightarrow \begin{bmatrix}
	\x \\\\ 1
\end{bmatrix} = C \begin{bmatrix}
	\vec X \\\\ 1
\end{bmatrix}
$$
with an implied homogenisation step afterwards.

But this assumes camera at origin. We also then need to apply rotation $R$ and translation $T$ matrices.
$$
\begin{bmatrix}
	x_c \\\\ y_c \\\\ z_c
\end{bmatrix} = \underset{(3 \times 3)}{R} \begin{bmatrix}
	x \\\\ y \\\\ z
\end{bmatrix} + \underset{(3 \times 1)}{T}
$$
In homogenous coordinates:
$$
\begin{bmatrix}
	X_c & Y_c & Z_c & 1
\end{bmatrix}\t = \begin{bmatrix}
	\vec R & \vec T \\\\ \vec 0 \t & 1
\end{bmatrix} \begin{bmatrix}
	\vec X \\\\ 1
\end{bmatrix}
$$

***Intrinsic parameters*** to a camera are things like focal length, aspect ratio, skew, shift. This can all be represented in a projection matrix 
$$
K = \begin{bmatrix}
	f \cdot a & s & x_0 & 0 \\\\ 0 & f & y_0 & 0 \\\\ 0 & 0 & 1 & 0
\end{bmatrix}
$$
Where $a$ is the aspect ratio, $x_0, y_0$ is the center of the camera (if it's not centred on the axis), $s$ is the skew. 

A full camera has both extrinsic (rotation and movement) and intrinsic parameters, hence putting everything together 
$$
\begin{bmatrix}
	\x \\\\ 1 
\end{bmatrix} = \vec K \begin{bmatrix}
	\vec R & \vec T \\\\ \vec 0 \t & 1
\end{bmatrix} \begin{bmatrix}
	\vec X \\\\ 1
\end{bmatrix}
$$

This is all well and good for video games and whatever, but if you have a real camera you need to know these unknowns beforehand to be able to model it. Some can come from the manufacturer, others must be worked out through calibration.

***Lens distortion.*** Is a non-linear distortion of the scene. We can do 2D post-correction using a radial model:
\begin{align}
	x &= x_d(1 + k_1 r^2 + k_2 r^4) \\\\
	y &= y_d (1 + k_1 r^2 + k_2 r^4)
\end{align}
Where $x,y$ are real positions, $x_d, y_d$ are distorted positions, and we estimate $k_1, k_2$. 

### Calibration

If we calibrate, we can measure.

Such as by using standard calibration patterns, like a checkerboard. We *must* ensure such a pattern is flat for accuracy.

In a full $3 \times 4$ projection matrix, there are 11 total unknowns that must be found (and a 1). We can treat this via linear least squares estimation:
\begin{align}
	\begin{bmatrix}
		x \\\\ y \\\\ 1
	\end{bmatrix} &\sim \begin{bmatrix}
		m_{11} & m_{12} & m_{13} & m_{14} \\\\
		m_{21} & m_{22} & m_{23} & m_{24} \\\\
		m_{31} & m_{32} & m_{33} & 1
	\end{bmatrix} \begin{bmatrix}
		X \\\\ Y \\\\ Z \\\\ 1
	\end{bmatrix} \\\\
	\therefore x &= \frac{ m_{11} X + m_{12} Y + m_{13} Z + m_{14} }{ m_{31} X + m_{32} Y + m_{33}Z + 1 } \\\\ 
	y &= \cdots 
\end{align}

Which can be made into a linear equation $A \x = \vec 0$, which we want to solve for a non-trivial $\x$.

Thus for all $x,y$ we can make a massive huge matrix $A\vec m = \vec0$ which I am not going to write, where $\vec m$ are our 11 coefficients, and $A$ are our world and screen coordinates set up in a way to make every line one linear equation.

The solution is given an eigenvector associated with the **smallest** eigenvalue of $\underset{(12 \times 12)}{A\t A}$.

This gives a combined product $M = K [R|T]$ of extrinsic and intrinsic features.

Once we know $M$, and assuming the camera doesn't move, then we can augment the view by projecting a constructed 3D object onto our 2D plane:
$$
\begin{bmatrix}
	x_i \\\\ y_i \\\\ 1 
\end{bmatrix} = K[R_k | T_k] \begin{bmatrix}
	X_i \\\\ Y_i \\\\ Z_i \\\\ 1
\end{bmatrix}
$$

If camera moves, we must update the calibration.


### Planar Homography

Are a class of projective transformations that take a point from 1 3D plane to another $\Pi(\x) \lra \pi(\x')$. 

This can be used to track camera motion from some reference points -- update position without recalibrating. It's another matrix:

$$
\begin{bmatrix}
	x_1' \\\\ x_2' \\\\ x_3' 
\end{bmatrix} \sim \begin{bmatrix}
	h_{11} & h_{12} & h_{13} \\\\
	h_{21} & h_{22} & h_{23} \\\\
	h_{31} & h_{32} & h_{33}
\end{bmatrix} \begin{bmatrix}
	x_1 \\\\ x_2 \\\\ x_3
\end{bmatrix}
$$

We then homogenise via $x_3'$.

We solve this by finding similar points within two images. 

It is *sufficient to pick 4 points*. Since there are 9 unknowns, 2 solved per point, so 4 points solves 8 which is enough to pick up the last one.

We also use projection like this to stitch together images, after having found SIFT/SURF points.

Planar homography assumes that points picked are **co-planar**, $z=0$. 

So in video augmentation, we calibrate $K$ once and then use this method to continue along the rest of the video.


</div>
</div>

<div class="colourband">
	<h2 id="ml">Machine Learning for Computer Vision</h2>
</div>

<div class="cbox">
<div class="md-conv">

So I've done two years of ML and DL based projects specifically to do with image processing and computer vision. So I don't really have notes for this, I'll put down some of the keypoints:

- Supervised / Unsupervised learning 
- ImageNet 
- Simple ML models can be solved directly 
- Perceptron and deep learning 
- Fully connected layers aren't great for image processing -> convolution layers 
- Convolution kernels can be learned, and usually have multiple per layer 
- Max pooling (or average pooling, stride) to downsample
- CNNs learn both the features (to make embeddings) and the actual classification all at once.
- Must mind overfitting, can use data augmentation techniques 
- Can transfer learn / fine-tune to save time

</div>
</div>

	<footer>
		<div class="cbox">
			<div class="columncontainer ctwo" id="fc2">
			</div>
			<script type="text/javascript" src="../../js/footerGen.js"></script>
		</div>
	</footer>

</div>

</body>
</html>